# Data Pipeline Playbook

This document explains how climate datasets flow into the project, which jobs keep them up to date, and how to reproduce the results locally. Two subsystems collaborate:

1. **This repository (`data-pipeline/`)** — handles NASA/NOAA/OWID ingest (`update_pipeline.py`) and post-processes Sentinel outputs into fjord aggregates (`update_fjord_data.py`).
2. **Companion repository ([uummannaq-ice-from-space](https://github.com/lukaskreibig/uummannaq-ice-from-space))** — performs the heavy Sentinel‑2 segmentation, producing CSV/PNG artefacts consumed by step #2 above.

This doc focuses on the pieces that live here while pointing to the companion repo for the satellite-specific workflow.

All pipelines ultimately publish to PostgreSQL (Railway) and export JSON fallbacks so the frontend can function without the database.

---

## 1. Global climate ingest (`update_pipeline.py`)

### Responsibilities

- Download and cache datasets from:
  - NASA GISTEMP (global & Arctic temperature anomalies)
  - NOAA NSIDC Sea Ice Index (daily extent)
  - Our World in Data (CO₂ emissions)
- Normalise calendars to a 365-day year (removing 29 February)
- Smooth daily sea-ice series (moving average/Hamming window)
- Compute decadal anomalies, z-scores, IQR stats, and correlation matrices
- Persist to PostgreSQL and export JSON/CSV artefacts

### Running locally

```bash
cd data-pipeline
export DATABASE_URL=postgresql://user:pass@localhost:5432/climate
python update_pipeline.py
```

Outputs:

| Table / File | Description |
|--------------|-------------|
| `annual` | Annual anomaly rows (global, Arctic, CO₂) |
| `daily_sea_ice` | Daily extent after smoothing + leap-day removal |
| `annual_anomaly` | Combined dataset for Recharts multi-line |
| `corr_matrix` | Pearson correlations between metrics |
| `iqr_stats` | Interquartile range data for violin plots |
| `partial_2025` | In-progress year data |
| `data/data.json` | JSON fallback served by FastAPI |

Environment variables (optional) to tune smoothing windows:

```ini
SEAICE_YR_MIN=1980
SEAICE_YR_MAX=2100
SEAICE_ANOM_BASELINE_START=1981
SEAICE_ANOM_BASELINE_END=2010
SEAICE_SMOOTH_WINDOW=7       # daily smoothing (moving average)
SEAICE_DECADAL_SMOOTH=15     # Hamming smoothing for decadal curve
```

### Deployment

- **Railway cron job** runs `python update_pipeline.py` daily.
- **GitHub Action** mirrors the run to ensure reproducibility (commits generated JSON when schemas change).
- Logs and metrics are forwarded to Slack (optional webhook).

---

## 2. Uummannaq fjord aggregates (`update_fjord_data.py`)

### Purpose

Transforms the high-resolution fjord time series (derived from Sentinel‑2) into the aggregates required by the frontend:

- Season bands (early vs. late years)
- Spring anomalies (difference from 2017–2020 baseline)
- Mean fraction of ice during the sunlit season
- Freeze/breakup dates per year

### Workflow

1. Reads `summary_test_cleaned.csv` (generated by the satellite run).
2. Ensures database schema matches current expectations (adds/renames columns as needed).
3. Truncates and re-populates the following tables: `fjord_daily`, `fjord_season_band`, `fjord_spring_anomaly`, `fjord_mean_fraction`, `fjord_freeze_breakup`.
4. Converts fractions to km² using a fjord area constant (`FJORD_KM2 = 3450`).

### Run locally

```bash
cd data-pipeline
export DATABASE_URL=postgresql://user:pass@localhost:5432/climate
export FJORD_CSV_PATH=/path/to/summary_test_cleaned.csv
python update_fjord_data.py
```

### Scheduling

- Runs after `update_pipeline.py` (same cron slot) because it depends on the postgres instance being available.
- On Railway define an environment variable `FJORD_CSV_PATH` pointing to the mounted volume or S3 path.

---

## 3. Sentinel‑2 sea-ice segmentation (external repository)

The GPU-bound satellite workflow is versioned separately to keep this dashboard lean. All scripts, notebooks, and model artefacts live in **[uummannaq-ice-from-space](https://github.com/lukaskreibig/uummannaq-ice-from-space)**. Highlights:

- Queries the [Element 84 Earth Search](https://earth-search.aws.element84.com/) STAC API for Sentinel‑2 Level‑1C tiles.
- Runs the CloudSEN12-powered UNetMobV2 model to classify solid ice, light ice, water, cloud, land, and nodata pixels.
- Produces colour overlays, QA panels, and CSV summaries (`summary_test_cleaned.csv`) that are consumed by `update_fjord_data.py` in this repo.
- Documents GPU setup, Docker images, and evaluation notebooks.

Refer to that repository for the full pipeline instructions, including model checkpoints and QA procedures.

---

## Orchestration & monitoring

| Job | Trigger | Runtime | Notes |
|-----|---------|---------|-------|
| `update_pipeline.py` | Railway cron (daily 03:00 UTC) + GitHub Actions | ~4 minutes | Retries twice on network errors; writes JSON fallback. |
| `update_fjord_data.py` | Railway cron (daily 03:10 UTC) | ~1 minute | Depends on latest `summary_test_cleaned.csv`. |
| Sentinel‑2 segmentation (external repo) | Manual / ad-hoc GPU runner | 10–30 minutes per batch | Run from [uummannaq-ice-from-space](https://github.com/lukaskreibig/uummannaq-ice-from-space); update CSV before fjord job. |

Monitoring:

- Railway logs feed to dashboard; configure alerts for non-zero exit codes.
- Optional Slack webhook (`SLACK_WEBHOOK_URL`) can notify the team on failure.
- Keep an eye on data drift: compare new outputs to previous values using notebooks under `backend/jupyter_notebook/`.

---

## Tips for extending the pipeline

- **Add a new data source** — create a helper under `data-pipeline/` that returns a tidy Pandas DataFrame. Extend `update_pipeline.py` to merge it and update `backend/schemas.py` / `frontend/types/`.
- **Backfill historical ranges** — edit environment windows (`SEAICE_YR_MIN`, etc.) or run scripts with custom date ranges. Never delete historical rows in Postgres; prefer inserts with timestamps for reproducibility.
- **Testing** — stub network calls with `responses` or cache to `data/raw/`. For unit tests, target aggregator functions (e.g. baseline calculators) and validate known values.
- **Performance** — pandas operations are vectorised; avoid Python loops. If runtime exceeds cron limits consider breaking the job into incremental updates by year.

For a higher-level architectural view, return to [docs/ARCHITECTURE.md](ARCHITECTURE.md). For operational guidance (deployment, secrets, cron), see [docs/OPERATIONS.md](OPERATIONS.md).
