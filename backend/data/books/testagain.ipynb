{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e64582c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 347\u001b[0m\n\u001b[1;32m    339\u001b[0m val_tf_dataset \u001b[38;5;241m=\u001b[39m processed_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_tf_dataset(\n\u001b[1;32m    340\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    341\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m    343\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39msimple_collate_fn\n\u001b[1;32m    344\u001b[0m )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting model training with TensorFlow...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 347\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_tf_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_tf_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m    351\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# Save the model and tokenizer.\u001b[39;00m\n\u001b[1;32m    354\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreenlandic_ner_model_tf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1209\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1208\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py:1804\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1798\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1801\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1802\u001b[0m ):\n\u001b[1;32m   1803\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1804\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1806\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1703\u001b[0m   )\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# 1. SETUP & IMPORTS\n",
    "############################################\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Reduce TensorFlow verbosity\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # In case any PyTorch code is used\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    TFAutoModelForTokenClassification  # TensorFlow model for token classification\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "\n",
    "############################################\n",
    "# 2. LOAD & EXPLORE FOLKTALES DATA\n",
    "############################################\n",
    "\n",
    "df = pd.read_pickle(\"eskimo_folktales.pkl\")\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "print(df.info())\n",
    "print(\"Duplicate story IDs:\", df[\"story_id\"].duplicated().sum())\n",
    "\n",
    "df[\"text_length\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "plt.hist(df[\"text_length\"], bins=20)\n",
    "plt.title(\"Distribution of Story Lengths\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Number of Stories\")\n",
    "plt.show()\n",
    "\n",
    "############################################\n",
    "# 3. CLEAN THE TEXT\n",
    "############################################\n",
    "\n",
    "def clean_text_for_ner(text: str) -> str:\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    paragraphs = re.split(r'\\n\\s*\\n+', text.strip())\n",
    "    cleaned_paragraphs = []\n",
    "    for para in paragraphs:\n",
    "        para = re.sub(r'\\n+', ' ', para)\n",
    "        para = para.replace('’', \"'\").replace('‘', \"'\").replace('—', '-')\n",
    "        para = re.sub(r'\\s+', ' ', para).strip()\n",
    "        cleaned_paragraphs.append(para)\n",
    "    return \"\\n\\n\".join(cleaned_paragraphs)\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_for_ner)\n",
    "print(\"Sample cleaned text:\\n\", df[\"clean_text\"].iloc[0][:300], \"...\\n\")\n",
    "\n",
    "############################################\n",
    "# 4. SCRAPE CANDIDATE ENTITIES FROM THE WEB\n",
    "############################################\n",
    "\n",
    "# 4a. Scrape candidate personal names from Wiktionary\n",
    "url_names = \"https://en.wiktionary.org/wiki/Appendix:Greenlandic_given_names\"\n",
    "resp_names = requests.get(url_names)\n",
    "soup_names = BeautifulSoup(resp_names.text, \"html.parser\")\n",
    "scraped_names = set()\n",
    "for dd in soup_names.select(\"dl dd\"):\n",
    "    for link in dd.find_all(\"a\"):\n",
    "        candidate = link.get_text(strip=True)\n",
    "        if candidate and len(candidate) > 1:\n",
    "            scraped_names.add(candidate)\n",
    "print(f\"Scraped {len(scraped_names)} candidate personal names from Wiktionary.\")\n",
    "\n",
    "# 4b. Scrape candidate town names from Wikipedia\n",
    "url_towns = \"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Greenland\"\n",
    "resp_towns = requests.get(url_towns)\n",
    "soup_towns = BeautifulSoup(resp_towns.text, \"html.parser\")\n",
    "scraped_towns = set()\n",
    "tables = soup_towns.find_all(\"table\", class_=\"wikitable\")\n",
    "for table in tables:\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        for link in row.find_all(\"a\", href=True):\n",
    "            candidate = link.get_text(strip=True)\n",
    "            if candidate and len(candidate) > 1:\n",
    "                if any(bad in candidate.lower() for bad in [\n",
    "                    \"edit\", \"coordinate\", \"article\", \"statement\", \"isbn\",\n",
    "                    \"list of\", \"administrative\", \"autonomy\", \"history\", \"portal\"\n",
    "                ]):\n",
    "                    continue\n",
    "                scraped_towns.add(candidate)\n",
    "print(f\"Scraped {len(scraped_towns)} candidate town names from Wikipedia.\")\n",
    "\n",
    "############################################\n",
    "# 5. LOAD & MERGE MANUALLY CURATED CANDIDATE CSV\n",
    "############################################\n",
    "\n",
    "try:\n",
    "    manual_df = pd.read_csv(\"candidate_entities_finished.csv\")\n",
    "    print(\"Loaded manually curated candidate entities:\")\n",
    "    print(manual_df.head())\n",
    "    # Standardize keys: strip and lower-case; fix accidental labels (\"B-O\" -> \"O\")\n",
    "    manual_df[\"entity_candidate\"] = manual_df[\"entity_candidate\"].str.strip().str.lower()\n",
    "    manual_df[\"entity\"] = manual_df[\"entity\"].str.strip().replace({\"B-O\": \"O\", \"B-O \": \"O\"})\n",
    "    manual_dict = dict(zip(manual_df[\"entity_candidate\"], manual_df[\"entity\"]))\n",
    "except Exception as e:\n",
    "    print(\"Manual candidate CSV not found; proceeding with scraped data only.\")\n",
    "    manual_dict = {}\n",
    "\n",
    "entity_dict = manual_dict.copy()\n",
    "for name in scraped_names:\n",
    "    key = name.strip().lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-PER\"  # Default scraped personal names as B-PER\n",
    "for town in scraped_towns:\n",
    "    key = town.strip().lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-LOC\"  # Default scraped town names as B-LOC\n",
    "\n",
    "print(\"Final Entity Dictionary (sample):\")\n",
    "for key, val in sorted(entity_dict.items())[:20]:\n",
    "    print(f\"{key}: {val}\")\n",
    "\n",
    "final_entity_df = pd.DataFrame(list(entity_dict.items()), columns=[\"entity_candidate\", \"entity\"])\n",
    "final_entity_df.to_csv(\"final_entity_dictionary.csv\", index=False)\n",
    "print(\"Saved final entity dictionary to 'final_entity_dictionary.csv'.\")\n",
    "\n",
    "############################################\n",
    "# 6. AUTO-LABEL FOLKTALE TEXTS USING ENTITY DICTIONARY (BIO FORMAT)\n",
    "############################################\n",
    "\n",
    "def get_entity_label_bio(token, entity_dict, prev_entity):\n",
    "    token_lower = token.strip().lower()\n",
    "    if token_lower in entity_dict:\n",
    "        label = entity_dict[token_lower]\n",
    "    elif token_lower.endswith(\"s\"):\n",
    "        label = entity_dict.get(token_lower[:-1], \"O\")\n",
    "    else:\n",
    "        label = \"O\"\n",
    "    if label == \"O\":\n",
    "        return \"O\", None\n",
    "    entity_type = label.split(\"-\", 1)[-1]  # e.g., \"PER\", \"LOC\", etc.\n",
    "    if prev_entity == entity_type:\n",
    "        return f\"I-{entity_type}\", entity_type\n",
    "    else:\n",
    "        return f\"B-{entity_type}\", entity_type\n",
    "\n",
    "def auto_label_bio_using_dict(text, entity_dict):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    data_rows = []\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        prev_entity = None\n",
    "        for token in tokens:\n",
    "            bio_label, current_entity = get_entity_label_bio(token, entity_dict, prev_entity)\n",
    "            data_rows.append({\n",
    "                \"sentence_id\": sent_id,\n",
    "                \"token\": token,\n",
    "                \"ner_label\": bio_label\n",
    "            })\n",
    "            prev_entity = current_entity if bio_label != \"O\" else None\n",
    "    return data_rows\n",
    "\n",
    "all_rows = []\n",
    "doc_id = 0\n",
    "for _, row in df.iterrows():\n",
    "    labeled_tokens = auto_label_bio_using_dict(row[\"clean_text\"], entity_dict)\n",
    "    for item in labeled_tokens:\n",
    "        all_rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sentence_id\": item[\"sentence_id\"],\n",
    "            \"token\": item[\"token\"],\n",
    "            \"ner_label\": item[\"ner_label\"]\n",
    "        })\n",
    "    doc_id += 1\n",
    "\n",
    "auto_ner_df = pd.DataFrame(all_rows)\n",
    "auto_ner_df.to_csv(\"auto_ner_data.csv\", index=False)\n",
    "print(\"\\nAuto-labeled DataFrame shape:\", auto_ner_df.shape)\n",
    "print(\"Saved auto-labeled NER data to 'auto_ner_data.csv'.\")\n",
    "\n",
    "############################################\n",
    "# 7. GROUP TOKENS BY SENTENCE FOR TRAINING EXAMPLES\n",
    "############################################\n",
    "\n",
    "grouped = auto_ner_df.groupby([\"doc_id\", \"sentence_id\"])\n",
    "examples = []\n",
    "for (doc_id, sent_id), group in grouped:\n",
    "    tokens = group[\"token\"].tolist()\n",
    "    labels = group[\"ner_label\"].tolist()\n",
    "    examples.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"sentence_id\": sent_id,\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": labels\n",
    "    })\n",
    "df_grouped = pd.DataFrame(examples)\n",
    "print(\"\\nGrouped DataFrame shape:\", df_grouped.shape)\n",
    "print(df_grouped.head())\n",
    "\n",
    "############################################\n",
    "# 8. SPLIT TRAIN/VALIDATION & CREATE DATASETS\n",
    "############################################\n",
    "\n",
    "train_size = int(0.8 * len(df_grouped))\n",
    "train_df = df_grouped.iloc[:train_size]\n",
    "val_df = df_grouped.iloc[train_size:]\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Validation size:\", val_df.shape)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "############################################\n",
    "# 9. TOKENIZATION & LABEL ALIGNMENT FOR TRAINING\n",
    "############################################\n",
    "\n",
    "# Define label list using standard BIO tags.\n",
    "label_list = [\"O\", \"B-PER\", \"B-LOC\", \"B-MISC\"]\n",
    "label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# Add special tokens from the entity dictionary to reduce undesired subword splitting.\n",
    "special_tokens = [k for k in entity_dict.keys() if len(k) > 4]\n",
    "num_added = tokenizer.add_tokens(special_tokens)\n",
    "print(\"\\nNumber of special tokens added:\", num_added)\n",
    "\n",
    "# Use a simple collate function based on Keras padding (pad_sequences)\n",
    "def simple_collate_fn(features):\n",
    "    collated = {}\n",
    "    for key in features[0].keys():\n",
    "        values = [f[key] for f in features]\n",
    "        # Use pad_sequences from Keras to pad variable-length lists\n",
    "        collated[key] = tf.keras.preprocessing.sequence.pad_sequences(values, padding='post')\n",
    "    return collated\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i in range(len(examples[\"tokens\"])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        example_labels = examples[\"ner_tags\"][i]\n",
    "        aligned_labels = []\n",
    "        prev_wid = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                label_str = example_labels[wid]\n",
    "                # If a token is split into subwords, change subsequent B- to I-\n",
    "                if wid == prev_wid and label_str != \"O\" and label_str.startswith(\"B-\"):\n",
    "                    label_str = \"I-\" + label_str[2:]\n",
    "                aligned_labels.append(label2id.get(label_str, 0))\n",
    "            prev_wid = wid\n",
    "        all_labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "print(\"\\nProcessed datasets ready for training:\")\n",
    "print(processed_datasets)\n",
    "\n",
    "############################################\n",
    "# 10. SHOW LABEL DISTRIBUTION & COMPUTE WEIGHTS\n",
    "############################################\n",
    "\n",
    "label_counts = Counter(auto_ner_df[\"ner_label\"])\n",
    "print(\"\\nLabel distribution in auto_ner_df:\", label_counts)\n",
    "# Compute weights for each label as 1/(count+1)\n",
    "weight_list = [1.0 / (label_counts.get(lbl, 0) + 1) for lbl in label_list]\n",
    "weight_tensor = tf.constant(weight_list, dtype=tf.float32)\n",
    "print(\"Weight tensor:\", weight_tensor.numpy())\n",
    "\n",
    "############################################\n",
    "# 11. BUILD & TRAIN THE CUSTOM NER MODEL WITH TENSORFLOW\n",
    "############################################\n",
    "\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "# Create the TensorFlow model.\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "# Resize token embeddings to account for added special tokens.\n",
    "try:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "except AttributeError:\n",
    "    # If not available, update the config manually.\n",
    "    model.config.vocab_size = len(tokenizer)\n",
    "\n",
    "# Define a custom loss function that masks out -100 labels.\n",
    "def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    # Mask: 1 where y_true != -100, else 0.\n",
    "    mask = tf.cast(tf.not_equal(y_true, -100), dtype=tf.float32)\n",
    "    # Replace -100 with 0.\n",
    "    y_true_mod = tf.where(tf.equal(y_true, -100), tf.zeros_like(y_true), y_true)\n",
    "    y_true_mod = tf.cast(y_true_mod, tf.int32)\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true_mod, y_pred, from_logits=True)\n",
    "    loss = tf.cast(loss, tf.float32) * mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=2e-5)\n",
    "model.compile(optimizer=optimizer, loss=masked_sparse_categorical_crossentropy)\n",
    "\n",
    "# Convert processed datasets to TensorFlow tf.data.Dataset.\n",
    "train_tf_dataset = processed_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=simple_collate_fn\n",
    ")\n",
    "val_tf_dataset = processed_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    "    collate_fn=simple_collate_fn\n",
    ")\n",
    "\n",
    "print(\"\\nStarting model training with TensorFlow...\")\n",
    "history = model.fit(\n",
    "    train_tf_dataset,\n",
    "    validation_data=val_tf_dataset,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "# Save the model and tokenizer.\n",
    "model.save_pretrained(\"greenlandic_ner_model_tf\")\n",
    "tokenizer.save_pretrained(\"greenlandic_ner_model_tf\")\n",
    "\n",
    "############################################\n",
    "# 12. INFERENCE USING THE TRAINED TENSORFLOW MODEL\n",
    "############################################\n",
    "\n",
    "ner_infer = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"greenlandic_ner_model_tf\",\n",
    "    tokenizer=\"greenlandic_ner_model_tf\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "test_text = \"Nukúnguasik traveled from Ikerssuaq to Nuuk.\"\n",
    "print(\"\\nInference output on sample text:\")\n",
    "print(ner_infer(test_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b142cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (51, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   story_id  51 non-null     int64 \n",
      " 1   title     51 non-null     object\n",
      " 2   text      51 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.3+ KB\n",
      "None\n",
      "Duplicate story IDs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample cleaned text (first 300 chars):\n",
      " Our forefathers have told us much of the coming of earth, and of men, and it was a long, long while ago. Those who lived long before our day, they did not know how to store their words in little black marks, as you do; they could only tell stories. And they told of many things, and therefore we are  ...\n",
      "\n",
      "Scraped 337 candidate personal names from Wiktionary.\n",
      "Scraped 76 candidate town names from Wikipedia.\n",
      "Loaded manually curated candidate entities (first 5 rows):\n",
      "  entity_candidate entity\n",
      "0            Ailaq  B-PER\n",
      "1             Aluk  B-PER\n",
      "2           Alátaq  B-PER\n",
      "3         Amerdloq  B-PER\n",
      "4          Anarteq  B-PER\n",
      "\n",
      "Final Entity Dictionary Sample:\n",
      "aaju: B-PER\n",
      "aaneeraq: B-PER\n",
      "aani: B-PER\n",
      "aaninnguaq: B-PER\n",
      "aannguaq: B-PER\n",
      "aappilattoq: B-LOC\n",
      "aaqa: B-PER\n",
      "aasiaat: B-LOC\n",
      "aggu: B-PER\n",
      "ailaq: B-PER\n",
      "aima: B-PER\n",
      "aja: B-PER\n",
      "ajaaja: B-PER\n",
      "aka: B-PER\n",
      "akisooq: B-PER\n",
      "akitsinnguaq: B-PER\n",
      "akunnaaq: B-LOC\n",
      "aleqa: B-PER\n",
      "alibak: B-PER\n",
      "alluitsup paa: B-LOC\n",
      "\n",
      "Auto-labeled DataFrame shape: (49656, 4)\n",
      "Sample of auto-labeled data (first 10 rows):\n",
      "   doc_id  sentence_id        token ner_label\n",
      "0       0            0          Our         O\n",
      "1       0            0  forefathers         O\n",
      "2       0            0         have         O\n",
      "3       0            0         told         O\n",
      "4       0            0           us         O\n",
      "5       0            0         much         O\n",
      "6       0            0           of         O\n",
      "7       0            0          the         O\n",
      "8       0            0       coming         O\n",
      "9       0            0           of         O\n",
      "\n",
      "Grouped DataFrame shape: (2044, 4)\n",
      "First 5 training examples:\n",
      "   doc_id  sentence_id                                             tokens  \\\n",
      "0       0            0  [Our, forefathers, have, told, us, much, of, t...   \n",
      "1       0            1  [Those, who, lived, long, before, our, day, ,,...   \n",
      "2       0            2  [And, they, told, of, many, things, ,, and, th...   \n",
      "3       0            3  [Old, women, do, not, waste, their, words, idl...   \n",
      "4       0            4                      [Old, age, does, not, lie, .]   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "3   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
      "4                                 [O, O, O, O, O, O]  \n",
      "\n",
      "Train size: (1635, 4)\n",
      "Validation size: (409, 4)\n",
      "\n",
      "Number of special tokens added to tokenizer: 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1635/1635 [00:00<00:00, 16384.90 examples/s]\n",
      "Map: 100%|██████████| 409/409 [00:00<00:00, 14193.04 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed datasets ready for training:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1635\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 409\n",
      "    })\n",
      "})\n",
      "\n",
      "Label distribution in auto_ner_df: Counter({'O': 49212, 'B-PER': 390, 'B-MISC': 44, 'B-LOC': 10})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744213603.067001 17759728 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1744213603.067039 17759728 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight tensor: [2.0319834e-05 2.5575447e-03 9.0909094e-02 2.2222223e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFXLMRobertaForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFXLMRobertaForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training with TensorFlow...\n",
      "Epoch 1/5\n",
      "205/205 [==============================] - 1425s 7s/step - loss: 0.0792 - val_loss: 0.0071\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node tfxlm_roberta_for_token_classification/roberta/encoder/layer_._0/attention/self/query/Tensordot/Reshape defined at (most recent call last):\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/var/folders/2l/6514_hd91tv5448lmq79vpbw0000gn/T/ipykernel_6330/2530496903.py\", line 347, in <module>\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1209, in fit\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1804, in fit\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1381, in step_function\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1652, in train_step\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 588, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1625, in run_call_with_unpacked_inputs\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 1635, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1625, in run_call_with_unpacked_inputs\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 918, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 700, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 706, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 598, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 479, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 342, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/layers/core/dense.py\", line 244, in call\n\nDetected at node tfxlm_roberta_for_token_classification/roberta/encoder/layer_._0/attention/self/query/Tensordot/Reshape defined at (most recent call last):\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/var/folders/2l/6514_hd91tv5448lmq79vpbw0000gn/T/ipykernel_6330/2530496903.py\", line 347, in <module>\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1209, in fit\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1804, in fit\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1381, in step_function\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1652, in train_step\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 588, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1625, in run_call_with_unpacked_inputs\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 1635, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1625, in run_call_with_unpacked_inputs\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 918, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 700, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 706, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 598, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 479, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 342, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/layers/core/dense.py\", line 244, in call\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Input to reshape is a tensor with 337920 values, but the requested shape has 0\n\t [[{{node tfxlm_roberta_for_token_classification/roberta/encoder/layer_._0/attention/self/query/Tensordot/Reshape}}]]\n\t [[tfxlm_roberta_for_token_classification/roberta/encoder/layer_._8/intermediate/Gelu/mul_1/_476]]\n  (1) INVALID_ARGUMENT:  Input to reshape is a tensor with 337920 values, but the requested shape has 0\n\t [[{{node tfxlm_roberta_for_token_classification/roberta/encoder/layer_._0/attention/self/query/Tensordot/Reshape}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_22291]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 347\u001b[0m\n\u001b[1;32m    339\u001b[0m val_tf_dataset \u001b[38;5;241m=\u001b[39m processed_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto_tf_dataset(\n\u001b[1;32m    340\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    341\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m    343\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39msimple_collate_fn\n\u001b[1;32m    344\u001b[0m )\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting model training with TensorFlow...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 347\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_tf_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_tf_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m    351\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m############################################\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# 12. INFERENCE USING THE TRAINED TENSORFLOW MODEL\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m############################################\u001b[39;00m\n\u001b[1;32m    357\u001b[0m ner_infer \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    359\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    360\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m    361\u001b[0m     aggregation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    362\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1209\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1208\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node tfxlm_roberta_for_token_classification/roberta/encoder/layer_._0/attention/self/query/Tensordot/Reshape defined at (most recent call last):\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/var/folders/2l/6514_hd91tv5448lmq79vpbw0000gn/T/ipykernel_6330/2530496903.py\", line 347, in <module>\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1209, in fit\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1804, in fit\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1381, in step_function\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1652, in train_step\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 588, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1625, in run_call_with_unpacked_inputs\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 1635, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1625, in run_call_with_unpacked_inputs\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 918, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 700, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 706, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 598, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 479, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 342, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/layers/core/dense.py\", line 244, in call\n\nDetected at node tfxlm_roberta_for_token_classification/roberta/encoder/layer_._0/attention/self/query/Tensordot/Reshape defined at (most recent call last):\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/opt/homebrew/Cellar/python@3.10/3.10.16/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/var/folders/2l/6514_hd91tv5448lmq79vpbw0000gn/T/ipykernel_6330/2530496903.py\", line 347, in <module>\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1209, in fit\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1804, in fit\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1398, in train_function\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1381, in step_function\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 1370, in run_step\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1652, in train_step\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/training.py\", line 588, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1625, in run_call_with_unpacked_inputs\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 1635, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1625, in run_call_with_unpacked_inputs\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 918, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 700, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 706, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 598, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 479, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py\", line 342, in call\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/engine/base_layer.py\", line 1142, in __call__\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/Users/lukaskreibig/Documents/Dokumente Cloud/climate-dashboard/.venv/lib/python3.10/site-packages/tf_keras/src/layers/core/dense.py\", line 244, in call\n\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  Input to reshape is a tensor with 337920 values, but the requested shape has 0\n\t [[{{node tfxlm_roberta_for_token_classification/roberta/encoder/layer_._0/attention/self/query/Tensordot/Reshape}}]]\n\t [[tfxlm_roberta_for_token_classification/roberta/encoder/layer_._8/intermediate/Gelu/mul_1/_476]]\n  (1) INVALID_ARGUMENT:  Input to reshape is a tensor with 337920 values, but the requested shape has 0\n\t [[{{node tfxlm_roberta_for_token_classification/roberta/encoder/layer_._0/attention/self/query/Tensordot/Reshape}}]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_22291]"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# 1. SETUP & IMPORTS\n",
    "############################################\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"  # Reduce TensorFlow verbosity\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # (In case any PyTorch code is used)\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    TFAutoModelForTokenClassification  # TensorFlow model for token classification\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "\n",
    "############################################\n",
    "# 2. LOAD & EXPLORE FOLKTALES DATA\n",
    "############################################\n",
    "\n",
    "df = pd.read_pickle(\"eskimo_folktales.pkl\")\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "print(df.info())\n",
    "print(\"Duplicate story IDs:\", df[\"story_id\"].duplicated().sum())\n",
    "\n",
    "df[\"text_length\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "plt.hist(df[\"text_length\"], bins=20)\n",
    "plt.title(\"Distribution of Story Lengths\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Number of Stories\")\n",
    "plt.show()\n",
    "\n",
    "############################################\n",
    "# 3. CLEAN THE TEXT\n",
    "############################################\n",
    "\n",
    "def clean_text_for_ner(text: str) -> str:\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    paragraphs = re.split(r'\\n\\s*\\n+', text.strip())\n",
    "    cleaned_paragraphs = []\n",
    "    for para in paragraphs:\n",
    "        para = re.sub(r'\\n+', ' ', para)\n",
    "        para = para.replace('’', \"'\").replace('‘', \"'\").replace('—', '-')\n",
    "        para = re.sub(r'\\s+', ' ', para).strip()\n",
    "        cleaned_paragraphs.append(para)\n",
    "    return \"\\n\\n\".join(cleaned_paragraphs)\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_for_ner)\n",
    "print(\"\\nSample cleaned text (first 300 chars):\\n\", df[\"clean_text\"].iloc[0][:300], \"...\\n\")\n",
    "\n",
    "############################################\n",
    "# 4. SCRAPE CANDIDATE ENTITIES FROM THE WEB\n",
    "############################################\n",
    "\n",
    "# 4a. Scrape candidate personal names from Wiktionary\n",
    "url_names = \"https://en.wiktionary.org/wiki/Appendix:Greenlandic_given_names\"\n",
    "resp_names = requests.get(url_names)\n",
    "soup_names = BeautifulSoup(resp_names.text, \"html.parser\")\n",
    "scraped_names = set()\n",
    "for dd in soup_names.select(\"dl dd\"):\n",
    "    for link in dd.find_all(\"a\"):\n",
    "        candidate = link.get_text(strip=True)\n",
    "        if candidate and len(candidate) > 1:\n",
    "            scraped_names.add(candidate)\n",
    "print(f\"Scraped {len(scraped_names)} candidate personal names from Wiktionary.\")\n",
    "\n",
    "# 4b. Scrape candidate town names from Wikipedia\n",
    "url_towns = \"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Greenland\"\n",
    "resp_towns = requests.get(url_towns)\n",
    "soup_towns = BeautifulSoup(resp_towns.text, \"html.parser\")\n",
    "scraped_towns = set()\n",
    "tables = soup_towns.find_all(\"table\", class_=\"wikitable\")\n",
    "for table in tables:\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        for link in row.find_all(\"a\", href=True):\n",
    "            candidate = link.get_text(strip=True)\n",
    "            if candidate and len(candidate) > 1:\n",
    "                # Skip noisy links\n",
    "                if any(bad in candidate.lower() for bad in [\n",
    "                    \"edit\", \"coordinate\", \"article\", \"statement\", \"isbn\",\n",
    "                    \"list of\", \"administrative\", \"autonomy\", \"history\", \"portal\"\n",
    "                ]):\n",
    "                    continue\n",
    "                scraped_towns.add(candidate)\n",
    "print(f\"Scraped {len(scraped_towns)} candidate town names from Wikipedia.\")\n",
    "\n",
    "############################################\n",
    "# 5. LOAD & MERGE MANUALLY CURATED CANDIDATE CSV\n",
    "############################################\n",
    "\n",
    "# If available, load a CSV of manually curated candidate entities.\n",
    "try:\n",
    "    manual_df = pd.read_csv(\"candidate_entities_finished.csv\")\n",
    "    print(\"Loaded manually curated candidate entities (first 5 rows):\")\n",
    "    print(manual_df.head())\n",
    "    # Standardize keys: lower-case and strip; fix accidental labels.\n",
    "    manual_df[\"entity_candidate\"] = manual_df[\"entity_candidate\"].str.strip().str.lower()\n",
    "    manual_df[\"entity\"] = manual_df[\"entity\"].str.strip().replace({\"B-O\": \"O\", \"B-O \": \"O\"})\n",
    "    manual_dict = dict(zip(manual_df[\"entity_candidate\"], manual_df[\"entity\"]))\n",
    "except Exception as e:\n",
    "    print(\"Manual candidate CSV not found; proceeding with scraped data only.\")\n",
    "    manual_dict = {}\n",
    "\n",
    "entity_dict = manual_dict.copy()\n",
    "for name in scraped_names:\n",
    "    key = name.strip().lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-PER\"  # Default scraped personal names as B-PER\n",
    "for town in scraped_towns:\n",
    "    key = town.strip().lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-LOC\"  # Default scraped town names as B-LOC\n",
    "\n",
    "print(\"\\nFinal Entity Dictionary Sample:\")\n",
    "for key, val in sorted(entity_dict.items())[:20]:\n",
    "    print(f\"{key}: {val}\")\n",
    "\n",
    "# (Here we do not save the dictionary to disk; we just print a sample.)\n",
    "\n",
    "############################################\n",
    "# 6. AUTO-LABEL FOLKTALE TEXTS USING ENTITY DICTIONARY (BIO FORMAT)\n",
    "############################################\n",
    "\n",
    "def get_entity_label_bio(token, entity_dict, prev_entity):\n",
    "    token_lower = token.strip().lower()\n",
    "    if token_lower in entity_dict:\n",
    "        label = entity_dict[token_lower]\n",
    "    elif token_lower.endswith(\"s\"):\n",
    "        label = entity_dict.get(token_lower[:-1], \"O\")\n",
    "    else:\n",
    "        label = \"O\"\n",
    "    if label == \"O\":\n",
    "        return \"O\", None\n",
    "    entity_type = label.split(\"-\", 1)[-1]  # e.g., PER, LOC, etc.\n",
    "    if prev_entity == entity_type:\n",
    "        return f\"I-{entity_type}\", entity_type\n",
    "    else:\n",
    "        return f\"B-{entity_type}\", entity_type\n",
    "\n",
    "def auto_label_bio_using_dict(text, entity_dict):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    data_rows = []\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        prev_entity = None\n",
    "        for token in tokens:\n",
    "            bio_label, current_entity = get_entity_label_bio(token, entity_dict, prev_entity)\n",
    "            data_rows.append({\n",
    "                \"sentence_id\": sent_id,\n",
    "                \"token\": token,\n",
    "                \"ner_label\": bio_label\n",
    "            })\n",
    "            prev_entity = current_entity if bio_label != \"O\" else None\n",
    "    return data_rows\n",
    "\n",
    "all_rows = []\n",
    "doc_id = 0\n",
    "for _, row in df.iterrows():\n",
    "    labeled_tokens = auto_label_bio_using_dict(row[\"clean_text\"], entity_dict)\n",
    "    for item in labeled_tokens:\n",
    "        all_rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sentence_id\": item[\"sentence_id\"],\n",
    "            \"token\": item[\"token\"],\n",
    "            \"ner_label\": item[\"ner_label\"]\n",
    "        })\n",
    "    doc_id += 1\n",
    "\n",
    "auto_ner_df = pd.DataFrame(all_rows)\n",
    "print(\"\\nAuto-labeled DataFrame shape:\", auto_ner_df.shape)\n",
    "print(\"Sample of auto-labeled data (first 10 rows):\")\n",
    "print(auto_ner_df.head(10))\n",
    "\n",
    "############################################\n",
    "# 7. GROUP TOKENS BY SENTENCE FOR TRAINING EXAMPLES\n",
    "############################################\n",
    "\n",
    "grouped = auto_ner_df.groupby([\"doc_id\", \"sentence_id\"])\n",
    "examples = []\n",
    "for (doc_id, sent_id), group in grouped:\n",
    "    tokens = group[\"token\"].tolist()\n",
    "    labels = group[\"ner_label\"].tolist()\n",
    "    examples.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"sentence_id\": sent_id,\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": labels\n",
    "    })\n",
    "df_grouped = pd.DataFrame(examples)\n",
    "print(\"\\nGrouped DataFrame shape:\", df_grouped.shape)\n",
    "print(\"First 5 training examples:\")\n",
    "print(df_grouped.head())\n",
    "\n",
    "############################################\n",
    "# 8. SPLIT TRAIN/VALIDATION & CREATE DATASETS\n",
    "############################################\n",
    "\n",
    "train_size = int(0.8 * len(df_grouped))\n",
    "train_df = df_grouped.iloc[:train_size]\n",
    "val_df = df_grouped.iloc[train_size:]\n",
    "print(\"\\nTrain size:\", train_df.shape)\n",
    "print(\"Validation size:\", val_df.shape)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "############################################\n",
    "# 9. TOKENIZATION & LABEL ALIGNMENT FOR TRAINING\n",
    "############################################\n",
    "\n",
    "# Define label list (standard BIO tags)\n",
    "label_list = [\"O\", \"B-PER\", \"B-LOC\", \"B-MISC\"]\n",
    "label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# Add special tokens from the entity dictionary to reduce undesired subword splitting.\n",
    "special_tokens = [k for k in entity_dict.keys() if len(k) > 4]\n",
    "num_added = tokenizer.add_tokens(special_tokens)\n",
    "print(\"\\nNumber of special tokens added to tokenizer:\", num_added)\n",
    "\n",
    "# We'll use a simple collate function based on Keras pad_sequences.\n",
    "def simple_collate_fn(features):\n",
    "    import tensorflow as tf\n",
    "    collated = {}\n",
    "    for key in features[0].keys():\n",
    "        values = [f[key] for f in features]\n",
    "        # Pad each key using Keras pad_sequences so each batch has the same shape.\n",
    "        collated[key] = tf.keras.preprocessing.sequence.pad_sequences(values, padding='post')\n",
    "    return collated\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i in range(len(examples[\"tokens\"])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        example_labels = examples[\"ner_tags\"][i]\n",
    "        aligned_labels = []\n",
    "        prev_wid = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                label_str = example_labels[wid]\n",
    "                # For contiguous subwords, convert a following B- to I-\n",
    "                if wid == prev_wid and label_str != \"O\" and label_str.startswith(\"B-\"):\n",
    "                    label_str = \"I-\" + label_str[2:]\n",
    "                aligned_labels.append(label2id.get(label_str, 0))\n",
    "            prev_wid = wid\n",
    "        all_labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    load_from_cache_file=False  # Disable caching to save disk space\n",
    ")\n",
    "print(\"\\nProcessed datasets ready for training:\")\n",
    "print(processed_datasets)\n",
    "\n",
    "############################################\n",
    "# 10. SHOW LABEL DISTRIBUTION & COMPUTE WEIGHTS\n",
    "############################################\n",
    "\n",
    "label_counts = Counter(auto_ner_df[\"ner_label\"])\n",
    "print(\"\\nLabel distribution in auto_ner_df:\", label_counts)\n",
    "# Compute class weights as 1/(count+1) for each label in our label list.\n",
    "weight_list = [1.0 / (label_counts.get(lbl, 0) + 1) for lbl in label_list]\n",
    "weight_tensor = tf.constant(weight_list, dtype=tf.float32)\n",
    "print(\"Weight tensor:\", weight_tensor.numpy())\n",
    "\n",
    "############################################\n",
    "# 11. BUILD & TRAIN THE CUSTOM NER MODEL WITH TENSORFLOW\n",
    "############################################\n",
    "\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "# Create the TensorFlow model.\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "# Resize model embeddings to account for added special tokens.\n",
    "try:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "except AttributeError:\n",
    "    print(\"resize_token_embeddings not available; updating model.config.vocab_size.\")\n",
    "    model.config.vocab_size = len(tokenizer)\n",
    "\n",
    "# Define a custom loss function that masks out -100 labels.\n",
    "def masked_sparse_categorical_crossentropy(y_true, y_pred):\n",
    "    mask = tf.cast(tf.not_equal(y_true, -100), dtype=tf.float32)\n",
    "    y_true_mod = tf.where(tf.equal(y_true, -100), tf.zeros_like(y_true), y_true)\n",
    "    y_true_mod = tf.cast(y_true_mod, tf.int32)\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true_mod, y_pred, from_logits=True)\n",
    "    loss *= mask\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=2e-5)\n",
    "model.compile(optimizer=optimizer, loss=masked_sparse_categorical_crossentropy)\n",
    "\n",
    "# Convert processed datasets to TensorFlow tf.data.Dataset.\n",
    "train_tf_dataset = processed_datasets[\"train\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    "    collate_fn=simple_collate_fn\n",
    ")\n",
    "val_tf_dataset = processed_datasets[\"validation\"].to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    "    collate_fn=simple_collate_fn\n",
    ")\n",
    "\n",
    "print(\"\\nStarting model training with TensorFlow...\")\n",
    "history = model.fit(\n",
    "    train_tf_dataset,\n",
    "    validation_data=val_tf_dataset,\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "############################################\n",
    "# 12. INFERENCE USING THE TRAINED TENSORFLOW MODEL\n",
    "############################################\n",
    "\n",
    "ner_infer = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "test_text = \"Nukúnguasik traveled from Ikerssuaq to Nuuk.\"\n",
    "print(\"\\nInference output on sample text:\")\n",
    "print(ner_infer(test_text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
