{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (51, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Coming of Men, A Long, Long While Ago</td>\n",
       "      <td>Our forefathers have told us much of the comin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Nukúnguasik, Who Escaped from the Tupilak</td>\n",
       "      <td>Nukúnguasik, it is said, had land in a place w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Qujâvârssuk</td>\n",
       "      <td>A strong man had land at Ikerssuaq. The only o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Kúnigseq</td>\n",
       "      <td>There was once a wizard whose name was Kúnigse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The Woman Who Had a Bear As a Foster-Son</td>\n",
       "      <td>There was once an old woman living in a place ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   story_id                                      title  \\\n",
       "0         1  The Coming of Men, A Long, Long While Ago   \n",
       "1         2  Nukúnguasik, Who Escaped from the Tupilak   \n",
       "2         3                                Qujâvârssuk   \n",
       "3         4                                   Kúnigseq   \n",
       "4         5   The Woman Who Had a Bear As a Foster-Son   \n",
       "\n",
       "                                                text  \n",
       "0  Our forefathers have told us much of the comin...  \n",
       "1  Nukúnguasik, it is said, had land in a place w...  \n",
       "2  A strong man had land at Ikerssuaq. The only o...  \n",
       "3  There was once a wizard whose name was Kúnigse...  \n",
       "4  There was once an old woman living in a place ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For huggingface pipelines / advanced modeling\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# If you plan on using sentence-transformers for embeddings:\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_pickle(\"eskimo_folktales.pkl\")\n",
    "\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   story_id  51 non-null     int64 \n",
      " 1   title     51 non-null     object\n",
      " 2   text      51 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.3+ KB\n",
      "None\n",
      "story_id    0\n",
      "title       0\n",
      "text        0\n",
      "dtype: int64\n",
      "Duplicate story IDs: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count      51.000000\n",
       "mean      840.450980\n",
       "std      1051.865701\n",
       "min       106.000000\n",
       "25%       453.000000\n",
       "50%       599.000000\n",
       "75%       888.500000\n",
       "max      7521.000000\n",
       "Name: text_length, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.info())\n",
    "\n",
    "print(df.isnull().sum())\n",
    "print(\"Duplicate story IDs:\", df.story_id.duplicated().sum())\n",
    "\n",
    "df[\"text_length\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "df[\"text_length\"].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df[\"text_length\"], bins=20)\n",
    "plt.title(\"Distribution of Story Lengths\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Number of Stories\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Concise Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df[\"text\"] = df[\"text\"].str.replace(r\"\\n\", \" \", regex=True)\n",
    "\n",
    "\n",
    "def clean_text_for_ner(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans raw folklore text while preserving paragraph breaks.\n",
    "    1. Unify all line endings to '\\n'.\n",
    "    2. Split into paragraphs on two or more newlines.\n",
    "    3. Clean each paragraph by removing extra internal line breaks and spaces.\n",
    "    4. Normalize curly quotes, dashes.\n",
    "    5. Rejoin paragraphs with a single blank line.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Unify line endings\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # 2. Split on two or more newlines to preserve paragraph boundaries\n",
    "    paragraphs = re.split(r'\\n\\s*\\n+', text.strip())\n",
    "\n",
    "    cleaned_paragraphs = []\n",
    "    for para in paragraphs:\n",
    "        # Remove stray newlines inside each paragraph\n",
    "        para = re.sub(r'\\n+', ' ', para)\n",
    "\n",
    "        # Normalize curly quotes and dashes\n",
    "        para = para.replace('’', \"'\").replace('‘', \"'\").replace('—', '-')\n",
    "\n",
    "        # Reduce multiple spaces to a single space\n",
    "        para = re.sub(r'\\s+', ' ', para).strip()\n",
    "\n",
    "        cleaned_paragraphs.append(para)\n",
    "\n",
    "    # 5. Rejoin paragraphs with exactly one blank line (two newlines)\n",
    "    cleaned_text = \"\\n\\n\".join(cleaned_paragraphs)\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the cleaning function\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_for_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate tokens (potential names/entities) not common in English:\n",
      "Ailaq\n",
      "Aluk\n",
      "Alátaq\n",
      "Amerdloq\n",
      "Anarteq\n",
      "Angiut\n",
      "Angmagssalik\n",
      "Angusinãnguaq\n",
      "Artuk\n",
      "Asalôq\n",
      "Atakana\n",
      "Atdlarneq\n",
      "Atungait\n",
      "Au\n",
      "Avôvang\n",
      "Ernilik\n",
      "Etah\n",
      "Eyes\n",
      "Ghosts\n",
      "Hahaha\n",
      "Has\n",
      "Having\n",
      "Hrrrr\n",
      "Hunters\n",
      "Ikerssuaq\n",
      "Isigâligârssik\n",
      "Kangerdlugssuaq\n",
      "Kangârssuk\n",
      "Kilitêraq\n",
      "Kumagdlak\n",
      "Kánagssuaq\n",
      "Kâgssagssuk\n",
      "Kúnigseq\n",
      "Kûgkat\n",
      "Makíte\n",
      "Misána\n",
      "Natsivilik\n",
      "Navaránâ\n",
      "Navaránâpaluk\n",
      "Navssârssuaq\n",
      "Nerrivik\n",
      "Neruvkâq\n",
      "Nipisartángivaq\n",
      "Nukúnguasik\n",
      "Nâlaussartoq\n",
      "Ones\n",
      "Papik\n",
      "Puagssuaq\n",
      "Pualúna\n",
      "Pâtussorssuaq\n",
      "Qalagánguasê\n",
      "Qasiagssaq\n",
      "Qautaq\n",
      "Qigdlugsuk\n",
      "Qilugtûssat\n",
      "Qujâvârssuk\n",
      "Sarqiserasak\n",
      "Saunikoq\n",
      "Suagaq\n",
      "Sârdloq\n",
      "Talîlarssuaq\n",
      "Tugto\n",
      "Tungujuluk\n",
      "Tupilak\n",
      "Tupilaks\n",
      "Tôrnârssuk\n",
      "Ukaleq\n",
      "Umerdlugtoq\n",
      "Ángángŭjuk\n",
      "Âtârssuaq\n",
      "Íkardlítuarssuk\n",
      "Ímarasugssuaq\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Get a set of English vocabulary words (in lowercase)\n",
    "english_vocab = set(w.lower() for w in words.words())\n",
    "\n",
    "def contains_non_ascii(token):\n",
    "    \"\"\"Return True if the token contains any non-ASCII characters.\"\"\"\n",
    "    return any(ord(char) > 127 for char in token)\n",
    "\n",
    "# Initialize a set to hold candidate tokens\n",
    "candidate_tokens = set()\n",
    "\n",
    "# Loop through each cleaned text in your folktales DataFrame\n",
    "for text in df['clean_text']:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        # Filter out tokens that are not alphabetic (ignore punctuation, numbers)\n",
    "        if not token.isalpha():\n",
    "            continue\n",
    "        \n",
    "        # Heuristic 1: Check if token is capitalized and not a common English word.\n",
    "        if token[0].isupper() and token.lower() not in english_vocab:\n",
    "            candidate_tokens.add(token)\n",
    "        \n",
    "        # Heuristic 2: If token contains non-ASCII characters, add it as well.\n",
    "        elif contains_non_ascii(token):\n",
    "            candidate_tokens.add(token)\n",
    "\n",
    "# Sort and display candidate tokens\n",
    "sorted_candidates = sorted(candidate_tokens)\n",
    "print(\"Candidate tokens (potential names/entities) not common in English:\")\n",
    "for token in sorted_candidates:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved candidate entities to 'candidate_entities.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Convert candidate tokens to a DataFrame and save to CSV for review\n",
    "candidate_df = pd.DataFrame(sorted_candidates, columns=[\"entity_candidate\"])\n",
    "candidate_df.to_csv(\"candidate_entities.csv\", index=False)\n",
    "print(\"Saved candidate entities to 'candidate_entities.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_candidate entity\n",
      "0            Ailaq    PER\n",
      "1             Aluk    PER\n",
      "2           Alátaq    PER\n",
      "3         Amerdloq    PER\n",
      "4          Anarteq    PER\n",
      "Entity Dictionary:\n",
      "ailaq: PER\n",
      "aluk: PER\n",
      "alátaq: PER\n",
      "amerdloq: PER\n",
      "anarteq: PER\n",
      "angiut: SPI\n",
      "angmagssalik: LOC\n",
      "angusinãnguaq: PER\n",
      "artuk: PER\n",
      "asalôq: PER\n",
      "atakana: PER\n",
      "atdlarneq: PER\n",
      "atungait: PER\n",
      "au: O\n",
      "avôvang: PER\n",
      "ernilik: PER\n",
      "etah: LOC\n",
      "eyes: O\n",
      "ghosts: O\n",
      "hahaha: O \n",
      "has: O\n",
      "having: O\n",
      "hrrrr: O\n",
      "hunters: O\n",
      "ikerssuaq: LOC\n",
      "isigâligârssik: PER\n",
      "kangerdlugssuaq: LOC\n",
      "kangârssuk: LOC\n",
      "kilitêraq: PER\n",
      "kumagdlak: PER\n",
      "kánagssuaq: PER\n",
      "kâgssagssuk: PER\n",
      "kúnigseq: PER\n",
      "kûgkat: PER\n",
      "makíte: PER\n",
      "misána: PER\n",
      "natsivilik: LOC\n",
      "navaránâ: PER\n",
      "navaránâpaluk: PER\n",
      "navssârssuaq: PER\n",
      "nerrivik: SPI\n",
      "neruvkâq: PER\n",
      "nipisartángivaq: PER\n",
      "nukúnguasik: PER\n",
      "nâlaussartoq: SPI\n",
      "ones: O\n",
      "papik: PER\n",
      "puagssuaq: PER\n",
      "pualúna: PER\n",
      "pâtussorssuaq: PER\n",
      "qalagánguasê: SPI\n",
      "qasiagssaq: PER\n",
      "qautaq: PER\n",
      "qigdlugsuk: PER\n",
      "qilugtûssat: SPI\n",
      "qujâvârssuk: PER\n",
      "sarqiserasak: MON\n",
      "saunikoq: PER\n",
      "suagaq: PER\n",
      "sârdloq: LOC\n",
      "talîlarssuaq: PER\n",
      "tugto: PER\n",
      "tungujuluk: PER\n",
      "tupilak: SPI\n",
      "tupilaks: SPI\n",
      "tôrnârssuk: SPI\n",
      "ukaleq: PER\n",
      "umerdlugtoq: PER\n",
      "ángángŭjuk: PER\n",
      "âtârssuaq: PER\n",
      "íkardlítuarssuk: PER\n",
      "ímarasugssuaq: PER\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your finished candidate entities CSV.\n",
    "# It should have columns like \"entity_candidate\" and \"entity\"\n",
    "candidates_df = pd.read_csv(\"candidate_entities_finished.csv\")\n",
    "print(candidates_df.head())\n",
    "\n",
    "# Create a dictionary mapping token (lowercase) to its label.\n",
    "# For example, both \"tupilak\" and \"tupilaks\" can be in your CSV.\n",
    "entity_dict = dict(zip(candidates_df[\"entity_candidate\"].str.lower(), candidates_df[\"entity\"]))\n",
    "print(\"Entity Dictionary:\")\n",
    "for key, val in entity_dict.items():\n",
    "    print(f\"{key}: {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the \"fast\" tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\", use_fast=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\")\n",
    "\n",
    "df[\"entities\"] = df[\"clean_text\"].apply(lambda x: ner_pipeline(x))\n",
    "df[\"entities\"].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity Classification / Custom Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classify_entity(entity_text: str) -> str:\n",
    "#     # example rules\n",
    "#     # in real production, you'd have a better approach or a fine-tuned model\n",
    "#     mythical_keywords = [\"spirit\", \"wizard\", \"shaman\", \"tupilak\"]\n",
    "#     animal_keywords   = [\"bear\", \"dog\", \"wolf\", \"fox\"]\n",
    "#     # expand with more, or use synonyms\n",
    "\n",
    "#     text_lower = entity_text.lower()\n",
    "#     if any(k in text_lower for k in mythical_keywords):\n",
    "#         return \"SUPER_NATURAL\"\n",
    "#     elif any(k in text_lower for k in animal_keywords):\n",
    "#         return \"ANIMAL\"\n",
    "#     else:\n",
    "#         return \"UNKNOWN\"\n",
    "\n",
    "# def annotate_entities(entities):\n",
    "#     annotated = []\n",
    "#     for ent in entities:\n",
    "#         ent_text = ent[\"word\"]\n",
    "#         # ent['entity_group'] might be 'PER', 'LOC', 'ORG', etc. from the HF model\n",
    "#         label = classify_entity(ent_text)\n",
    "#         # create new dict with custom label\n",
    "#         annotated.append({\n",
    "#             \"text\": ent_text,\n",
    "#             \"hf_label\": ent[\"entity_group\"],\n",
    "#             \"custom_label\": label,\n",
    "#             \"score\": ent[\"score\"],\n",
    "#         })\n",
    "#     return annotated\n",
    "\n",
    "# df[\"annotated_entities\"] = df[\"entities\"].apply(annotate_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_subwords(token_entities, max_gap=1):\n",
    "    \"\"\"\n",
    "    Merge adjacent subwords if all conditions match:\n",
    "     - They share the same entity_group.\n",
    "     - The next subword starts within 'max_gap' characters of the previous token's end\n",
    "       OR the subword starts with '##'.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    current = None\n",
    "\n",
    "    for item in token_entities:\n",
    "        w = item[\"word\"]\n",
    "        entity_group = item[\"entity_group\"]\n",
    "        start = item[\"start\"]\n",
    "        end = item[\"end\"]\n",
    "\n",
    "        if current is None:\n",
    "            current = {**item}\n",
    "            continue\n",
    "\n",
    "        # Same entity group?\n",
    "        same_group = (entity_group == current[\"entity_group\"])\n",
    "\n",
    "        # Subword indicator or near-adjacent offsets?\n",
    "        subword_prefix = w.startswith(\"##\")\n",
    "        close_offsets = (start - current[\"end\"]) <= max_gap\n",
    "\n",
    "        if same_group and (subword_prefix or close_offsets):\n",
    "            # Merge into current\n",
    "            current[\"word\"] += w.replace(\"##\", \"\")\n",
    "            current[\"end\"] = end\n",
    "            # Optional: keep the highest score\n",
    "            current[\"score\"] = max(current[\"score\"], item[\"score\"])\n",
    "        else:\n",
    "            # Different entity group or not adjacent => close off current\n",
    "            merged.append(current)\n",
    "            current = {**item}\n",
    "\n",
    "    if current:\n",
    "        merged.append(current)\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"merged_entities\"] = df[\"entities\"].apply(merge_subwords)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one = df[\"entities\"][1]\n",
    "len(df_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one = df[\"merged_entities\"][1]\n",
    "print(len(df_one))\n",
    "df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wiktionary.org/wiki/Appendix:Greenlandic_given_names\"\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# We expect names to appear inside <dd> or <a> elements within the main content area.\n",
    "names = set()  # use a set to avoid duplicates\n",
    "\n",
    "# Strategy 1: Grab all <dd> elements inside <dl> (the dictionary list) and parse them\n",
    "all_dds = soup.select(\"dl dd\")\n",
    "\n",
    "for dd in all_dds:\n",
    "    # (a) If there's a direct <a> link with a name\n",
    "    links = dd.find_all(\"a\")\n",
    "    for link in links:\n",
    "        # Link text is often the name, e.g. \"Aaju\" or \"Amâsa\"\n",
    "        candidate_name = link.get_text(strip=True)\n",
    "        if candidate_name:\n",
    "            names.add(candidate_name)\n",
    "\n",
    "    # (b) The dd might have text outside <a>, e.g. \"Amasa* (<i>Amâsa</i>) m 11 1939-2007\"\n",
    "    # We can do a simple extraction of the entire text of <dd> to see if there's a name we missed:\n",
    "    full_text = dd.get_text(\" \", strip=True)\n",
    "    # If you want to do more advanced parsing here, do it with regex or string splitting.\n",
    "    # E.g., look for capital words near parentheses. For now, we’ll just demonstrate:\n",
    "    # (If \"Amasa\" is outside the <a>, we might parse it.)\n",
    "    # This approach can be more refined based on your exact HTML structure:\n",
    "    # \n",
    "    # import re\n",
    "    # pattern = re.compile(r\"[A-Z][a-zA-Zûâåé]*(?=\\s*\\()\")  # example\n",
    "    # found = pattern.findall(full_text)\n",
    "    # for f in found:\n",
    "    #     names.add(f)\n",
    "\n",
    "# Convert set to sorted list for consistency\n",
    "sorted_names = sorted(names)\n",
    "print(\"Extracted names:\")\n",
    "for nm in sorted_names:\n",
    "    print(nm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sorted_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Greenland\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "towns = set()\n",
    "\n",
    "# Find ALL the tables with class=\"wikitable\"\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable\")\n",
    "\n",
    "# Loop through each table\n",
    "for table in tables:\n",
    "    # Each row might contain a <th> or <td> with a link to the town\n",
    "    rows = table.find_all(\"tr\")\n",
    "\n",
    "    for row in rows:\n",
    "        # Some rows have multiple <td>, or might store the link in <th>, so let's find all <a> in the row\n",
    "        links = row.find_all(\"a\", href=True)\n",
    "        for link in links:\n",
    "            name = link.get_text(strip=True)\n",
    "            if not name or len(name) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Simple filter for generic or obviously non-town links\n",
    "            # e.g., skip \"edit\", \"Coordinates\", \"Articles\", etc.\n",
    "            if any(bad_word in name.lower() for bad_word in [\n",
    "                \"edit\", \"coordinate\", \"article\", \"statement\", \"isbn\",\n",
    "                \"list of\", \"administrative\", \"autonomy\",\n",
    "                \"history\", \"portal\"\n",
    "            ]):\n",
    "                continue\n",
    "\n",
    "            towns.add(name)\n",
    "\n",
    "# Convert to sorted list for easy viewing\n",
    "sorted_towns = sorted(towns)\n",
    "print(f\"Found {len(sorted_towns)} possible town names from all tables:\")\n",
    "for town in sorted_towns:\n",
    "    print(town)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# 1. SCRAPE GREENLANDIC NAMES/TOWNS\n",
    "####################################\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Scrape personal names from Wiktionary\n",
    "url_names = \"https://en.wiktionary.org/wiki/Appendix:Greenlandic_given_names\"\n",
    "resp_names = requests.get(url_names)\n",
    "html_names = resp_names.text\n",
    "\n",
    "soup_names = BeautifulSoup(html_names, \"html.parser\")\n",
    "\n",
    "greenlandic_names = set()\n",
    "all_dds = soup_names.select(\"dl dd\")\n",
    "for dd in all_dds:\n",
    "    links = dd.find_all(\"a\")\n",
    "    for link in links:\n",
    "        candidate = link.get_text(strip=True)\n",
    "        if candidate:\n",
    "            greenlandic_names.add(candidate)\n",
    "\n",
    "    full_text = dd.get_text(\" \", strip=True)\n",
    "    # If you want deeper parsing with regex, you can do so here\n",
    "\n",
    "greenlandic_names = {nm for nm in greenlandic_names if len(nm) > 1}\n",
    "print(f\"Collected {len(greenlandic_names)} possible Greenlandic personal names.\")\n",
    "\n",
    "\n",
    "# Scrape town/city names from Wikipedia\n",
    "url_towns = \"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Greenland\"\n",
    "resp_towns = requests.get(url_towns)\n",
    "html_towns = resp_towns.text\n",
    "\n",
    "soup_towns = BeautifulSoup(html_towns, \"html.parser\")\n",
    "town_tables = soup_towns.find_all(\"table\", class_=\"wikitable\")\n",
    "\n",
    "greenlandic_towns = set()\n",
    "for table in town_tables:\n",
    "    rows = table.find_all(\"tr\")\n",
    "    for row in rows:\n",
    "        links = row.find_all(\"a\", href=True)\n",
    "        for link in links:\n",
    "            candidate = link.get_text(strip=True)\n",
    "            if not candidate or len(candidate) < 2:\n",
    "                continue\n",
    "            # Filter out typical wiki noise\n",
    "            if any(bad_word in candidate.lower() for bad_word in [\n",
    "                \"edit\", \"coordinate\", \"article\", \"statement\", \"isbn\",\n",
    "                \"list of\", \"administrative\", \"autonomy\", \"history\", \"portal\"\n",
    "            ]):\n",
    "                continue\n",
    "            greenlandic_towns.add(candidate)\n",
    "\n",
    "print(f\"Collected {len(greenlandic_towns)} possible Greenlandic town/city names.\")\n",
    "\n",
    "\n",
    "############################\n",
    "# 2. LOAD & CLEAN FOLKTALES\n",
    "############################\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "df = pd.read_pickle(\"eskimo_folktales.pkl\")  # your original data\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "\n",
    "# Basic cleaning: remove stray newlines, unify line endings, etc.\n",
    "df[\"text\"] = df[\"text\"].str.replace(r\"\\r\\n|\\n|\\r\", \" \", regex=True)\n",
    "\n",
    "def clean_text_for_ner(text: str) -> str:\n",
    "    # Normalize quotes/dashes\n",
    "    text = text.replace('’', \"'\").replace('‘', \"'\").replace('—', '-')\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_for_ner)\n",
    "print(df.head(3))\n",
    "\n",
    "\n",
    "###################################\n",
    "# 3. AUTOMATIC NER LABEL GENERATION\n",
    "###################################\n",
    "# We'll create a quick method that:\n",
    "#  - Splits text into sentences (very basic approach)\n",
    "#  - Splits each sentence into tokens\n",
    "#  - Checks if token is in the personal name or town set\n",
    "#  - Assigns \"PER\" or \"LOC\" if matched, else \"O\"\n",
    "#\n",
    "# Note: This is a naive approach but a good starting point for building\n",
    "# a \"silver\" dataset you can refine manually.\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")  # Ensure you have the NLTK tokenizer data\n",
    "\n",
    "def auto_label_greenlandic(text, person_names, town_names):\n",
    "    \"\"\"\n",
    "    Splits text into sentences and tokens, then assigns NER tags based on dictionary lookups.\n",
    "    B- / I- tagging is optional if you want strict BIO formatting. We'll keep it simple: PER, LOC, or O.\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    data_rows = []\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        for token_id, token in enumerate(tokens):\n",
    "            # default is O\n",
    "            ner_label = \"O\"\n",
    "            # check dictionary matches (case-insensitive)\n",
    "            token_lower = token.lower()\n",
    "\n",
    "            # If EXACT match is too strict, consider substring or partial match\n",
    "            # (But exact is simpler for this demonstration)\n",
    "            if token in person_names:\n",
    "                ner_label = \"PER\"\n",
    "            elif token in town_names:\n",
    "                ner_label = \"LOC\"\n",
    "\n",
    "            # Alternatively, do case-insensitive match:\n",
    "            # if token_lower in {n.lower() for n in person_names}:\n",
    "            #     ner_label = \"PER\"\n",
    "            # elif token_lower in {t.lower() for t in town_names}:\n",
    "            #     ner_label = \"LOC\"\n",
    "\n",
    "            data_rows.append({\n",
    "                \"sentence_id\": sent_id,\n",
    "                \"token\": token,\n",
    "                \"ner_label\": ner_label\n",
    "            })\n",
    "\n",
    "    return data_rows\n",
    "\n",
    "# We'll do this for each row in df and accumulate results\n",
    "all_rows = []\n",
    "doc_id = 0\n",
    "for idx, row in df.iterrows():\n",
    "    text = row[\"clean_text\"]\n",
    "    # auto-label\n",
    "    labeled_tokens = auto_label_greenlandic(text, greenlandic_names, greenlandic_towns)\n",
    "    for item in labeled_tokens:\n",
    "        # add the doc id so we can separate by text\n",
    "        all_rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sentence_id\": item[\"sentence_id\"],\n",
    "            \"token\": item[\"token\"],\n",
    "            \"ner_label\": item[\"ner_label\"]\n",
    "        })\n",
    "    doc_id += 1\n",
    "\n",
    "# Create a DataFrame\n",
    "auto_ner_df = pd.DataFrame(all_rows)\n",
    "print(\"Auto-labeled DataFrame size:\", auto_ner_df.shape)\n",
    "auto_ner_df.head(20)\n",
    "\n",
    "# We can now save this in a typical \"CoNLL-style\" CSV or TSV for manual refinement\n",
    "auto_ner_df.to_csv(\"auto_ner_data.csv\", index=False)\n",
    "\n",
    "print(\"Saved auto-labeled NER data to 'auto_ner_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_token(token, prev_label, name_set, town_set):\n",
    "    # default\n",
    "    label = \"O\"\n",
    "\n",
    "    if token in name_set:\n",
    "        label = \"B-PER\" if prev_label != \"PER\" else \"I-PER\"\n",
    "    elif token in town_set:\n",
    "        label = \"B-LOC\" if prev_label != \"LOC\" else \"I-LOC\"\n",
    "\n",
    "    return label\n",
    "\n",
    "# ... then for each token, check if the last label was PER or LOC, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, pipeline\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Scrape personal names\n",
    "########################\n",
    "url_names = \"https://en.wiktionary.org/wiki/Appendix:Greenlandic_given_names\"\n",
    "resp_names = requests.get(url_names)\n",
    "soup_names = BeautifulSoup(resp_names.text, \"html.parser\")\n",
    "\n",
    "greenlandic_names = set()\n",
    "for dd in soup_names.select(\"dl dd\"):\n",
    "    # gather names in <a>\n",
    "    for link in dd.find_all(\"a\"):\n",
    "        candidate = link.get_text(strip=True)\n",
    "        if len(candidate) > 1:\n",
    "            greenlandic_names.add(candidate)\n",
    "\n",
    "    # optionally parse text outside <a>\n",
    "    # ...\n",
    "print(f\"Collected {len(greenlandic_names)} possible Greenlandic personal names.\")\n",
    "\n",
    "\n",
    "###########################\n",
    "# Scrape town/city names\n",
    "###########################\n",
    "url_towns = \"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Greenland\"\n",
    "resp_towns = requests.get(url_towns)\n",
    "soup_towns = BeautifulSoup(resp_towns.text, \"html.parser\")\n",
    "\n",
    "greenlandic_towns = set()\n",
    "tables = soup_towns.find_all(\"table\", class_=\"wikitable\")\n",
    "for table in tables:\n",
    "    rows = table.find_all(\"tr\")\n",
    "    for row in rows:\n",
    "        links = row.find_all(\"a\", href=True)\n",
    "        for link in links:\n",
    "            candidate = link.get_text(strip=True)\n",
    "            # skip short, empty, or wiki nav links\n",
    "            if len(candidate) < 2:\n",
    "                continue\n",
    "            if any(bad in candidate.lower() for bad in [\n",
    "                \"edit\", \"coordinate\", \"article\", \"statement\", \"isbn\",\n",
    "                \"list of\", \"administrative\", \"autonomy\", \"history\", \"portal\"\n",
    "            ]):\n",
    "                continue\n",
    "            greenlandic_towns.add(candidate)\n",
    "\n",
    "print(f\"Collected {len(greenlandic_towns)} possible Greenlandic town/city names.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"eskimo_folktales.pkl\")\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "\n",
    "# Simple cleaning\n",
    "df[\"text\"] = df[\"text\"].str.replace(r\"\\r\\n|\\n|\\r\", \" \", regex=True)\n",
    "def clean_text_for_ner(text: str) -> str:\n",
    "    text = text.replace('’', \"'\").replace('‘', \"'\").replace('—', '-')\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_for_ner)\n",
    "print(df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample Greenlandic Names:\", list(greenlandic_names)[:10])\n",
    "print(\"Sample Greenlandic Towns:\", list(greenlandic_towns)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_label_greenlandic(text, person_names, town_names):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    data_rows = []\n",
    "\n",
    "    # Convert dictionaries to lowercase for matching\n",
    "    person_names_lower = {name.lower() for name in person_names}\n",
    "    town_names_lower = {name.lower() for name in town_names}\n",
    "\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        for token in tokens:\n",
    "            ner_label = \"O\"\n",
    "            # Use lowercase for matching\n",
    "            token_lower = token.lower()\n",
    "            if token_lower in person_names_lower:\n",
    "                ner_label = \"PER\"\n",
    "            elif token_lower in town_names_lower:\n",
    "                ner_label = \"LOC\"\n",
    "            data_rows.append({\n",
    "                \"sentence_id\": sent_id,\n",
    "                \"token\": token,\n",
    "                \"ner_label\": ner_label\n",
    "            })\n",
    "    return data_rows\n",
    "\n",
    "\n",
    "all_rows = []\n",
    "doc_id = 0\n",
    "for idx, row in df.iterrows():\n",
    "    labeled_tokens = auto_label_greenlandic(row[\"clean_text\"], greenlandic_names, greenlandic_towns)\n",
    "    for item in labeled_tokens:\n",
    "        all_rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sentence_id\": item[\"sentence_id\"],\n",
    "            \"token\": item[\"token\"],\n",
    "            \"ner_label\": item[\"ner_label\"]\n",
    "        })\n",
    "    doc_id += 1\n",
    "\n",
    "auto_ner_df = pd.DataFrame(all_rows)\n",
    "print(\"Auto-labeled DataFrame size:\", auto_ner_df.shape)\n",
    "auto_ner_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For document 0, sentence 0:\n",
    "doc0_sent0 = auto_ner_df[(auto_ner_df['doc_id'] == 0) & (auto_ner_df['sentence_id'] == 0)]\n",
    "print(doc0_sent0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = auto_ner_df.groupby([\"doc_id\", \"sentence_id\"])\n",
    "\n",
    "examples = []\n",
    "for (doc_id, sent_id), group in grouped:\n",
    "    tokens = group[\"token\"].tolist()\n",
    "    labels = group[\"ner_label\"].tolist()\n",
    "    examples.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"sentence_id\": sent_id,\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": labels\n",
    "    })\n",
    "\n",
    "df_grouped = pd.DataFrame(examples)\n",
    "print(\"Grouped DataFrame shape:\", df_grouped.shape)\n",
    "df_grouped.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(df_grouped))\n",
    "train_df = df_grouped.iloc[:train_size]\n",
    "val_df = df_grouped.iloc[train_size:]\n",
    "\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Val size:\", val_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"O\", \"PER\", \"LOC\"]\n",
    "label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"  # better for non-English\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    # Tokenize the list-of-tokens for each example.\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "    all_labels = []\n",
    "    # Loop over each example in the batch\n",
    "    for i in range(len(examples[\"tokens\"])):\n",
    "        # Get the mapping from subword to word index for this example\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        example_labels = examples[\"ner_tags\"][i]\n",
    "        aligned_labels = []\n",
    "        # For each token produced by the tokenizer, assign a label\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                # Special tokens like [CLS], [SEP], or padding\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                aligned_labels.append(label2id[example_labels[word_id]])\n",
    "        all_labels.append(aligned_labels)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    \n",
    "    all_labels = []\n",
    "    # Process each example in the batch individually\n",
    "    for i in range(len(examples[\"tokens\"])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        example_labels = examples[\"ner_tags\"][i]\n",
    "        aligned_labels = []\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                aligned_labels.append(label2id[example_labels[word_id]])\n",
    "        all_labels.append(aligned_labels)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ner_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ner_df['ner_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_sentences = auto_ner_df.groupby(['doc_id', 'sentence_id'])['token'].apply(list)\n",
    "grouped_labels = auto_ner_df.groupby(['doc_id', 'sentence_id'])['ner_label'].apply(list)\n",
    "\n",
    "grouped_sentences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import evaluate\n",
    "\n",
    "# seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     predictions, labels = p\n",
    "#     # Get predicted label indices (take the argmax over the last dimension)\n",
    "#     predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "#     true_labels = []\n",
    "#     true_preds = []\n",
    "    \n",
    "#     # Iterate over each example in the batch\n",
    "#     for pred_row, label_row in zip(predictions, labels):\n",
    "#         temp_true_labels = []\n",
    "#         temp_true_preds = []\n",
    "#         for p_i, l_i in zip(pred_row, label_row):\n",
    "#             # Skip special tokens that are marked as -100\n",
    "#             if l_i == -100:\n",
    "#                 continue\n",
    "#             temp_true_labels.append(id2label[l_i])\n",
    "#             temp_true_preds.append(id2label[p_i])\n",
    "#         # Only add examples that have at least one valid token\n",
    "#         if temp_true_labels:\n",
    "#             true_labels.append(temp_true_labels)\n",
    "#             true_preds.append(temp_true_preds)\n",
    "    \n",
    "#     # If no valid tokens exist, return default values\n",
    "#     if len(true_labels) == 0:\n",
    "#         return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"accuracy\": 1.0}\n",
    "    \n",
    "#     results = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "#     return {\n",
    "#         \"precision\": results.get(\"overall_precision\", 0.0),\n",
    "#         \"recall\": results.get(\"overall_recall\", 0.0),\n",
    "#         \"f1\": results.get(\"overall_f1\", 0.0),\n",
    "#         \"accuracy\": results.get(\"overall_accuracy\", 1.0)\n",
    "#     }\n",
    "\n",
    "\n",
    "# from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n",
    "\n",
    "# num_labels = len(label_list)\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\n",
    "#     model_checkpoint,\n",
    "#     num_labels=num_labels,\n",
    "#     id2label=id2label,\n",
    "#     label2id=label2id\n",
    "# )\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"greenlandic_ner_checkpoints\",\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     logging_steps=50\n",
    "# )\n",
    "\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=processed_datasets[\"train\"],\n",
    "#     eval_dataset=processed_datasets[\"validation\"],\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=data_collator,  # Add this line\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model(\"greenlandic_ner_model\")\n",
    "# tokenizer.save_pretrained(\"greenlandic_ner_model\")\n",
    "\n",
    "# ner_infer = pipeline(\n",
    "#     \"ner\",\n",
    "#     model=\"greenlandic_ner_model\",\n",
    "#     tokenizer=\"greenlandic_ner_model\",\n",
    "#     aggregation_strategy=\"simple\"\n",
    "# )\n",
    "\n",
    "# test_text = \"Nukúnguasik traveled from Ikerssuaq to Nuuk.\"\n",
    "# print(ner_infer(test_text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
