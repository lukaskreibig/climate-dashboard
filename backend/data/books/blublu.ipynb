{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "/Users/lukaskreibig/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (51, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   story_id  51 non-null     int64 \n",
      " 1   title     51 non-null     object\n",
      " 2   text      51 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.3+ KB\n",
      "None\n",
      "Duplicate story IDs: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned text:\n",
      "0    Our forefathers have told us much of the comin...\n",
      "Name: clean_text, dtype: object\n",
      "Scraped 337 candidate personal names from Wiktionary.\n",
      "Scraped 76 candidate town names from Wikipedia.\n",
      "Loaded manually curated candidate entities:\n",
      "  entity_candidate entity\n",
      "0            Ailaq  B-PER\n",
      "1             Aluk  B-PER\n",
      "2           Alátaq  B-PER\n",
      "3         Amerdloq  B-PER\n",
      "4          Anarteq  B-PER\n",
      "Final Entity Dictionary (sample):\n",
      "aaju: B-PER\n",
      "aaneeraq: B-PER\n",
      "aani: B-PER\n",
      "aaninnguaq: B-PER\n",
      "aannguaq: B-PER\n",
      "aappilattoq: B-LOC\n",
      "aaqa: B-PER\n",
      "aasiaat: B-LOC\n",
      "aggu: B-PER\n",
      "ailaq: B-PER\n",
      "aima: B-PER\n",
      "aja: B-PER\n",
      "ajaaja: B-PER\n",
      "aka: B-PER\n",
      "akisooq: B-PER\n",
      "akitsinnguaq: B-PER\n",
      "akunnaaq: B-LOC\n",
      "aleqa: B-PER\n",
      "alibak: B-PER\n",
      "alluitsup paa: B-LOC\n",
      "Saved final entity dictionary to 'final_entity_dictionary.csv'.\n",
      "Auto-labeled DataFrame shape: (49656, 4)\n",
      "Saved auto-labeled NER data to 'auto_ner_data.csv'.\n",
      "Grouped DataFrame shape: (2044, 4)\n",
      "   doc_id  sentence_id                                             tokens  \\\n",
      "0       0            0  [Our, forefathers, have, told, us, much, of, t...   \n",
      "1       0            1  [Those, who, lived, long, before, our, day, ,,...   \n",
      "2       0            2  [And, they, told, of, many, things, ,, and, th...   \n",
      "3       0            3  [Old, women, do, not, waste, their, words, idl...   \n",
      "4       0            4                      [Old, age, does, not, lie, .]   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "3   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
      "4                                 [O, O, O, O, O, O]  \n",
      "Train size: (1635, 4)\n",
      "Validation size: (409, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1635/1635 [00:00<00:00, 15911.00 examples/s]\n",
      "Map: 100%|██████████| 409/409 [00:00<00:00, 18362.20 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed datasets ready for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/2l/6514_hd91tv5448lmq79vpbw0000gn/T/ipykernel_26644/4252213788.py:338: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='306' max='306' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [306/306 04:58, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.010451</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.944099</td>\n",
       "      <td>0.998167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.008965</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.944099</td>\n",
       "      <td>0.997583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukaskreibig/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/lukaskreibig/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/lukaskreibig/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference output:\n",
      "[{'entity_group': 'PER', 'score': 0.9710044, 'word': 'Nukúnguasik', 'start': 0, 'end': 11}, {'entity_group': 'PER', 'score': 0.7703736, 'word': 'Ikerssuaq', 'start': 26, 'end': 35}, {'entity_group': 'PER', 'score': 0.7509668, 'word': 'Nuuk', 'start': 39, 'end': 43}]\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# 1. SETUP & IMPORTS\n",
    "############################\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Adjust for your MPS backend\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from transformers import (\n",
    "    pipeline, AutoTokenizer, AutoModelForTokenClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "############################\n",
    "# 2. LOAD & EXPLORE DATA\n",
    "############################\n",
    "\n",
    "df = pd.read_pickle(\"eskimo_folktales.pkl\")\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "print(df.info())\n",
    "print(\"Duplicate story IDs:\", df.story_id.duplicated().sum())\n",
    "\n",
    "df[\"text_length\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "plt.hist(df[\"text_length\"], bins=20)\n",
    "plt.title(\"Distribution of Story Lengths\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Number of Stories\")\n",
    "plt.show()\n",
    "\n",
    "############################\n",
    "# 3. CLEAN THE TEXT\n",
    "############################\n",
    "\n",
    "def clean_text_for_ner(text: str) -> str:\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    paragraphs = re.split(r'\\n\\s*\\n+', text.strip())\n",
    "    cleaned_paragraphs = []\n",
    "    for para in paragraphs:\n",
    "        para = re.sub(r'\\n+', ' ', para)\n",
    "        para = para.replace('’', \"'\").replace('‘', \"'\").replace('—', '-')\n",
    "        para = re.sub(r'\\s+', ' ', para).strip()\n",
    "        cleaned_paragraphs.append(para)\n",
    "    return \"\\n\\n\".join(cleaned_paragraphs)\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_for_ner)\n",
    "print(\"Sample cleaned text:\")\n",
    "print(df[\"clean_text\"].head(1))\n",
    "\n",
    "############################\n",
    "# 4. SCRAPE CANDIDATE ENTITIES FROM THE WEB\n",
    "############################\n",
    "\n",
    "# 4a. Scrape candidate personal names from Wiktionary\n",
    "url_names = \"https://en.wiktionary.org/wiki/Appendix:Greenlandic_given_names\"\n",
    "resp_names = requests.get(url_names)\n",
    "soup_names = BeautifulSoup(resp_names.text, \"html.parser\")\n",
    "scraped_names = set()\n",
    "for dd in soup_names.select(\"dl dd\"):\n",
    "    for link in dd.find_all(\"a\"):\n",
    "        candidate = link.get_text(strip=True)\n",
    "        if candidate and len(candidate) > 1:\n",
    "            scraped_names.add(candidate)\n",
    "print(f\"Scraped {len(scraped_names)} candidate personal names from Wiktionary.\")\n",
    "\n",
    "# 4b. Scrape candidate town names from Wikipedia\n",
    "url_towns = \"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Greenland\"\n",
    "resp_towns = requests.get(url_towns)\n",
    "soup_towns = BeautifulSoup(resp_towns.text, \"html.parser\")\n",
    "scraped_towns = set()\n",
    "for table in soup_towns.find_all(\"table\", class_=\"wikitable\"):\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        for link in row.find_all(\"a\", href=True):\n",
    "            candidate = link.get_text(strip=True)\n",
    "            if candidate and len(candidate) > 1:\n",
    "                if any(bad in candidate.lower() for bad in [\n",
    "                    \"edit\", \"coordinate\", \"article\", \"statement\", \"isbn\",\n",
    "                    \"list of\", \"administrative\", \"autonomy\", \"history\", \"portal\"\n",
    "                ]):\n",
    "                    continue\n",
    "                scraped_towns.add(candidate)\n",
    "print(f\"Scraped {len(scraped_towns)} candidate town names from Wikipedia.\")\n",
    "\n",
    "############################\n",
    "# 5. LOAD MANUALLY CURATED CSV & MERGE WITH SCRAPED DATA\n",
    "############################\n",
    "\n",
    "# The CSV \"candidate_entities_finished.csv\" should have columns:\n",
    "# \"entity_candidate\", \"entity\" (e.g., \"Ikerssuaq,B-LOC\")\n",
    "try:\n",
    "    manual_df = pd.read_csv(\"candidate_entities_finished.csv\")\n",
    "    print(\"Loaded manually curated candidate entities:\")\n",
    "    print(manual_df.head())\n",
    "    manual_dict = dict(zip(manual_df[\"entity_candidate\"].str.lower(), manual_df[\"entity\"]))\n",
    "except Exception as e:\n",
    "    print(\"Manual candidate CSV not found; proceeding with scraped data only.\")\n",
    "    manual_dict = {}\n",
    "\n",
    "# Merge: manual data takes precedence; add scraped names and towns if not present.\n",
    "entity_dict = manual_dict.copy()\n",
    "for name in scraped_names:\n",
    "    key = name.lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-PER\"  # Default scraped personal names as beginning of PER\n",
    "for town in scraped_towns:\n",
    "    key = town.lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-LOC\"  # Default scraped town names as beginning of LOC\n",
    "\n",
    "print(\"Final Entity Dictionary (sample):\")\n",
    "for k, v in list(sorted(entity_dict.items()))[:20]:\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# Save final entity dictionary for review.\n",
    "final_entity_df = pd.DataFrame(list(entity_dict.items()), columns=[\"entity_candidate\", \"entity\"])\n",
    "final_entity_df.to_csv(\"final_entity_dictionary.csv\", index=False)\n",
    "print(\"Saved final entity dictionary to 'final_entity_dictionary.csv'.\")\n",
    "\n",
    "############################\n",
    "# 6. AUTO-LABEL TEXT USING THE ENTITY DICTIONARY (BIO FORMAT)\n",
    "############################\n",
    "\n",
    "def get_entity_label_bio(token, entity_dict, prev_entity):\n",
    "    token_lower = token.lower()\n",
    "    if token_lower in entity_dict:\n",
    "        label = entity_dict[token_lower]\n",
    "    elif token_lower.endswith(\"s\"):\n",
    "        label = entity_dict.get(token_lower[:-1], \"O\")\n",
    "    else:\n",
    "        label = \"O\"\n",
    "    if label == \"O\":\n",
    "        return \"O\", None\n",
    "    # Extract entity type from label (assumes label like \"B-PER\")\n",
    "    entity_type = label.split(\"-\", 1)[-1]\n",
    "    if prev_entity == entity_type:\n",
    "        return f\"I-{entity_type}\", entity_type\n",
    "    else:\n",
    "        return f\"B-{entity_type}\", entity_type\n",
    "\n",
    "def auto_label_bio_using_dict(text, entity_dict):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    data_rows = []\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        prev_entity = None\n",
    "        for token in tokens:\n",
    "            bio_label, current_entity = get_entity_label_bio(token, entity_dict, prev_entity)\n",
    "            data_rows.append({\n",
    "                \"sentence_id\": sent_id,\n",
    "                \"token\": token,\n",
    "                \"ner_label\": bio_label\n",
    "            })\n",
    "            prev_entity = current_entity if bio_label != \"O\" else None\n",
    "    return data_rows\n",
    "\n",
    "all_rows = []\n",
    "doc_id = 0\n",
    "for idx, row in df.iterrows():\n",
    "    labeled_tokens = auto_label_bio_using_dict(row[\"clean_text\"], entity_dict)\n",
    "    for item in labeled_tokens:\n",
    "        all_rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sentence_id\": item[\"sentence_id\"],\n",
    "            \"token\": item[\"token\"],\n",
    "            \"ner_label\": item[\"ner_label\"]\n",
    "        })\n",
    "    doc_id += 1\n",
    "\n",
    "auto_ner_df = pd.DataFrame(all_rows)\n",
    "print(\"Auto-labeled DataFrame shape:\", auto_ner_df.shape)\n",
    "auto_ner_df.to_csv(\"auto_ner_data.csv\", index=False)\n",
    "print(\"Saved auto-labeled NER data to 'auto_ner_data.csv'.\")\n",
    "\n",
    "############################\n",
    "# 7. GROUP TOKENS BY SENTENCE FOR TRAINING\n",
    "############################\n",
    "\n",
    "grouped = auto_ner_df.groupby([\"doc_id\", \"sentence_id\"])\n",
    "examples = []\n",
    "for (doc_id, sent_id), group in grouped:\n",
    "    tokens = group[\"token\"].tolist()\n",
    "    labels = group[\"ner_label\"].tolist()\n",
    "    examples.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"sentence_id\": sent_id,\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": labels\n",
    "    })\n",
    "\n",
    "df_grouped = pd.DataFrame(examples)\n",
    "print(\"Grouped DataFrame shape:\", df_grouped.shape)\n",
    "print(df_grouped.head())\n",
    "\n",
    "# Split into training and validation sets (80/20 split)\n",
    "train_size = int(0.8 * len(df_grouped))\n",
    "train_df = df_grouped.iloc[:train_size]\n",
    "val_df = df_grouped.iloc[train_size:]\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Validation size:\", val_df.shape)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "############################\n",
    "# 8. TOKENIZATION & LABEL ALIGNMENT\n",
    "############################\n",
    "\n",
    "# Define a full BIO label list.\n",
    "label_list = [\"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"]\n",
    "label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, words in enumerate(examples[\"tokens\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        example_labels = examples[\"ner_tags\"][i]\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is None:\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                # If the token is part of the same word as the previous token,\n",
    "                # and the word's label is not \"O\", set it to I- if not first token.\n",
    "                if word_id == previous_word_idx and example_labels[word_id] != \"O\":\n",
    "                    # Convert B- label to I- if needed.\n",
    "                    label = example_labels[word_id]\n",
    "                    if label.startswith(\"B-\"):\n",
    "                        label = \"I-\" + label[2:]\n",
    "                    aligned_labels.append(label2id.get(label, 0))\n",
    "                else:\n",
    "                    aligned_labels.append(label2id.get(example_labels[word_id], 0))\n",
    "                previous_word_idx = word_id\n",
    "        all_labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "print(\"Processed datasets ready for training.\")\n",
    "\n",
    "############################\n",
    "# 9. TRAINING THE NER MODEL\n",
    "############################\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "    for pred_row, label_row in zip(predictions, labels):\n",
    "        tmp_true_labels = []\n",
    "        tmp_true_preds = []\n",
    "        for p_i, l_i in zip(pred_row, label_row):\n",
    "            if l_i == -100:\n",
    "                continue\n",
    "            tmp_true_labels.append(id2label[l_i])\n",
    "            tmp_true_preds.append(id2label[p_i])\n",
    "        if tmp_true_labels:\n",
    "            true_labels.append(tmp_true_labels)\n",
    "            true_preds.append(tmp_true_preds)\n",
    "    if not true_labels:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"accuracy\": 1.0}\n",
    "    results = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results.get(\"overall_precision\", 0.0),\n",
    "        \"recall\": results.get(\"overall_recall\", 0.0),\n",
    "        \"f1\": results.get(\"overall_f1\", 0.0),\n",
    "        \"accuracy\": results.get(\"overall_accuracy\", 1.0)\n",
    "    }\n",
    "\n",
    "# Use a custom data collator to ensure no extra keys are passed.\n",
    "class MinimalDataCollator(DataCollatorForTokenClassification):\n",
    "    def __call__(self, features):\n",
    "        batch = super().__call__(features)\n",
    "        # Remove any extraneous keys if present\n",
    "        for k in list(batch.keys()):\n",
    "            if k not in {\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\", \"special_tokens_mask\"}:\n",
    "                batch.pop(k, None)\n",
    "        return batch\n",
    "\n",
    "data_collator = MinimalDataCollator(tokenizer, padding=True)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"greenlandic_ner_checkpoints\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=50,\n",
    "    fp16=False,  # Disable fp16 on MPS\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"greenlandic_ner_model\")\n",
    "tokenizer.save_pretrained(\"greenlandic_ner_model\")\n",
    "\n",
    "############################\n",
    "# 10. INFERENCE\n",
    "############################\n",
    "\n",
    "from transformers import pipeline\n",
    "ner_infer = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"greenlandic_ner_model\",\n",
    "    tokenizer=\"greenlandic_ner_model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "test_text = \"Nukúnguasik traveled from Ikerssuaq to Nuuk.\"\n",
    "print(\"Inference output:\")\n",
    "print(ner_infer(test_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (51, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   story_id  51 non-null     int64 \n",
      " 1   title     51 non-null     object\n",
      " 2   text      51 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.3+ KB\n",
      "None\n",
      "Duplicate story IDs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAHHCAYAAABXx+fLAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQVBJREFUeJzt3Qd4U/X+x/FvS6FlFpBRkELZe6vsoaCIXAS3iDJEuCKKiCDUwfQKgiAoCMqV4UVkqICCFhGQISCCVEAR2UPZQtll9Pyf7+95kn/SRQNJk+a8X89zbHJycvI7Scz58FsnxLIsSwAAAGwk1N8FAAAAyGwEIAAAYDsEIAAAYDsEIAAAYDsEIAAAYDsEIAAAYDsEIAAAYDsEIAAAYDsEIAAAYDsEIMDLhgwZIiEhIZnyWs2bNzeLww8//GBe+/PPP8+U1+/SpYvExMRIIDt37pw888wzEhUVZd6bPn36+LtISMW+ffvM5/POO+/4uyiwCQIQkI7p06ebH2XHEhERIcWLF5dWrVrJe++9J2fPnvXK6/z9998mOMXHx0ugCeSyZcRbb71lPseePXvK//73P3nqqafS3Pby5csyfvx4qV27tuTLl0/y588vVatWlR49esgff/zh3G7t2rXmPTl9+rQEGg3E1apVk0D1zTffmPcO8LcwfxcAyAqGDRsmpUuXlitXrsiRI0dMTYvWJIwdO1a++uorqVGjhnPb119/XQYOHOhxyBg6dKipTalVq1aGn/fdd9+Jr6VXtilTpkhSUpIEsuXLl0v9+vVl8ODB1932oYcekm+//VY6dOgg3bt3N5+3Bp9FixZJw4YNpVKlSs4ApO+J1oBpSIJnAWjixImEIPgdAQjIgNatW8ttt93mvB8bG2tOrP/617/k/vvvl+3bt0vOnDnNY2FhYWbxpQsXLkiuXLkkR44c4k/Zs2eXQHfs2DGpUqXKdbf7+eefTdD5z3/+I6+++qrbYxMmTPB5bY9el/rSpUvO7xEA36IJDLhBd911l7zxxhuyf/9+mTlzZrp9gJYuXSqNGzc2tQV58uSRihUrOk+yWpt0++23m9tdu3Z1Nrdps41rk8amTZukadOmJvg4npu8D5DDtWvXzDba7yV37twmpB08eNBtG63R0RqM5Fz3eb2ypdYH6Pz58/Lyyy9LdHS0hIeHm2PVfh16gnel+3n++edlwYIF5vh0W21uiouLy3Cw6datmxQtWtQ0TdasWVNmzJiRoj/U3r17ZfHixc6ya1+T1Ozevdv8bdSoUYrHsmXLJrfccovz8+3fv7+5rbWCyfd79epVGT58uJQtW9Yck74/+lkkJia67VPXa4BesmSJCdcafD788ENp1qyZOZbU6Hupza/eoDVdTZo0Md+PvHnzSps2beS3335z20Y/X/2+/vXXX9K+fXtzu3DhwtKvXz/zHXN18uRJ07zoaDrs3Lmz/Prrrym+L1r7o1yblpP76KOPnO+ffv80nLrSWlj9PpYoUcJsU6xYMWnXrl2any2QGmqAgJugP/h6ctOmKG0ySY2eVPREp81k2pSmP9i7du2SH3/80TxeuXJls37QoEGmr4melJQ2ubieXLQW6vHHH5cnn3zSnPTTo7UYemIZMGCACQrjxo2Tli1bmn48ntQwZKRsrjTkaNhasWKFCSfaZKYneA0MehJ999133bZfs2aNfPnll/Lcc8+Zk7D2q9JmqAMHDjgDR2ouXrxoQpq+jxqiNIjMmzfPnGC1pubFF180Zdc+Py+99JI5UWooU3oCT02pUqXM308//dSEoLRq8R588EH5888/5bPPPjPHU6hQIbf9aodrDWIPP/ywec2ffvpJRowYYWoJ58+f77avHTt2mOa2f//73+b7owFHQ4be3rZtm1tfHg0B+rraxHqz9H3RgKJh6u233zY1ipMmTTIhffPmzW6hVoOOblevXj0TZL///nsZM2aMCSjar0ppM2jbtm1lw4YNZp02FS5cuNC8his9Tm1S1X8QaBlSM2vWLNO3TrfV7/CoUaPMe75nzx5njaN+R/T/qxdeeMGUVb/juk/93gR6p3wEEAtAmqZNm6bVFtbPP/+c5jaRkZFW7dq1nfcHDx5snuPw7rvvmvvHjx9Pcx+6f91GXy+5Zs2amccmT56c6mO6OKxYscJse+utt1pnzpxxrp87d65ZP378eOe6UqVKWZ07d77uPtMrmz5f9+OwYMECs+2bb77ptt3DDz9shYSEWLt27XKu0+1y5Mjhtu7XX381699//30rPePGjTPbzZw507nu8uXLVoMGDaw8efK4HbuWr02bNtb1JCUlOd/rokWLWh06dLAmTpxo7d+/P8W2o0ePNtvt3bvXbX18fLxZ/8wzz7it79evn1m/fPlyt3Lpuri4OLdtT58+bUVERFgDBgxwW9+7d28rd+7c1rlz59I9Dj2GqlWrpvn42bNnrfz581vdu3d3W3/kyBHzXXZdr5+vlnHYsGFu2+r3vW7dus77X3zxhdlOPxeHa9euWXfddVeK706vXr3c/v9w0PdS199yyy3WP//841y/cOFCs/7rr78290+dOmXu62cA3AyawICbpP9iT280mKOTrP6L+EY7DGutkVb5Z1SnTp1MjYqD1kZoM4F2QPUl3b82F/Xu3dttvdaEaObRZhdXWiulNQkOWkumTSj6r/3rvY4272ntiYPWDujr6rD3lStXelx2rW3Q2qo333xTChQoYGp4evXqZWqGHnvssQz1AXK8v3379nVb76h90qY4V1pzlbxJKzIy0jTn6Os7mg21FmbOnDmmGUqbrG6G1pToseh7d+LECeein5vW8mjtXXLPPvus232tCXT9jLTZUt9/11rQ0NBQ8/55St9rff9dX0s5Xk9rMLXvmzZxnjp1yuP9Aw4EIOAm6QnXNWyk9oOuTSraNKJNV9qMNXfuXI/C0K233upRh+fy5cunOLmXK1fO530ktD+UThOQ/P3Q5ijH465KliyZYh968rveiU33o8eoJ9mMvI4nQfO1114zzVXaVKMhREeQ6eelTW3Xo6+rZdL32pWGNQ3CyculASitAKvNOatXrzb3tdnp6NGj6Q7hz6idO3c6+7Bps53rok252pzkSvtXJW82TP4Z6XFpwNb+aa6Svw8Zkfw74QhDjtfTz0ib7TRM6/9P2i9Om8m0XxDgCQIQcBMOHTokCQkJ6f7Q679YV61aZU5iegLbsmWLCUV33313io6k6e3D29KarDGjZfIGrXVITfIO0/6gJ3QNq/rZadjSEKQdnDMioxNhpvW5aq2Qntwdnev1r4YorTG7WY7grX1wtDYo+aI1lRn5jPz5ndApKLQ/lPat0oCmgxE0/Gr/JSCjCEDATXB05LzeyBytFWjRooWZN+j33383nZR1GL2jucHbM0c7/pXvevLQDsOuHUT1X9apNeskr6XwpGzaXKQ1J8mbBB2TCDo6Gt8s3Y8eY/JaNG+/jtKmHW2a0zmBtKkovfdEX1fLlPz919obfa8zWi4NAU888YSZ0VtrPnSknDZZeSOMOJocixQpYgJV8iW1UYXXo8d1+PBh05nalX7nkvPWd12PQ5sWtdZKO4zrJJbaORvIKAIQcIM0wOhwZ23G6NixY5rb/fPPPynWOSYUdAyNdvTr8NZcM5988olbCNETqZ6gdCSZ6wlk/fr15sThoPPgJB8u70nZ7rvvPlODpPPmuNLRUnric339m6Gvo00e2i/GQWtn3n//fdMnS4eSe0pDizY7JafHvW7dOhMYHU1Bab0nWi6lo+5cafBVOtQ8o7S2UMOPjobSZlYd/ecNGta1n5XOkK2hLrnjx4/f0D51XzoxpoMGQceQd1c3+13XkKXzJbnS77I2uyafagBID8PggQzQ/gZau6AnWf3XvIYfbS7Qf/nqTNBaDZ8WHUauzSh68tPttY/FBx98YIZm67Bjxw+49hGZPHmy+SHXk4R2SE2rj8j1FCxY0OxbO05refWErM10rp1UtU+SBqN7771XHn30UTMPjja1uHZK9rRsOhT6zjvvNP1otL+Rzmej/0LXZhVttki+7xulQ/J1zhwd9q7zI2nNlh6LTi2gx5pen6y06Jw1WuuiIU073up7qEP3dUi71mrpfh01MHXr1jV/9Ti1mUxrifTY9Xh16LfOY6MneA1iOjRc96EdmPW9ySi9HIcOg9fh/dq8U6dOnQw/V0OMduZOzhHWdci7Bizdp5Zfg52GP+2krf3VkgfY69Fju+OOO0yNjNb66DB4/f/CEf5da30c7512WNfgpO+pliGjtOlLa1P1O6sTXOp0BTq9gH7PPdkPwDB4IAPD4B2LDtuOioqy7r77bjOk3HW4dVrD4JctW2a1a9fOKl68uHm+/tUh1n/++afb83S4b5UqVaywsDC3ocPpDWtOaxj8Z599ZsXGxlpFihSxcubMaYaBpzace8yYMWbIfHh4uNWoUSNr48aNKfaZXtmSD4N3DLN+6aWXzHFmz57dKl++vBmyrMPMXel+dEh0cmkNz0/u6NGjVteuXa1ChQqZ97V69eqpDtXP6DB43d/IkSPNsRcrVswca4ECBcxQ7s8//zzF9sOHDzfvXWhoqNuQ+CtXrlhDhw61SpcubY4/OjrafBaXLl3yuFyjRo0y+37rrbesjHIM5U9tadGihdt3pVWrVmbouw67L1u2rNWlSxfzHXDQz0GH3l/vO650mocnnnjCyps3r9mn7uvHH380282ePdu53dWrV60XXnjBKly4sJkawbEfxzD41Ia363p9TXXixAnzvalUqZIpm75WvXr1zFQPgCdC9D/+DmEAgJT0wqw6kaPWpqU2Yi7Qad+lBx54wEx4mdoM24A/EYAAIADpT7M2qemM2KnNzRNodHZu11Ft2hfsnnvukY0bN5r+WlzjDIGGPkAAEED0Wmraf0ZDz9atW1MMSw9UelkKDUENGjQwnZH1Eidr1641na0JPwhE1AABQADR5i7trKwdz/UaaTplQlag1/DSYejaCVpHaWmne70uWEYmkAT8gQAEAABsh3mAAACA7RCAAACA7dAJOhU6g6lOfKaTqXn7EgUAAMA3tFePzoKvF2VOfrHk5AhAqdDwEx0d7e9iAACAG6CX9NHZ9tNDAEqFYxp9fQP1mjkAACDwnTlzxlRgZORyOASgVDiavTT8EIAAAMhaMtJ9hU7QAADAdghAAADAdghAAADAdghAAADAdghAAADAdghAAADAdghAAADAdghAAADAdghAAADAdghAAADAdghAAADAdghAAADAdghAAADAdghAAADAdghAAADAdsL8XQB4T8zAxT7b976RbXy2bwAAMhs1QAAAwHYIQAAAwHYIQAAAwHYIQAAAwHYIQAAAwHYIQAAAwHYIQAAAwHYIQAAAwHYIQAAAwHYIQAAAwHYIQAAAwHb8GoBGjBght99+u+TNm1eKFCki7du3lx07drhtc+nSJenVq5fccsstkidPHnnooYfk6NGj6e7XsiwZNGiQFCtWTHLmzCktW7aUnTt3+vhoAABAVuHXALRy5UoTbtavXy9Lly6VK1euyD333CPnz593bvPSSy/J119/LfPmzTPb//333/Lggw+mu99Ro0bJe++9J5MnT5affvpJcufOLa1atTJhCgAAIMTS6pIAcfz4cVMTpEGnadOmkpCQIIULF5ZZs2bJww8/bLb5448/pHLlyrJu3TqpX79+in3o4RQvXlxefvll6devn1mn+ylatKhMnz5dHn/88euW48yZMxIZGWmely9fPskquBo8AMDOznhw/g6oPkBaYFWwYEHzd9OmTaZWSJuwHCpVqiQlS5Y0ASg1e/fulSNHjrg9R9+MevXqpfmcxMRE86a5LgAAIHgFTABKSkqSPn36SKNGjaRatWpmnQaZHDlySP78+d221docfSw1jvW6TUafo32RNCQ5lujoaC8dFQAACEQBE4C0L9C2bdtk9uzZmf7asbGxpvbJsRw8eDDTywAAAGwWgJ5//nlZtGiRrFixQkqUKOFcHxUVJZcvX5bTp0+7ba+jwPSx1DjWJx8plt5zwsPDTVuh6wIAAIKXXwOQdljW8DN//nxZvny5lC5d2u3xunXrSvbs2WXZsmXOdTpM/sCBA9KgQYNU96n70KDj+hzt06OjwdJ6DgAAsJdQfzd7zZw504zy0rmAtI+OLhcvXjSPa3+cbt26Sd++fU3tkHaK7tq1qwkyriPAtGO0higVEhJi+hK9+eab8tVXX8nWrVulU6dOZmSYzjMEAAAQ5s8XnzRpkvnbvHlzt/XTpk2TLl26mNvvvvuuhIaGmgkQdbSWzufzwQcfuG2vtUKOEWTqlVdeMXMJ9ejRwzSfNW7cWOLi4iQiIiJTjgsAAAS2gJoHKFAwD1BKzAMEAAh0WXYeIAAAgMxAAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALbj1wC0atUqadu2rRQvXlxCQkJkwYIFbo/rutSW0aNHp7nPIUOGpNi+UqVKmXA0AAAgq/BrADp//rzUrFlTJk6cmOrjhw8fdlumTp1qAs1DDz2U7n6rVq3q9rw1a9b46AgAAEBWFObPF2/durVZ0hIVFeV2f+HChXLnnXdKmTJl0t1vWFhYiucCAABkuT5AR48elcWLF0u3bt2uu+3OnTtNs5oGpY4dO8qBAwfS3T4xMVHOnDnjtgAAgOCVZQLQjBkzJG/evPLggw+mu129evVk+vTpEhcXJ5MmTZK9e/dKkyZN5OzZs2k+Z8SIERIZGelcoqOjfXAEAAAgUGSZAKT9f7Q2JyIiIt3ttEntkUcekRo1akirVq3km2++kdOnT8vcuXPTfE5sbKwkJCQ4l4MHD/rgCAAAQKDwax+gjFq9erXs2LFD5syZ4/Fz8+fPLxUqVJBdu3aluU14eLhZAACAPWSJGqCPP/5Y6tata0aMeercuXOye/duKVasmE/KBgAAsh6/BiANJ/Hx8WZR2l9Hb7t2WtYOyfPmzZNnnnkm1X20aNFCJkyY4Lzfr18/Wblypezbt0/Wrl0rDzzwgGTLlk06dOiQCUcEAACyAr82gW3cuNEMa3fo27ev+du5c2fTkVnNnj1bLMtKM8Bo7c6JEyec9w8dOmS2PXnypBQuXFgaN24s69evN7cBAABUiKXpAm601klHg2mH6Hz58klWETNwsc/2vW9kG5/tGwCAzD5/Z4k+QAAAAN5EAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALZDAAIAALYT5u8C2FHMwMX+LgIAALZGDRAAALAdAhAAALAdAhAAALAdAhAAALAdAhAAALAdAhAAALAdAhAAALAdAhAAALAdAhAAALAdAhAAALAdAhAAALAdvwagVatWSdu2baV48eISEhIiCxYscHu8S5cuZr3rcu+99153vxMnTpSYmBiJiIiQevXqyYYNG3x4FAAAIKvxawA6f/681KxZ0wSWtGjgOXz4sHP57LPP0t3nnDlzpG/fvjJ48GD55ZdfzP5btWolx44d88ERAACArMivV4Nv3bq1WdITHh4uUVFRGd7n2LFjpXv37tK1a1dzf/LkybJ48WKZOnWqDBw48KbLDAAAsr6A7wP0ww8/SJEiRaRixYrSs2dPOXnyZJrbXr58WTZt2iQtW7Z0rgsNDTX3161bl+bzEhMT5cyZM24LAAAIXgEdgLT565NPPpFly5bJ22+/LStXrjQ1RteuXUt1+xMnTpjHihYt6rZe7x85ciTN1xkxYoRERkY6l+joaK8fCwAACBx+bQK7nscff9x5u3r16lKjRg0pW7asqRVq0aKF114nNjbW9Bty0BogQhAAAMEroGuAkitTpowUKlRIdu3alerj+li2bNnk6NGjbuv1fnr9iLSfUb58+dwWAAAQvLJUADp06JDpA1SsWLFUH8+RI4fUrVvXNJk5JCUlmfsNGjTIxJICAIBA5tcAdO7cOYmPjzeL2rt3r7l94MAB81j//v1l/fr1sm/fPhNi2rVrJ+XKlTPD2h20KWzChAnO+9qUNWXKFJkxY4Zs377ddJzW4faOUWEAAAA33QdI+8ssX77cjNKqXLmyR8/duHGj3Hnnnc77jn44nTt3lkmTJsmWLVtMkDl9+rSZLPGee+6R4cOHmyYrh927d5vOzw6PPfaYHD9+XAYNGmQ6PteqVUvi4uJSdIwGAAD2FWJZluXJEx599FFp2rSpPP/883Lx4kUz0aDW0OhuZs+eLQ899JBkdRrqdDRYQkKCT/oDxQxcLFnNvpFt/F0EAAC8dv4OvZHLVzRp0sTcnj9/vgk+WkPz3nvvyZtvvunp7gAAADKdxwFIU1XBggXNbW1a0hqfXLlySZs2bWTnzp2+KCMAAIB/A5DOj6OzKmvHYg1A2i9HnTp1ylx8FAAAIOg6Qffp00c6duwoefLkkZIlS0rz5s2dTWM6WSEAAEDQBaDnnntO7rjjDjl48KDcfffd5lpbjkkK6QMEAACCdhj8bbfdZi5LofP26KUpwsLCTB8gAACAoOwDdOHCBenWrZvp+Fy1alUzaaF64YUXZOTIkb4oIwAAgH8DkF449NdffzUXJHXt9NyyZUuZM2eOd0sHAAAQCE1gCxYsMEGnfv36EhIS4lyvtUE6KzMAAEDQ1QDpZSaKFCmSYr0Oi3cNRAAAAEETgLQD9OLF/38pB0fo+e9//8sV1wEAQHA2gb311lvSunVr+f333+Xq1asyfvx4c3vt2rWycuVK35QSAADAnzVAjRs3lvj4eBN+dOLD7777zjSJ6ezQdevW9WbZAAAAAmceIJ37Z8qUKd4vDQAAQKAEIL28vOOy8no7Pde7/DwAAECWCEAFChSQw4cPm6au/Pnzpzray7Iss/7atWu+KCcAAEDmBqDly5dLwYIFze0VK1Z479UBAAACNQA1a9bM/NWOzzrS6+mnn5YSJUr4umwAAAD+HwWmFz0dPXq0CUIAAAC2GQZ/1113Md8PAACw1zB4nQRx4MCBsnXrVjPvT+7cud0ev//++71ZPgAAAP8HoOeee878HTt2bIrHGAUGAACCMgAlJSX5piQAAACB2gcIAADAlgFIO0G3bdtWypUrZxbt97N69Wrvlw4AACAQAtDMmTOlZcuWkitXLundu7dZcubMKS1atJBZs2b5oowAAABeFWLpNSw8ULlyZenRo4e89NJLbuu1U7ReIHX79u2S1en1ziIjIyUhIcEn1zaLGbhYspp9I9v4uwgAAHjt/O1xDdCePXtM81dy2gy2d+9eT3cHAACQ6TwOQNHR0bJs2bIU67///nvzGAAAQNANg3/55ZdNv5/4+Hhp2LChWffjjz/K9OnTZfz48b4oIwAAgH8DUM+ePSUqKkrGjBkjc+fOdfYLmjNnjrRr1867pQMAAAiUYfAPPPCArFmzRk6ePGkWvX0j4WfVqlWmP1Hx4sXNLNILFixwPnblyhUZMGCAVK9e3VxuQ7fp1KmT/P333+nuc8iQIWZfrkulSpVu5DABAECQ8jgAlSlTxoSe5E6fPm0e88T58+elZs2aMnHixBSPXbhwQX755Rd54403zN8vv/xSduzYkaFrjVWtWlUOHz7sXDSgAQAA3HAT2L59+1K93ldiYqL89ddfHl9YVZfU6DC2pUuXuq2bMGGC3HHHHXLgwAEpWbJkmvsNCwszzXQAAAA3FYC++uor5+0lS5aYgOKggUhHhsXExIgv6bh+bdLKnz9/utvt3LnTNJlFRERIgwYNZMSIEekGJg1vurjOIwAAAIJXhgNQ+/btzV8NIJ07d3Z7LHv27Cb8aMdoX7l06ZLpE9ShQ4d0JzeqV6+eGZFWsWJF0/w1dOhQadKkiWzbtk3y5s2b6nM0IOl2AADAHsI8vQp86dKl5eeff5ZChQpJZtEO0Y8++qjopNWTJk1Kd1vXJrUaNWqYQFSqVCkzYq1bt26pPic2Nlb69u3rVgPEnEYAAAQvj/sAZfZsz47ws3//flm+fLnHl6bQ5rIKFSrIrl270twmPDzcLAAAwB4yPAps3bp1smjRIrd1n3zyiakRKlKkiLk+mGs/Gm+GH+3TozNN33LLLR7v49y5c7J7924pVqyYV8sGAABsEICGDRsmv/32m/P+1q1bTZOSXhl+4MCB8vXXX5u+NJ6GE51RWhdH7ZLe1lFeGn4efvhh2bhxo3z66aemo/WRI0fMcvnyZec+9Cr0OjrMoV+/frJy5UozWm3t2rVmzqJs2bKZvkMAAAAeNYFpMBk+fLjz/uzZs03/Gr0CvNI+M4MHDzYTEWaUhps777zTed/RD0c7Wet+HCPPatWq5fa8FStWSPPmzc1trd05ceKE87FDhw6ZsKNzFRUuXFgaN24s69evN7cBAAA8CkCnTp2SokWLOu9rLYtrh+Pbb79dDh486NG7qiFGOzanJb3HHLSmx5UGMwAAAK80gWn4cXSA1iYonZ25fv36zsfPnj1rhsMDAAAETQC67777TF+f1atXm2HjuXLlMvPrOGzZskXKli3rq3ICAABkfhOY9v958MEHpVmzZpInTx6ZMWOG5MiRw/n41KlT5Z577vFeyQAAAPwdgHTiQ716u16OQgOQjqxyNW/ePLMeAAAg6CZCdL0GmKuCBQt6ozwAAACB0wcIAAAgWBCAAACA7RCAAACA7WQoANWpU8dMhOi4JMaFCxd8XS4AAAD/BqDt27fL+fPnze2hQ4eaa3gBAAAE9SgwvRZX165dzXW19PIU77zzTppD3gcNGuTtMgIAAGR+AJo+fbq50OmiRYskJCREvv32WwkLS/lUfYwABAAAgiIAVaxY0XmR0dDQUFm2bJkUKVLE12UDAAAIjIkQk5KSfFMSAACAQA1Aavfu3TJu3DjTOVpVqVJFXnzxRS6GCgAAgnMeoCVLlpjAs2HDBqlRo4ZZfvrpJ6lataosXbrUN6UEAADwZw3QwIED5aWXXpKRI0emWD9gwAC5++67vVk+AAAA/9cAabNXt27dUqx/+umn5ffff/dWuQAAAAInABUuXFji4+NTrNd1jAwDAABB2QTWvXt36dGjh+zZs0caNmxo1v3444/y9ttvS9++fX1RRgAAAP8GoDfeeEPy5s0rY8aMkdjYWLOuePHiMmTIEOndu7d3SwcAABAIAUhne9ZO0LqcPXvWrNNABAAAENTzADkQfAAAgC06QQMAAGR1BCAAAGA7BCAAAGA7HgWgK1euSIsWLWTnzp2+KxEAAEAgBaDs2bPLli1bfFcaAACAQGwCe/LJJ+Xjjz/2TWkAAAACcRj81atXZerUqfL9999L3bp1JXfu3G6Pjx071pvlAwAA8H8A2rZtm9SpU8fc/vPPP1NMkggAABB0TWArVqxIc1m+fLlH+1q1apW0bdvWXEpDw9OCBQvcHrcsSwYNGiTFihWTnDlzSsuWLTPUAXvixIkSExMjERERUq9ePdmwYYOnhwkAAILYDQ+D37VrlyxZskQuXrzoDCueOn/+vNSsWdMEltSMGjVK3nvvPZk8ebL89NNPprmtVatWcunSpTT3OWfOHHNR1sGDB8svv/xi9q/POXbsmMflAwAAwcnjAHTy5EkzFL5ChQpy3333yeHDh836bt26ycsvv+zRvlq3bi1vvvmmPPDAAyke00A1btw4ef3116Vdu3ZSo0YN+eSTT+Tvv/9OUVOUvA+SXrG+a9euUqVKFROecuXKZfotAQAA3FAA0oug6nD4AwcOmGDh8Nhjj0lcXJzX3tW9e/fKkSNHTLOXQ2RkpGnSWrduXarPuXz5smzatMntOaGhoeZ+Ws9RiYmJcubMGbcFAAAEL48D0HfffSdvv/22lChRwm19+fLlZf/+/V4rmIYfVbRoUbf1et/xWHInTpyQa9euefQcNWLECBOuHEt0dLRXjgEAAARJANJ+O641Pw7//POPhIeHS1YUGxsrCQkJzuXgwYP+LhIAAAikANSkSRPTF8dBR28lJSWZDst33nmn1woWFRVl/h49etRtvd53PJZcoUKFJFu2bB49R2lwy5cvn9sCAACCl8cBSIPORx99ZDowa5+bV155RapVq2aGtGvTmLeULl3ahJZly5Y512nfHB0N1qBBg1SfkyNHDjM5o+tzNJzp/bSeAwAA7MfjAKRhRydAbNy4sRmdpU1iDz74oGzevFnKli3r0b7OnTsn8fHxZnF0fNbb2sFaa5b69OljRol99dVXsnXrVunUqZOZM6h9+/bOfeiItAkTJjjv6xD4KVOmyIwZM2T79u3Ss2dPU0YdFQYAAHBDM0Er7Sj82muv3fQ7uHHjRrdmMw0vqnPnzjJ9+nRTu6ThpUePHnL69GkTunSkmU5w6LB7927T+dl1NNrx48fNBIra8blWrVrmOck7RgMAAPsKsW5gBsNTp06ZC6JqDYvS+Xa0hqVgwYISDLSpTUOedoj2RX+gmIGLJavZN7KNv4sAAIDXzt8eN4FpXx+9zITO0KxBSBe9rX129DEAAICgawLr1auXaWaaNGmSGXGldO6d5557zjymfXUAAAACWeiNXANML3nhCD9Kb2v/HX0MAAAg6AJQnTp1nH1/XOk6vfAoAABAUDSBbdmyxXm7d+/e8uKLL5ranvr165t169evN1d0HzlypO9KCgAAkJmjwPSCojovz/U21W20P1BWxyiwlBgFBgAIpvN3hmqAdIJCAACAYJGhAFSqVCnflwQAACCQZ4L++++/Zc2aNXLs2DFzrS1X2kcIAAAgqAKQXqLi3//+t7nw6C233GL6/TjobQIQAAAIugD0xhtvmOtsxcbGms7RAAAAWY3HCebChQvy+OOPE34AAECW5XGK6datm8ybN883pQEAAAjEJrARI0bIv/71L4mLi5Pq1atL9uzZ3R4fO3asN8sHAAAQGAFoyZIlUrFiRXM/eSdoAACAoAtAY8aMkalTp0qXLl18UyIAAIBA6wMUHh4ujRo18k1pAAAAAjEA6YVQ33//fd+UBgAAIBCbwDZs2CDLly+XRYsWSdWqVVN0gv7yyy+9WT4AAAD/B6D8+fPLgw8+6P2SAAAABGoAmjZtmm9KAgAAkEmYzhkAANiOxzVApUuXTne+nz179txsmQAAAAIrAPXp08ft/pUrV2Tz5s1mZuj+/ft7s2wAAACBEYB0GHxqJk6cKBs3bvRGmQAAALJGH6DWrVvLF1984a3dAQAABH4A+vzzz6VgwYLe2h0AAEDgNIHVrl3brRO0ZVly5MgROX78uHzwwQfeLh8AAID/A1D79u3d7oeGhkrhwoWlefPmUqlSJW+WDQAAwCc8DkCDBw/2TUkAAAAyCRMhAgAA28lwANKmrmzZsqW7hIV5XKF0XTExMabPUfKlV69eqW4/ffr0FNtGRER4vVwAACDrynBimT9/fpqPrVu3Tt577z1JSkoSb/v555/l2rVrzvvbtm2Tu+++Wx555JE0n5MvXz7ZsWOH8356M1cDAAD7yXAAateuXYp1GjIGDhwoX3/9tXTs2FGGDRvm7fKZDtauRo4cKWXLlpVmzZql+RwNPFFRUV4vCwAAsHEfoL///lu6d+8u1atXl6tXr0p8fLzMmDFDSpUqJb50+fJlmTlzpjz99NPp1uqcO3fOlCU6OtoEt99++y3d/SYmJsqZM2fcFgAAELw8CkAJCQkyYMAAKVeunAkVy5YtM7U/1apVk8ywYMECOX36tHTp0iXNbSpWrChTp06VhQsXmrCkzXINGzaUQ4cOpfmcESNGSGRkpHPR4AQAAIJXiKUzGWbAqFGj5O233zZNS2+99VaqTWK+1qpVK8mRI4cJXRmlF2utXLmydOjQQYYPH55mDZAuDloDpCFIA5/2J/K2mIGLJavZN7KNv4sAAEC69PytFRkZOX9nuA+Q9vXJmTOnqf3R5i5dUvPll1+KL+zfv1++//57j/efPXt2M3v1rl270twmPDzcLAAAwB4yHIA6derk19FU06ZNkyJFikibNp7VROgIsq1bt8p9993ns7IBAIAgDUA6v46/aD8eDUCdO3dOMdeQBrNbb73V9ONROhKtfv36pqZK+wuNHj3a1B4988wzfio9AAAINN6fudAHtOnrwIEDZvRXcrpeJ2l0OHXqlBmhphdoLVCggNStW1fWrl0rVapUyeRSAwCALN8J2k486UR1I+gEDQCAf8/fXAsMAADYDgEIAADYDgEIAADYDgEIAADYDgEIAADYDgEIAADYDgEIAADYDgEIAADYDgEIAADYDgEIAADYDgEIAADYDgEIAADYDgEIAADYDgEIAADYDgEIAADYDgEIAADYDgEIAADYDgEIAADYTpi/C4CsIWbgYp/sd9/INj7ZLwAA6aEGCAAA2A4BCAAA2A4BCAAA2A4BCAAA2A4BCAAA2A4BCAAA2A4BCAAA2A4BCAAA2A4BCAAA2A4BCAAA2A4BCAAA2A4BCAAA2E5AB6AhQ4ZISEiI21KpUqV0nzNv3jyzTUREhFSvXl2++eabTCsvAADIGgI6AKmqVavK4cOHncuaNWvS3Hbt2rXSoUMH6datm2zevFnat29vlm3btmVqmQEAQGAL+AAUFhYmUVFRzqVQoUJpbjt+/Hi59957pX///lK5cmUZPny41KlTRyZMmJCpZQYAAIEt4APQzp07pXjx4lKmTBnp2LGjHDhwIM1t161bJy1btnRb16pVK7M+PYmJiXLmzBm3BQAABK8wCWD16tWT6dOnS8WKFU3z19ChQ6VJkyamSStv3rwptj9y5IgULVrUbZ3e1/XpGTFihNk3Ml/MwMU+2/e+kW18tm8AQNYW0DVArVu3lkceeURq1KhhanK0Q/Pp06dl7ty5Xn2d2NhYSUhIcC4HDx706v4BAEBgCegaoOTy588vFSpUkF27dqX6uPYROnr0qNs6va/r0xMeHm4WAABgDwFdA5TcuXPnZPfu3VKsWLFUH2/QoIEsW7bMbd3SpUvNegAAgCwRgPr16ycrV66Uffv2mSHuDzzwgGTLls0MdVedOnUyzVcOL774osTFxcmYMWPkjz/+MPMIbdy4UZ5//nk/HgUAAAg0Ad0EdujQIRN2Tp48KYULF5bGjRvL+vXrzW2lI8JCQ/8/wzVs2FBmzZolr7/+urz66qtSvnx5WbBggVSrVs2PRwEAAAJNiGVZlr8LEWh0GHxkZKTpEJ0vX74sNfIJ/49RYABgL2c8OH8HdBMYAACALxCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7QR0ABoxYoTcfvvtkjdvXilSpIi0b99eduzYke5zpk+fLiEhIW5LREREppUZAAAEvoAOQCtXrpRevXrJ+vXrZenSpXLlyhW555575Pz58+k+L1++fHL48GHnsn///kwrMwAACHxhEsDi4uJS1O5oTdCmTZukadOmaT5Pa32ioqIyoYQAACArCugaoOQSEhLM34IFC6a73blz56RUqVISHR0t7dq1k99++y3d7RMTE+XMmTNuCwAACF5ZJgAlJSVJnz59pFGjRlKtWrU0t6tYsaJMnTpVFi5cKDNnzjTPa9iwoRw6dCjdvkaRkZHORYMTAAAIXiGWZVmSBfTs2VO+/fZbWbNmjZQoUSLDz9N+Q5UrV5YOHTrI8OHD06wB0sVBa4A0BGmNk/Yn8raYgYu9vk+ktG9kG38XAQCQifT8rRUZGTl/B3QfIIfnn39eFi1aJKtWrfIo/Kjs2bNL7dq1ZdeuXWluEx4ebhYAAGAPAd0EppVTGn7mz58vy5cvl9KlS3u8j2vXrsnWrVulWLFiPikjAADIegK6BkiHwM+aNcv059G5gI4cOWLWa/VWzpw5ze1OnTrJrbfeavrxqGHDhkn9+vWlXLlycvr0aRk9erQZBv/MM8/49VgAAEDgCOgANGnSJPO3efPmbuunTZsmXbp0MbcPHDggoaH/X5F16tQp6d69uwlLBQoUkLp168ratWulSpUqmVx6AAAQqLJMJ+hA7UR1I+gEnTnoBA0A9nLGg/N3QPcBAgAA8AUCEAAAsB0CEAAAsB0CEAAAsB0CEAAAsB0CEAAAsB0CEAAAsB0CEAAAsB0CEAAAsB0CEAAAsB0CEAAAsB0CEAAAsB0CEAAAsB0CEAAAsB0CEAAAsB0CEAAAsB0CEAAAsB0CEAAAsJ0wfxcA8JWYgYt9st99I9uIr2TFMgMIXjFB/JtEDRAAALAdAhAAALAdAhAAALAdAhAAALAdAhAAALAdAhAAALAdAhAAALAdAhAAALAdAhAAALAdAhAAALAdAhAAALCdLBGAJk6cKDExMRIRESH16tWTDRs2pLv9vHnzpFKlSmb76tWryzfffJNpZQUAAIEv4APQnDlzpG/fvjJ48GD55ZdfpGbNmtKqVSs5duxYqtuvXbtWOnToIN26dZPNmzdL+/btzbJt27ZMLzsAAAhMAR+Axo4dK927d5euXbtKlSpVZPLkyZIrVy6ZOnVqqtuPHz9e7r33Xunfv79UrlxZhg8fLnXq1JEJEyZketkBAEBgCugAdPnyZdm0aZO0bNnSuS40NNTcX7duXarP0fWu2yutMUprewAAYD9hEsBOnDgh165dk6JFi7qt1/t//PFHqs85cuRIqtvr+rQkJiaaxSEhIcH8PXPmjPhCUuIFn+wXmcNX3wtffjd8WWYAwSspi/0mOfZrWVbWDkCZZcSIETJ06NAU66Ojo/1SHgS2yHGS5WTFMgMIXpE+/k06e/asREZGZt0AVKhQIcmWLZscPXrUbb3ej4qKSvU5ut6T7VVsbKzpaO2QlJQk//zzj9xyyy0SEhJyQwlUw9PBgwclX758YgccM8ccjOx2vIpj5pizMq350fBTvHjx624b0AEoR44cUrduXVm2bJkZyeUIJ3r/+eefT/U5DRo0MI/36dPHuW7p0qVmfVrCw8PN4ip//vw3XX79UgXTFysjOGZ7sNsx2+14FcdsD/mC8JivV/OTJQKQ0pqZzp07y2233SZ33HGHjBs3Ts6fP29GhalOnTrJrbfeapqx1IsvvijNmjWTMWPGSJs2bWT27NmyceNG+eijj/x8JAAAIFAEfAB67LHH5Pjx4zJo0CDTkblWrVoSFxfn7Oh84MABMzLMoWHDhjJr1ix5/fXX5dVXX5Xy5cvLggULpFq1an48CgAAEEgCPgApbe5Kq8nrhx9+SLHukUceMYu/aHOaTtyYvFktmHHM9mC3Y7bb8SqO2R7CbXjMyYVYGRkrBgAAEEQCeiJEAAAAXyAAAQAA2yEAAQAA2yEAAQAA2yEA+cDEiRMlJiZGIiIipF69erJhwwbJClatWiVt27Y1M2jqDNg6fYAr7S+v0xEUK1ZMcubMaS46u3PnTrdtdAbtjh07mom1dDLJbt26yblz59y22bJlizRp0sS8PzoT6ahRo8QfdO6o22+/XfLmzStFihQxk23u2LHDbZtLly5Jr169zKzgefLkkYceeijFTOM6FYPOOZUrVy6zn/79+8vVq1dTjFasU6eOGXFRrlw5mT59uvjDpEmTpEaNGs7Jz3SC0G+//TZojzc1I0eONN9v18lSg+24hwwZYo7RdalUqVLQHq/666+/5MknnzTHpL9P1atXN3PABevvl55jkn/GuujnGqyfsdfpKDB4z+zZs60cOXJYU6dOtX777Tere/fuVv78+a2jR49age6bb76xXnvtNevLL7/UkYHW/Pnz3R4fOXKkFRkZaS1YsMD69ddfrfvvv98qXbq0dfHiRec29957r1WzZk1r/fr11urVq61y5cpZHTp0cD6ekJBgFS1a1OrYsaO1bds267PPPrNy5sxpffjhh1Zma9WqlTVt2jRTjvj4eOu+++6zSpYsaZ07d865zbPPPmtFR0dby5YtszZu3GjVr1/fatiwofPxq1evWtWqVbNatmxpbd682byHhQoVsmJjY53b7Nmzx8qVK5fVt29f6/fff7fef/99K1u2bFZcXFymH/NXX31lLV682Przzz+tHTt2WK+++qqVPXt28x4E4/Emt2HDBismJsaqUaOG9eKLLzrXB9txDx482Kpatap1+PBh53L8+PGgPd5//vnHKlWqlNWlSxfrp59+MmVbsmSJtWvXrqD9/Tp27Jjb57t06VLzu71ixYqg/Ix9gQDkZXfccYfVq1cv5/1r165ZxYsXt0aMGGFlJckDUFJSkhUVFWWNHj3aue706dNWeHi4+RFQ+j+IPu/nn392bvPtt99aISEh1l9//WXuf/DBB1aBAgWsxMRE5zYDBgywKlasaPmb/qBo+VeuXOk8Pg0H8+bNc26zfft2s826devMff3RCA0NtY4cOeLcZtKkSVa+fPmcx/jKK6+Yk5Grxx57zASwQKCfx3//+9+gP96zZ89a5cuXNyeKZs2aOQNQMB63BiA9kacmGI9Xf0MaN26c5uN2+P3S73PZsmXNsQbjZ+wLNIF50eXLl2XTpk2matVBZ6nW++vWrZOsbO/evWYmbtdj0+utaBOf49j0r1Yb62VLHHR7fQ9++ukn5zZNmzY113lzaNWqlWl6OnXqlPhTQkKC+VuwYEHzVz/LK1euuB2zNiOULFnS7Zi1qt0xM7njePRCg7/99ptzG9d9OLbx93fi2rVr5lIxemkZbQoL9uPV5gCt7k9etmA9bm3e0ebsMmXKmGYdbe4I1uP96quvzO+OToCrTTm1a9eWKVOm2Ob3S889M2fOlKeffto0gwXjZ+wLBCAvOnHihDmpuH6hlN7X//myMkf50zs2/as/Pq7CwsJMoHDdJrV9uL6GP+hFdrVPSKNGjZyXTdHy6A9d8gvjJj/m6x1PWtvoD83Fixcls23dutX0CdA2/WeffVbmz58vVapUCdrjVRr0fvnlF+c1A10F43HriV37auhlg7TflwYA7beiV8kOxuPds2ePOU699NGSJUukZ8+e0rt3b5kxY4Ytfr+0v+bp06elS5cuzrIE22ds20thAJlRO7Bt2zZZs2aNBLuKFStKfHy8qfH6/PPPzcWGV65cKcHq4MGD5iLJS5cuNR1X7aB169bO29rpXQNRqVKlZO7cuaYDcLDRf8Bozc1bb71l7msNkP7/PHnyZPP9DnYff/yx+cy1xg8ZRw2QFxUqVEiyZcuWoqe93o+KipKszFH+9I5N/x47dsztcR1RoCMrXLdJbR+ur5HZ9DpzixYtkhUrVkiJEiWc67U8WrWs/7JK75ivdzxpbaMjTfxxMtJ/Gepojrp165oakZo1a8r48eOD9ni1OUC/lzqSRf9Fr4sGvvfee8/c1n/RBuNxu9KagAoVKsiuXbuC8nPWkV1ai+mqcuXKzma/YP792r9/v3z//ffyzDPPONcF42fsCwQgL59Y9KSybNkyt3+Z6H3tY5GVlS5d2vzP4HpsWg2qbeOOY9O/+j+cnnAcli9fbt4D/ReoYxsdbq/t0w76L3OtlShQoECmHpP29dbwo01AWk49Rlf6WWbPnt3tmLWtX39UXY9Zm5Rcfzj1ePQHwvGDrNu47sOxTaB8J/TzSUxMDNrjbdGihSmz1no5Fq0t0H4xjtvBeNyudCj37t27TVAIxs9Zm66TT2Hx559/mlqvYP39cpg2bZpputP+bQ7B+Bn7hE+6Vtt8GLyOLJg+fboZVdCjRw8zDN61p32g0lEyOhxSF/1qjB071tzev3+/cxipHsvChQutLVu2WO3atUt1GGnt2rXNUNQ1a9aYUTeuw0h1dIIOI33qqafMMFJ9v3SYpT+Gkfbs2dMMi/3hhx/chpNeuHDBuY0OJdWh8cuXLzdDSRs0aGCW5ENJ77nnHjOUXoeHFi5cONWhpP379zcjMSZOnOi3oaQDBw40o9z27t1rPkO9r6Ncvvvuu6A83rS4jgILxuN++eWXzfdaP+cff/zRDHXWIc460jEYj1enNwgLC7P+85//WDt37rQ+/fRTU7aZM2c6twm23y/HKGP9HHUkWnLB9hn7AgHIB3SuBP3i6XxAOixe55TICnT+CA0+yZfOnTubx3V45RtvvGF+ADTktWjRwswl4+rkyZPmByNPnjxmOGXXrl1NsHKlc3DokFXdx6233mp+mPwhtWPVRecGctAfx+eee84MfdUfggceeMCEJFf79u2zWrdubeYD0ZOMnnyuXLmS4r2tVauW+U6UKVPG7TUy09NPP23mS9Fy6I+dfoaO8BOMx5vRABRsx61DlYsVK2bKof+P6X3XOXGC7XjV119/bU7o+rtSqVIl66OPPnJ7PNh+v5TOdaS/WcmPI1g/Y28L0f/4pm4JAAAgMNEHCAAA2A4BCAAA2A4BCAAA2A4BCAAA2A4BCAAA2A4BCAAA2A4BCAAA2A4BCEDQad68ufTp08ffxQAQwAhAALxKr8CdN29ecyFJ12tR6bWJNJi4+uGHHyQkJMRcpyqz6cUiR40aZS4GmytXLnMxY72mlF5byfVaT5mBwAZkvjA/vCaAIHbnnXeawLNx40apX7++Wbd69WpzMUq9+OSlS5ckIiLCrF+xYoWULFlSypYt6/Hr6CT2165dM1d0v5Hw06pVK/n1119l+PDhJvjoRSDXr18v77zzjtSuXVtq1arl8X4BZB3UAAHwKr0ytl51XGt3HPR2u3btzFW5NWS4rtfApPSK9L179zZXttaA1LhxY/n5559T1BZ9++235mrX4eHhsmbNGjl//rx06tRJ8uTJY153zJgx1y3juHHjzFW99UrXvXr1MmGnTJky8sQTT5iQVr58+QyVafr06ZI/f363fS9YsMCU02HIkCFm///73/8kJiZGIiMj5fHHH5ezZ8+ax7t06SIrV66U8ePHm+fpsm/fvht89wFkFAEIgNdpqNHaHQe9rc08zZo1c66/ePGiCRuOAPTKK6/IF198ITNmzJBffvlFypUrZ2pp/vnnH7d9Dxw4UEaOHCnbt2+XGjVqSP/+/U2AWLhwoXz33XcmKOnz0/Ppp59Ky5YtTU1PctpUlzt3bo/KdD3axKfBaNGiRWbR8uoxKA0+DRo0kO7du8vhw4fNEh0d7dH+AXiOAATA6zTU/Pjjj6YfkNZ0bN682YSfpk2bOmuG1q1bZ2pYdFutxZk0aZKMHj1aWrduLVWqVJEpU6ZIzpw55eOPP3bb97Bhw+Tuu+82zWY5cuQwj2uzVYsWLaR69eomrLj2P0rNzp07pVKlSulu40mZricpKcnUFlWrVk2aNGkiTz31lKl9UlojpMeh/ZC0mVCXbNmyebR/AJ4jAAHwOq3t0QChzUXa/6dChQpSuHBhE4Ic/YA0CGmzk/YB0hoS7XisfXFca2LuuOMOU9Pj6rbbbnPe1udpf5569eo51xUsWNA0w12v/9D1eFKm69GmL+0Y7qBNdceOHfNoHwC8i07QALxOm4pKlChhmrtOnTplgo8qXry4ad5Zu3ateeyuu+7yeN+O5qmboYHsjz/+uOn9hIaGpghTqY0g0+DkSvv5aK0QAP+hBgiAT2jTltby6OI6/F2bwbQj84YNG5z9fxzNWdps5hoktAZJm57Sos/TcKG1Sg4auP788890y6adnb///nvTNJecvq7WXmWkTFqrpU18ur1DfHy8eEpfR0e0Acg8BCAAPqHhRkdpaSBw1AApvf3hhx+apitHANJanZ49e5oOzXFxcfL777+bTsEXLlyQbt26pfkaOvJLH9fnLV++XLZt22ZGVWnNTHp0zh1t2tJ+QxMnTjTD4ffs2SNz5841Q/e1j1BGyqRNb9p359VXXzVNZrNmzTJ9fTylTWQa4nT014kTJ6gdAjIBTWAAfELDjY700s7GRYsWdQtAWmviGC7voKOi9MSvHYT1ce3rs2TJEilQoEC6r6OdlHXeobZt25p+Ni+//LIkJCSk+xwdQr906VJ59913TRjr16+fCTKVK1c2w961s3JGyqT9jWbOnGlCknaQ1kClw9579Ojh0Xulr9+5c2dTs6Tv2d69e00oAuA7IVZGegMCAAAEEZrAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7RCAAACA7fwfOaAMbioUmJoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned text:\n",
      "0    Our forefathers have told us much of the comin...\n",
      "Name: clean_text, dtype: object\n",
      "Scraped 337 candidate personal names from Wiktionary.\n",
      "Scraped 76 candidate town names from Wikipedia.\n",
      "Loaded manually curated candidate entities:\n",
      "  entity_candidate entity\n",
      "0            Ailaq  B-PER\n",
      "1             Aluk  B-PER\n",
      "2           Alátaq  B-PER\n",
      "3         Amerdloq  B-PER\n",
      "4          Anarteq  B-PER\n",
      "Final Entity Dictionary (sample):\n",
      "aaju: B-PER\n",
      "aaneeraq: B-PER\n",
      "aani: B-PER\n",
      "aaninnguaq: B-PER\n",
      "aannguaq: B-PER\n",
      "aappilattoq: B-LOC\n",
      "aaqa: B-PER\n",
      "aasiaat: B-LOC\n",
      "aggu: B-PER\n",
      "ailaq: B-PER\n",
      "aima: B-PER\n",
      "aja: B-PER\n",
      "ajaaja: B-PER\n",
      "aka: B-PER\n",
      "akisooq: B-PER\n",
      "akitsinnguaq: B-PER\n",
      "akunnaaq: B-LOC\n",
      "aleqa: B-PER\n",
      "alibak: B-PER\n",
      "alluitsup paa: B-LOC\n",
      "Saved final entity dictionary to 'final_entity_dictionary.csv'.\n",
      "Auto-labeled DataFrame shape: (49656, 4)\n",
      "Saved auto-labeled NER data to 'auto_ner_data.csv'.\n",
      "Grouped DataFrame shape: (2044, 4)\n",
      "   doc_id  sentence_id                                             tokens  \\\n",
      "0       0            0  [Our, forefathers, have, told, us, much, of, t...   \n",
      "1       0            1  [Those, who, lived, long, before, our, day, ,,...   \n",
      "2       0            2  [And, they, told, of, many, things, ,, and, th...   \n",
      "3       0            3  [Old, women, do, not, waste, their, words, idl...   \n",
      "4       0            4                      [Old, age, does, not, lie, .]   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "3   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
      "4                                 [O, O, O, O, O, O]  \n",
      "Train size: (1635, 4)\n",
      "Validation size: (409, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1635/1635 [00:00<00:00, 6583.64 examples/s]\n",
      "Map: 100%|██████████| 409/409 [00:00<00:00, 7024.11 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed datasets ready for training.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1635\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 409\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/2l/6514_hd91tv5448lmq79vpbw0000gn/T/ipykernel_26644/1580203717.py:318: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 318\u001b[0m\n\u001b[1;32m    301\u001b[0m model\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable()\n\u001b[1;32m    303\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m    304\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreenlandic_ner_checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    305\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m     remove_unused_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    316\u001b[0m )\n\u001b[0;32m--> 318\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessed_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessed_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    330\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreenlandic_ner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/trainer.py:614\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mplace_model_on_device\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantization_method\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m QuantizationMethod\u001b[38;5;241m.\u001b[39mBITS_AND_BYTES\n\u001b[1;32m    613\u001b[0m ):\n\u001b[0;32m--> 614\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_move_model_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_model_parallel:\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/trainer.py:901\u001b[0m, in \u001b[0;36mTrainer._move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_move_model_to_device\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, device):\n\u001b[0;32m--> 901\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mparallel_mode \u001b[38;5;241m==\u001b[39m ParallelMode\u001b[38;5;241m.\u001b[39mTPU \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtie_weights\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:3712\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3708\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3709\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3710\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3711\u001b[0m         )\n\u001b[0;32m-> 3712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1343\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 903 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:903\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    902\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 903\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    914\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:930\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 930\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1329\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1323\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1324\u001b[0m             device,\n\u001b[1;32m   1325\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1326\u001b[0m             non_blocking,\n\u001b[1;32m   1327\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1328\u001b[0m         )\n\u001b[0;32m-> 1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# 1. SETUP & IMPORTS\n",
    "############################################\n",
    "import os\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Adjust for your MPS backend\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from transformers import (\n",
    "    pipeline, AutoTokenizer, AutoModelForTokenClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "############################################\n",
    "# 2. LOAD & EXPLORE DATA\n",
    "############################################\n",
    "df = pd.read_pickle(\"eskimo_folktales.pkl\")\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "print(df.info())\n",
    "print(\"Duplicate story IDs:\", df.story_id.duplicated().sum())\n",
    "\n",
    "df[\"text_length\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "plt.hist(df[\"text_length\"], bins=20)\n",
    "plt.title(\"Distribution of Story Lengths\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Number of Stories\")\n",
    "plt.show()\n",
    "\n",
    "############################################\n",
    "# 3. CLEAN THE TEXT\n",
    "############################################\n",
    "def clean_text_for_ner(text: str) -> str:\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    paragraphs = re.split(r'\\n\\s*\\n+', text.strip())\n",
    "    cleaned_paragraphs = []\n",
    "    for para in paragraphs:\n",
    "        para = re.sub(r'\\n+', ' ', para)\n",
    "        para = para.replace('’', \"'\").replace('‘', \"'\").replace('—', '-')\n",
    "        para = re.sub(r'\\s+', ' ', para).strip()\n",
    "        cleaned_paragraphs.append(para)\n",
    "    return \"\\n\\n\".join(cleaned_paragraphs)\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_for_ner)\n",
    "print(\"Sample cleaned text:\")\n",
    "print(df[\"clean_text\"].head(1))\n",
    "\n",
    "############################################\n",
    "# 4. SCRAPE CANDIDATE ENTITIES FROM WEB\n",
    "############################################\n",
    "# a) Personal names from Wiktionary\n",
    "url_names = \"https://en.wiktionary.org/wiki/Appendix:Greenlandic_given_names\"\n",
    "resp_names = requests.get(url_names)\n",
    "soup_names = BeautifulSoup(resp_names.text, \"html.parser\")\n",
    "scraped_names = set()\n",
    "for dd in soup_names.select(\"dl dd\"):\n",
    "    for link in dd.find_all(\"a\"):\n",
    "        candidate = link.get_text(strip=True)\n",
    "        if candidate and len(candidate) > 1:\n",
    "            scraped_names.add(candidate)\n",
    "print(f\"Scraped {len(scraped_names)} candidate personal names from Wiktionary.\")\n",
    "\n",
    "# b) Town names from Wikipedia\n",
    "url_towns = \"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Greenland\"\n",
    "resp_towns = requests.get(url_towns)\n",
    "soup_towns = BeautifulSoup(resp_towns.text, \"html.parser\")\n",
    "scraped_towns = set()\n",
    "for table in soup_towns.find_all(\"table\", class_=\"wikitable\"):\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        for link in row.find_all(\"a\", href=True):\n",
    "            candidate = link.get_text(strip=True)\n",
    "            if candidate and len(candidate) > 1:\n",
    "                if any(bad in candidate.lower() for bad in [\n",
    "                    \"edit\", \"coordinate\", \"article\", \"statement\", \"isbn\",\n",
    "                    \"list of\", \"administrative\", \"autonomy\", \"history\", \"portal\"\n",
    "                ]):\n",
    "                    continue\n",
    "                scraped_towns.add(candidate)\n",
    "print(f\"Scraped {len(scraped_towns)} candidate town names from Wikipedia.\")\n",
    "\n",
    "############################################\n",
    "# 5. LOAD & MERGE MANUAL CANDIDATE CSV\n",
    "############################################\n",
    "try:\n",
    "    manual_df = pd.read_csv(\"candidate_entities_finished.csv\")\n",
    "    print(\"Loaded manually curated candidate entities:\")\n",
    "    print(manual_df.head())\n",
    "    manual_dict = dict(zip(manual_df[\"entity_candidate\"].str.lower(), manual_df[\"entity\"]))\n",
    "except Exception as e:\n",
    "    print(\"Manual candidate CSV not found; proceeding with scraped data only.\")\n",
    "    manual_dict = {}\n",
    "\n",
    "entity_dict = manual_dict.copy()\n",
    "for name in scraped_names:\n",
    "    key = name.lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-PER\"\n",
    "for town in scraped_towns:\n",
    "    key = town.lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-LOC\"\n",
    "\n",
    "print(\"Final Entity Dictionary (sample):\")\n",
    "for k, v in list(sorted(entity_dict.items()))[:20]:\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "final_entity_df = pd.DataFrame(list(entity_dict.items()), columns=[\"entity_candidate\", \"entity\"])\n",
    "final_entity_df.to_csv(\"final_entity_dictionary.csv\", index=False)\n",
    "print(\"Saved final entity dictionary to 'final_entity_dictionary.csv'.\")\n",
    "\n",
    "############################################\n",
    "# 6. AUTO-LABEL TEXT WITH BIO USING THE ENTITY DICTIONARY\n",
    "############################################\n",
    "def get_entity_label_bio(token, entity_dict, prev_entity):\n",
    "    token_lower = token.lower()\n",
    "    if token_lower in entity_dict:\n",
    "        label = entity_dict[token_lower]\n",
    "    elif token_lower.endswith(\"s\"):\n",
    "        label = entity_dict.get(token_lower[:-1], \"O\")\n",
    "    else:\n",
    "        label = \"O\"\n",
    "    if label == \"O\":\n",
    "        return \"O\", None\n",
    "    entity_type = label.split(\"-\", 1)[-1]\n",
    "    if prev_entity == entity_type:\n",
    "        return f\"I-{entity_type}\", entity_type\n",
    "    else:\n",
    "        return f\"B-{entity_type}\", entity_type\n",
    "\n",
    "def auto_label_bio_using_dict(text, entity_dict):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    data_rows = []\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        prev_entity = None\n",
    "        for token in tokens:\n",
    "            bio_label, current_ent = get_entity_label_bio(token, entity_dict, prev_entity)\n",
    "            data_rows.append({\n",
    "                \"sentence_id\": sent_id,\n",
    "                \"token\": token,\n",
    "                \"ner_label\": bio_label\n",
    "            })\n",
    "            prev_entity = current_ent if bio_label != \"O\" else None\n",
    "    return data_rows\n",
    "\n",
    "all_rows = []\n",
    "doc_id = 0\n",
    "for _, row in df.iterrows():\n",
    "    labeled_tokens = auto_label_bio_using_dict(row[\"clean_text\"], entity_dict)\n",
    "    for item in labeled_tokens:\n",
    "        all_rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sentence_id\": item[\"sentence_id\"],\n",
    "            \"token\": item[\"token\"],\n",
    "            \"ner_label\": item[\"ner_label\"]\n",
    "        })\n",
    "    doc_id += 1\n",
    "\n",
    "auto_ner_df = pd.DataFrame(all_rows)\n",
    "print(\"Auto-labeled DataFrame shape:\", auto_ner_df.shape)\n",
    "auto_ner_df.to_csv(\"auto_ner_data.csv\", index=False)\n",
    "print(\"Saved auto-labeled NER data to 'auto_ner_data.csv'.\")\n",
    "\n",
    "############################################\n",
    "# 7. GROUP TOKENS BY SENTENCE FOR TRAINING\n",
    "############################################\n",
    "grouped = auto_ner_df.groupby([\"doc_id\", \"sentence_id\"])\n",
    "examples = []\n",
    "for (doc_id, sent_id), group in grouped:\n",
    "    tokens = group[\"token\"].tolist()\n",
    "    labels = group[\"ner_label\"].tolist()\n",
    "    examples.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"sentence_id\": sent_id,\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": labels\n",
    "    })\n",
    "df_grouped = pd.DataFrame(examples)\n",
    "print(\"Grouped DataFrame shape:\", df_grouped.shape)\n",
    "print(df_grouped.head())\n",
    "\n",
    "train_size = int(0.8 * len(df_grouped))\n",
    "train_df = df_grouped.iloc[:train_size]\n",
    "val_df = df_grouped.iloc[train_size:]\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Validation size:\", val_df.shape)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "############################################\n",
    "# 8. TOKENIZATION & LABEL ALIGNMENT\n",
    "############################################\n",
    "label_list = [\"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"]\n",
    "label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, words in enumerate(examples[\"tokens\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        example_labels = examples[\"ner_tags\"][i]\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                if word_id == previous_word_idx and example_labels[word_id] != \"O\":\n",
    "                    # Change a B- label to I- for subsequent subword tokens.\n",
    "                    label = example_labels[word_id]\n",
    "                    if label.startswith(\"B-\"):\n",
    "                        label = \"I-\" + label[2:]\n",
    "                    aligned_labels.append(label2id.get(label, 0))\n",
    "                else:\n",
    "                    aligned_labels.append(label2id.get(example_labels[word_id], 0))\n",
    "                previous_word_idx = word_id\n",
    "        all_labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "print(\"Processed datasets ready for training.\")\n",
    "print(processed_datasets)\n",
    "\n",
    "############################################\n",
    "# 9. TRAINING THE NER MODEL\n",
    "############################################\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "    for pred_row, label_row in zip(predictions, labels):\n",
    "        tmp_true_labels = []\n",
    "        tmp_true_preds = []\n",
    "        for p_i, l_i in zip(pred_row, label_row):\n",
    "            if l_i == -100:\n",
    "                continue\n",
    "            tmp_true_labels.append(id2label[l_i])\n",
    "            tmp_true_preds.append(id2label[p_i])\n",
    "        if tmp_true_labels:\n",
    "            true_labels.append(tmp_true_labels)\n",
    "            true_preds.append(tmp_true_preds)\n",
    "    if not true_labels:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"accuracy\": 1.0}\n",
    "    results = seqeval.compute(predictions=true_preds, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results.get(\"overall_precision\", 0.0),\n",
    "        \"recall\": results.get(\"overall_recall\", 0.0),\n",
    "        \"f1\": results.get(\"overall_f1\", 0.0),\n",
    "        \"accuracy\": results.get(\"overall_accuracy\", 1.0)\n",
    "    }\n",
    "\n",
    "# A minimal data collator that removes extra keys.\n",
    "class MinimalDataCollator(DataCollatorForTokenClassification):\n",
    "    def __call__(self, features):\n",
    "        batch = super().__call__(features)\n",
    "        for key in list(batch.keys()):\n",
    "            if key not in {\"input_ids\", \"attention_mask\", \"labels\", \"token_type_ids\", \"special_tokens_mask\"}:\n",
    "                batch.pop(key, None)\n",
    "        return batch\n",
    "\n",
    "data_collator = MinimalDataCollator(tokenizer, padding=True)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"greenlandic_ner_checkpoints\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=50,\n",
    "    fp16=False,  # Disable fp16 on MPS\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"greenlandic_ner_model\")\n",
    "tokenizer.save_pretrained(\"greenlandic_ner_model\")\n",
    "\n",
    "############################################\n",
    "# 10. INFERENCE\n",
    "############################################\n",
    "from transformers import pipeline\n",
    "\n",
    "ner_infer = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"greenlandic_ner_model\",\n",
    "    tokenizer=\"greenlandic_ner_model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "test_text = \"Nukúnguasik traveled from Ikerssuaq to Nuuk.\"\n",
    "print(\"Inference output:\")\n",
    "print(ner_infer(test_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ner_label\n",
       "O         49211\n",
       "B-PER       390\n",
       "B-MISC       44\n",
       "B-LOC        10\n",
       "B-O           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_ner_df[\"ner_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "/Users/lukaskreibig/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (51, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   story_id  51 non-null     int64 \n",
      " 1   title     51 non-null     object\n",
      " 2   text      51 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.3+ KB\n",
      "None\n",
      "Duplicate story IDs: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned text:\n",
      "0    Our forefathers have told us much of the comin...\n",
      "Name: clean_text, dtype: object\n",
      "Scraped 337 candidate personal names from Wiktionary.\n",
      "Scraped 76 candidate town names from Wikipedia.\n",
      "Loaded manually curated candidate entities:\n",
      "  entity_candidate entity\n",
      "0            Ailaq  B-PER\n",
      "1             Aluk  B-PER\n",
      "2           Alátaq  B-PER\n",
      "3         Amerdloq  B-PER\n",
      "4          Anarteq  B-PER\n",
      "\n",
      "Checking 'nuuk' => B-LOC\n",
      "Checking 'ikerssuaq' => B-LOC\n",
      "Auto-labeled DataFrame shape: (49656, 4)\n",
      "Saved auto-labeled NER data to 'auto_ner_data.csv'.\n",
      "\n",
      "Label distribution in auto-labeled data:\n",
      "ner_label\n",
      "O         49211\n",
      "B-PER       390\n",
      "B-MISC       44\n",
      "B-LOC        10\n",
      "B-O           1\n",
      "Name: count, dtype: int64\n",
      "Grouped DataFrame shape: (2044, 4)\n",
      "Train size: (1635, 4)\n",
      "Validation size: (409, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1635/1635 [00:00<00:00, 14028.20 examples/s]\n",
      "Map: 100%|██████████| 409/409 [00:00<00:00, 15539.95 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed datasets ready for training.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1635\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 409\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/2l/6514_hd91tv5448lmq79vpbw0000gn/T/ipykernel_38399/4032551669.py:333: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='510' max='510' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [510/510 09:10, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.016364</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.997583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.015543</td>\n",
       "      <td>0.892857</td>\n",
       "      <td>0.974026</td>\n",
       "      <td>0.931677</td>\n",
       "      <td>0.997417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.017112</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.997500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.018240</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.997500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "greenlandic_ner_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/greenlandic_ner_model/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:424\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1486\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1482\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[1;32m   1483\u001b[0m ):\n\u001b[1;32m   1484\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[1;32m   1485\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1488\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1376\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1296\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1305\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:280\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 280\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:304\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    303\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 304\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:458\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    450\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m     )\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-67f4e6d2-6d2bb093329c706c2799bcba;3d740313-4ba1-4fcf-9c3d-8ea249298525)\n\nRepository Not Found for url: https://huggingface.co/greenlandic_ner_model/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 358\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# 10. INFERENCE\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m--> 358\u001b[0m ner_infer \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgreenlandic_ner_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgreenlandic_ner_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43maggregation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    363\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m test_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNukúnguasik traveled from Ikerssuaq to Nuuk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInference output:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/pipelines/__init__.py:812\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m     pretrained_model_name_or_path \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig) \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    811\u001b[0m     \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m--> 812\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcache_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m     hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:266\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[1;32m    209\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    210\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:456\u001b[0m, in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    457\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    458\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    460\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    461\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    464\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    465\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: greenlandic_ner_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# 1. SETUP & IMPORTS\n",
    "############################\n",
    "import os\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from transformers import (\n",
    "    pipeline, \n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "############################\n",
    "# 2. LOAD & EXPLORE FOLKTALES DATA\n",
    "############################\n",
    "df = pd.read_pickle(\"eskimo_folktales.pkl\")\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "print(df.info())\n",
    "print(\"Duplicate story IDs:\", df.story_id.duplicated().sum())\n",
    "\n",
    "df[\"text_length\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "plt.hist(df[\"text_length\"], bins=20)\n",
    "plt.title(\"Distribution of Story Lengths\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Number of Stories\")\n",
    "plt.show()\n",
    "\n",
    "############################\n",
    "# 3. CLEAN TEXT\n",
    "############################\n",
    "def clean_text_for_ner(text: str) -> str:\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    paragraphs = re.split(r'\\n\\s*\\n+', text.strip())\n",
    "    cleaned_paragraphs = []\n",
    "    for para in paragraphs:\n",
    "        para = re.sub(r'\\n+', ' ', para)\n",
    "        para = para.replace('’', \"'\").replace('‘', \"'\").replace('—', '-')\n",
    "        para = re.sub(r'\\s+', ' ', para).strip()\n",
    "        cleaned_paragraphs.append(para)\n",
    "    return \"\\n\\n\".join(cleaned_paragraphs)\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_for_ner)\n",
    "print(\"Sample cleaned text:\")\n",
    "print(df[\"clean_text\"].head(1))\n",
    "\n",
    "############################\n",
    "# 4. SCRAPE CANDIDATE ENTITIES\n",
    "############################\n",
    "url_names = \"https://en.wiktionary.org/wiki/Appendix:Greenlandic_given_names\"\n",
    "resp_names = requests.get(url_names)\n",
    "soup_names = BeautifulSoup(resp_names.text, \"html.parser\")\n",
    "\n",
    "scraped_names = set()\n",
    "for dd in soup_names.select(\"dl dd\"):\n",
    "    for link in dd.find_all(\"a\"):\n",
    "        candidate = link.get_text(strip=True)\n",
    "        if candidate and len(candidate) > 1:\n",
    "            scraped_names.add(candidate)\n",
    "print(f\"Scraped {len(scraped_names)} candidate personal names from Wiktionary.\")\n",
    "\n",
    "url_towns = \"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Greenland\"\n",
    "resp_towns = requests.get(url_towns)\n",
    "soup_towns = BeautifulSoup(resp_towns.text, \"html.parser\")\n",
    "\n",
    "scraped_towns = set()\n",
    "for table in soup_towns.find_all(\"table\", class_=\"wikitable\"):\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        for link in row.find_all(\"a\", href=True):\n",
    "            candidate = link.get_text(strip=True)\n",
    "            if candidate and len(candidate) > 1:\n",
    "                if any(bad in candidate.lower() for bad in [\n",
    "                    \"edit\",\"coordinate\",\"article\",\"statement\",\"isbn\",\n",
    "                    \"list of\",\"administrative\",\"autonomy\",\"history\",\"portal\"\n",
    "                ]):\n",
    "                    continue\n",
    "                scraped_towns.add(candidate)\n",
    "print(f\"Scraped {len(scraped_towns)} candidate town names from Wikipedia.\")\n",
    "\n",
    "############################\n",
    "# 5. LOAD MANUAL CSV & MERGE\n",
    "############################\n",
    "try:\n",
    "    manual_df = pd.read_csv(\"candidate_entities_finished.csv\")\n",
    "    print(\"Loaded manually curated candidate entities:\")\n",
    "    print(manual_df.head())\n",
    "    manual_dict = dict(zip(manual_df[\"entity_candidate\"].str.lower(), manual_df[\"entity\"]))\n",
    "except Exception as e:\n",
    "    print(\"Manual candidate CSV not found; proceeding with scraped data only.\")\n",
    "    manual_dict = {}\n",
    "\n",
    "entity_dict = manual_dict.copy()\n",
    "\n",
    "for name in scraped_names:\n",
    "    key = name.lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-PER\"\n",
    "\n",
    "for town in scraped_towns:\n",
    "    key = town.lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-LOC\"\n",
    "\n",
    "# CRUCIAL: Ensure \"nuuk\" => \"B-LOC\", \"ikerssuaq\" => \"B-LOC\"\n",
    "print(\"\\nChecking 'nuuk' =>\", entity_dict.get(\"nuuk\"))\n",
    "print(\"Checking 'ikerssuaq' =>\", entity_dict.get(\"ikerssuaq\"))\n",
    "\n",
    "############################\n",
    "# 6. AUTO-LABEL WITH BIO\n",
    "############################\n",
    "def get_entity_label_bio(token, entity_dict, prev_entity):\n",
    "    token_lower = token.lower()\n",
    "    if token_lower in entity_dict:\n",
    "        label = entity_dict[token_lower]\n",
    "    elif token_lower.endswith(\"s\"):\n",
    "        label = entity_dict.get(token_lower[:-1], \"O\")\n",
    "    else:\n",
    "        label = \"O\"\n",
    "    if label == \"O\":\n",
    "        return \"O\", None\n",
    "    # Extract the entity type from e.g. \"B-LOC\" => \"LOC\"\n",
    "    ent_type = label.split(\"-\", 1)[-1]\n",
    "    if prev_entity == ent_type:\n",
    "        return f\"I-{ent_type}\", ent_type\n",
    "    else:\n",
    "        return f\"B-{ent_type}\", ent_type\n",
    "\n",
    "def auto_label_bio_using_dict(text, entity_dict):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    data_rows = []\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        prev_entity = None\n",
    "        for token in tokens:\n",
    "            bio_label, current_ent = get_entity_label_bio(token, entity_dict, prev_entity)\n",
    "            data_rows.append({\n",
    "                \"sentence_id\": sent_id,\n",
    "                \"token\": token,\n",
    "                \"ner_label\": bio_label\n",
    "            })\n",
    "            prev_entity = current_ent if bio_label != \"O\" else None\n",
    "    return data_rows\n",
    "\n",
    "all_rows = []\n",
    "doc_id = 0\n",
    "for idx, row in df.iterrows():\n",
    "    labeled_tokens = auto_label_bio_using_dict(row[\"clean_text\"], entity_dict)\n",
    "    for item in labeled_tokens:\n",
    "        all_rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sentence_id\": item[\"sentence_id\"],\n",
    "            \"token\": item[\"token\"],\n",
    "            \"ner_label\": item[\"ner_label\"]\n",
    "        })\n",
    "    doc_id += 1\n",
    "\n",
    "auto_ner_df = pd.DataFrame(all_rows)\n",
    "auto_ner_df.to_csv(\"auto_ner_data.csv\", index=False)\n",
    "\n",
    "print(\"Auto-labeled DataFrame shape:\", auto_ner_df.shape)\n",
    "print(\"Saved auto-labeled NER data to 'auto_ner_data.csv'.\")\n",
    "\n",
    "############################\n",
    "# (Optional) Distribution Check\n",
    "############################\n",
    "print(\"\\nLabel distribution in auto-labeled data:\")\n",
    "print(auto_ner_df[\"ner_label\"].value_counts())\n",
    "\n",
    "############################\n",
    "# 7. GROUP TOKENS PER SENTENCE\n",
    "############################\n",
    "grouped = auto_ner_df.groupby([\"doc_id\",\"sentence_id\"])\n",
    "examples = []\n",
    "for (doc_id, sent_id), group in grouped:\n",
    "    tokens = group[\"token\"].tolist()\n",
    "    labels = group[\"ner_label\"].tolist()\n",
    "    examples.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"sentence_id\": sent_id,\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": labels\n",
    "    })\n",
    "\n",
    "df_grouped = pd.DataFrame(examples)\n",
    "print(\"Grouped DataFrame shape:\", df_grouped.shape)\n",
    "\n",
    "train_size = int(0.8 * len(df_grouped))\n",
    "train_df = df_grouped.iloc[:train_size]\n",
    "val_df = df_grouped.iloc[train_size:]\n",
    "\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Validation size:\", val_df.shape)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "############################\n",
    "# 8. TOKENIZE & ALIGN LABELS\n",
    "############################\n",
    "label_list = [\"O\",\"B-PER\",\"I-PER\",\"B-LOC\",\"I-LOC\",\"B-MISC\",\"I-MISC\"]\n",
    "label2id = {lbl:i for i,lbl in enumerate(label_list)}\n",
    "id2label = {i:lbl for lbl,i in label2id.items()}\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i, words in enumerate(examples[\"tokens\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        example_labels = examples[\"ner_tags\"][i]\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                # If same word as previous subword, convert B- => I- for that subword\n",
    "                if word_id == previous_word_idx and example_labels[word_id] != \"O\":\n",
    "                    label = example_labels[word_id]\n",
    "                    if label.startswith(\"B-\"):\n",
    "                        label = \"I-\" + label[2:]\n",
    "                    aligned_labels.append(label2id.get(label,0))\n",
    "                else:\n",
    "                    aligned_labels.append(label2id.get(example_labels[word_id],0))\n",
    "                previous_word_idx = word_id\n",
    "        all_labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "\n",
    "print(\"Processed datasets ready for training.\")\n",
    "print(processed_datasets)\n",
    "\n",
    "############################\n",
    "# 9. TRAINING\n",
    "############################\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "    for pred_row,label_row in zip(predictions,labels):\n",
    "        tmp_true_labels = []\n",
    "        tmp_true_preds = []\n",
    "        for p_i,l_i in zip(pred_row,label_row):\n",
    "            if l_i == -100:\n",
    "                continue\n",
    "            tmp_true_labels.append(id2label[l_i])\n",
    "            tmp_true_preds.append(id2label[p_i])\n",
    "        if tmp_true_labels:\n",
    "            true_labels.append(tmp_true_labels)\n",
    "            true_preds.append(tmp_true_preds)\n",
    "    if not true_labels:\n",
    "        return {\"precision\":0.0,\"recall\":0.0,\"f1\":0.0,\"accuracy\":1.0}\n",
    "    results = seqeval.compute(predictions=true_preds, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }\n",
    "\n",
    "class MinimalDataCollator(DataCollatorForTokenClassification):\n",
    "    def __call__(self, features):\n",
    "        batch = super().__call__(features)\n",
    "        keep_keys = {\"input_ids\",\"attention_mask\",\"labels\",\"token_type_ids\",\"special_tokens_mask\"}\n",
    "        for k in list(batch.keys()):\n",
    "            if k not in keep_keys:\n",
    "                batch.pop(k,None)\n",
    "        return batch\n",
    "\n",
    "data_collator = MinimalDataCollator(tokenizer, padding=True)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"greenlandic_ner_checkpoints\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,  # Increase epochs to help location class\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=50,\n",
    "    fp16=False,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "best_model = trainer.model\n",
    "best_model.eval()\n",
    "best_model.half()\n",
    "\n",
    "save_path = \"greenlandic_ner_model_fp16\"\n",
    "best_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference output:\n",
      "[{'entity_group': 'PER', 'score': 0.9886085, 'word': 'Nukúnguasik', 'start': 0, 'end': 11}, {'entity_group': 'PER', 'score': 0.7009339, 'word': 'I', 'start': 26, 'end': 27}, {'entity_group': 'LOC', 'score': 0.28212953, 'word': 'kers', 'start': 27, 'end': 31}, {'entity_group': 'PER', 'score': 0.9799397, 'word': 'suaq', 'start': 31, 'end': 35}, {'entity_group': 'PER', 'score': 0.8069587, 'word': 'Nuuk', 'start': 39, 'end': 43}]\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# 10. INFERENCE\n",
    "############################\n",
    "from transformers import pipeline\n",
    "\n",
    "ner_infer = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"greenlandic_ner_model_fp16\",\n",
    "    tokenizer=\"greenlandic_ner_model_fp16\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "test_text = \"Nukúnguasik traveled from Ikerssuaq to Nuuk.\"\n",
    "print(\"\\nInference output:\")\n",
    "print(ner_infer(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'O': 49211, 'B-PER': 390, 'B-MISC': 44, 'B-LOC': 10, 'B-O ': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "label_counts = Counter(auto_ner_df[\"ner_label\"])\n",
    "print(label_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (51, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   story_id  51 non-null     int64 \n",
      " 1   title     51 non-null     object\n",
      " 2   text      51 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.3+ KB\n",
      "None\n",
      "Duplicate story IDs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample cleaned text:\n",
      " Our forefathers have told us much of the coming of earth, and of men, and it was a long, long while ago. Those who lived long before our day, they did not know how to store their words in little black marks, as you do; they could only tell stories. And they told of many things, and therefore we are  ...\n",
      "\n",
      "Loaded dictionary. Sample:\n",
      "  entity_candidate  entity\n",
      "0            ailaq   B-PER\n",
      "1             aluk   B-PER\n",
      "2           alátaq   B-PER\n",
      "3         amerdloq   B-PER\n",
      "4          anarteq   B-PER\n",
      "5           angiut  B-MISC\n",
      "6     angmagssalik   B-LOC\n",
      "7    angusinãnguaq   B-PER\n",
      "8            artuk   B-PER\n",
      "9           asalôq   B-PER\n",
      "\n",
      "Check 'nuuk': B-LOC\n",
      "Check 'ikerssuaq': B-LOC\n",
      "\n",
      "Auto-labeled DataFrame shape: (49656, 4)\n",
      "Saved to 'auto_ner_data.csv'\n",
      "\n",
      "Grouped DataFrame shape: (2044, 4)\n",
      "   doc_id  sentence_id                                             tokens  \\\n",
      "0       0            0  [Our, forefathers, have, told, us, much, of, t...   \n",
      "1       0            1  [Those, who, lived, long, before, our, day, ,,...   \n",
      "2       0            2  [And, they, told, of, many, things, ,, and, th...   \n",
      "3       0            3  [Old, women, do, not, waste, their, words, idl...   \n",
      "4       0            4                      [Old, age, does, not, lie, .]   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "3   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
      "4                                 [O, O, O, O, O, O]  \n",
      "Train size: (1635, 4)\n",
      "Validation size: (409, 4)\n",
      "\n",
      "Number of special tokens added: 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1635/1635 [00:00<00:00, 6548.72 examples/s]\n",
      "Map: 100%|██████████| 409/409 [00:00<00:00, 7105.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed datasets ready for training:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1635\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 409\n",
      "    })\n",
      "})\n",
      "\n",
      "Label distribution in auto_ner_df: Counter({'O': 49211, 'B-PER': 390, 'B-MISC': 44, 'B-LOC': 10, 'B-O ': 1})\n",
      "Weights: tensor([2.0320e-05, 2.5575e-03, 1.0000e+00, 9.0909e-02, 1.0000e+00, 2.2222e-02,\n",
      "        1.0000e+00])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "/Users/lukaskreibig/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/2l/6514_hd91tv5448lmq79vpbw0000gn/T/ipykernel_38399/2312735759.py:270: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedNERTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "WeightedNERTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 366\u001b[0m\n\u001b[1;32m    340\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m    341\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreenlandic_ner_checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    342\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# or \"steps\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m     remove_unused_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    353\u001b[0m )\n\u001b[1;32m    355\u001b[0m trainer \u001b[38;5;241m=\u001b[39m WeightedNERTrainer(\n\u001b[1;32m    356\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    357\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[1;32m    364\u001b[0m )\n\u001b[0;32m--> 366\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m############################################\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# 12. SAVE MODEL & TEST\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m############################################\u001b[39;00m\n\u001b[1;32m    371\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreenlandic_ner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/trainer.py:2556\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2549\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2550\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2554\u001b[0m )\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2556\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2559\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2561\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2562\u001b[0m ):\n\u001b[1;32m   2563\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2564\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/trainer.py:3718\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3717\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3718\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3720\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3723\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3724\u001b[0m ):\n",
      "\u001b[0;31mTypeError\u001b[0m: WeightedNERTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# 1. SETUP & IMPORTS\n",
    "############################################\n",
    "\n",
    "import os\n",
    "# For Apple M-series users, allow more memory usage on MPS if desired\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from collections import Counter\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import (\n",
    "    pipeline, AutoTokenizer, AutoModelForTokenClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "############################################\n",
    "# 2. NLTK SETUP & LOAD FOLKTALES\n",
    "############################################\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "\n",
    "df = pd.read_pickle(\"eskimo_folktales.pkl\")\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "print(df.info())\n",
    "print(\"Duplicate story IDs:\", df.story_id.duplicated().sum())\n",
    "\n",
    "# Quick EDA: distribution of text lengths\n",
    "df[\"text_length\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "plt.hist(df[\"text_length\"], bins=20)\n",
    "plt.title(\"Distribution of Story Lengths\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Number of Stories\")\n",
    "plt.show()\n",
    "\n",
    "############################################\n",
    "# 3. CLEAN TEXT\n",
    "############################################\n",
    "def clean_text_for_ner(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Basic cleanup: unify line endings, remove extra spaces, etc.\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    paragraphs = re.split(r'\\n\\s*\\n+', text.strip())\n",
    "    cleaned_paragraphs = []\n",
    "    for para in paragraphs:\n",
    "        # Remove multiple newlines inside each paragraph\n",
    "        para = re.sub(r'\\n+', ' ', para)\n",
    "        # Normalize or remove fancy quotes/dashes\n",
    "        para = para.replace('’', \"'\").replace('‘', \"'\").replace('—', '-')\n",
    "        # Remove extra spaces\n",
    "        para = re.sub(r'\\s+', ' ', para).strip()\n",
    "        cleaned_paragraphs.append(para)\n",
    "    return \"\\n\\n\".join(cleaned_paragraphs)\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_for_ner)\n",
    "print(\"\\nSample cleaned text:\\n\", df[\"clean_text\"].iloc[0][:300], \"...\")\n",
    "\n",
    "############################################\n",
    "# 4. LOAD DICTIONARY & (OPTIONAL) SCRAPE DATA\n",
    "############################################\n",
    "# If you have a CSV with your final dictionary, load it:\n",
    "final_dict_path = \"final_entity_dictionary.csv\"  # or \"candidate_entities_finished.csv\"\n",
    "dictionary_df = pd.read_csv(final_dict_path)\n",
    "print(\"\\nLoaded dictionary. Sample:\")\n",
    "print(dictionary_df.head(10))\n",
    "\n",
    "# Convert to a Python dict: token.lower() -> 'B-PER' / 'B-LOC' / ...\n",
    "entity_dict = dict(zip(dictionary_df[\"entity_candidate\"].str.lower(), dictionary_df[\"entity\"]))\n",
    "\n",
    "# Debug check: e.g. 'nuuk' or 'ikerssuaq'\n",
    "print(\"\\nCheck 'nuuk':\", entity_dict.get(\"nuuk\"))\n",
    "print(\"Check 'ikerssuaq':\", entity_dict.get(\"ikerssuaq\"))\n",
    "\n",
    "############################################\n",
    "# 5. AUTO-LABEL FOLKTALES WITH BIO TAGS\n",
    "############################################\n",
    "\n",
    "def get_entity_label_bio(token, entity_dict, prev_entity):\n",
    "    \"\"\"\n",
    "    Assign a B- or I- tag based on dictionary lookups.\n",
    "    If token not found, label = 'O'.\n",
    "    If found, label = 'B-XXX' or 'I-XXX' depending on previous entity.\n",
    "    \"\"\"\n",
    "    token_lower = token.lower()\n",
    "    # 1) Direct match or remove trailing 's' if it ends with s\n",
    "    if token_lower in entity_dict:\n",
    "        label = entity_dict[token_lower]\n",
    "    elif token_lower.endswith(\"s\"):\n",
    "        label = entity_dict.get(token_lower[:-1], \"O\")\n",
    "    else:\n",
    "        label = \"O\"\n",
    "\n",
    "    # If not recognized, done\n",
    "    if label == \"O\":\n",
    "        return \"O\", None\n",
    "\n",
    "    # e.g. if label = \"B-LOC\", then entity_type = \"LOC\"\n",
    "    entity_type = label.split(\"-\", 1)[-1]\n",
    "\n",
    "    if prev_entity == entity_type:\n",
    "        return f\"I-{entity_type}\", entity_type\n",
    "    else:\n",
    "        return f\"B-{entity_type}\", entity_type\n",
    "\n",
    "def auto_label_bio(text, entity_dict):\n",
    "    \"\"\"\n",
    "    Convert raw text -> list of (token, label) via dictionary-based BIO labeling.\n",
    "    \"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    data_rows = []\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        prev_entity_type = None\n",
    "        for token in tokens:\n",
    "            bio_label, current_type = get_entity_label_bio(token, entity_dict, prev_entity_type)\n",
    "            data_rows.append({\n",
    "                \"sentence_id\": sent_id,\n",
    "                \"token\": token,\n",
    "                \"ner_label\": bio_label\n",
    "            })\n",
    "            prev_entity_type = current_type if bio_label != \"O\" else None\n",
    "    return data_rows\n",
    "\n",
    "all_rows = []\n",
    "doc_id = 0\n",
    "for idx, row in df.iterrows():\n",
    "    labeled = auto_label_bio(row[\"clean_text\"], entity_dict)\n",
    "    for item in labeled:\n",
    "        all_rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sentence_id\": item[\"sentence_id\"],\n",
    "            \"token\": item[\"token\"],\n",
    "            \"ner_label\": item[\"ner_label\"]\n",
    "        })\n",
    "    doc_id += 1\n",
    "\n",
    "auto_ner_df = pd.DataFrame(all_rows)\n",
    "auto_ner_df.to_csv(\"auto_ner_data.csv\", index=False)\n",
    "print(\"\\nAuto-labeled DataFrame shape:\", auto_ner_df.shape)\n",
    "print(\"Saved to 'auto_ner_data.csv'\")\n",
    "\n",
    "############################################\n",
    "# 6. GROUP BY SENTENCE -> TRAINING EXAMPLES\n",
    "############################################\n",
    "grouped = auto_ner_df.groupby([\"doc_id\", \"sentence_id\"])\n",
    "examples = []\n",
    "for (doc_id, sent_id), group in grouped:\n",
    "    tokens = group[\"token\"].tolist()\n",
    "    labels = group[\"ner_label\"].tolist()\n",
    "    examples.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"sentence_id\": sent_id,\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": labels\n",
    "    })\n",
    "\n",
    "df_grouped = pd.DataFrame(examples)\n",
    "print(\"\\nGrouped DataFrame shape:\", df_grouped.shape)\n",
    "print(df_grouped.head(5))\n",
    "\n",
    "############################################\n",
    "# 7. SPLIT TRAIN / VAL\n",
    "############################################\n",
    "train_size = int(0.8 * len(df_grouped))\n",
    "train_df = df_grouped.iloc[:train_size]\n",
    "val_df = df_grouped.iloc[train_size:]\n",
    "\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Validation size:\", val_df.shape)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "############################################\n",
    "# 8. DEFINE LABELS & TOKENIZE\n",
    "############################################\n",
    "# Suppose your domain has at least these labels:\n",
    "label_list = [\"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"]\n",
    "label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# (A) Add known tokens so \"Ikerssuaq\" or \"Nuuk\" won't be subword-split\n",
    "#     For simplicity, let's add all dictionary tokens of length > 4\n",
    "special_tokens = [k for k in entity_dict.keys() if len(k) > 4]\n",
    "num_added = tokenizer.add_tokens(special_tokens)\n",
    "print(\"\\nNumber of special tokens added:\", num_added)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i in range(len(examples[\"tokens\"])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        example_labels = examples[\"ner_tags\"][i]\n",
    "        aligned_labels = []\n",
    "        prev_wid = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                # Retrieve label: e.g. \"B-LOC\"\n",
    "                original_label = example_labels[wid]\n",
    "                if wid == prev_wid and original_label != \"O\":\n",
    "                    # Convert B-xxx to I-xxx\n",
    "                    if original_label.startswith(\"B-\"):\n",
    "                        original_label = \"I-\" + original_label[2:]\n",
    "                aligned_labels.append(label2id.get(original_label, 0))\n",
    "                prev_wid = wid\n",
    "        all_labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "print(\"\\nProcessed datasets ready for training:\")\n",
    "print(processed_datasets)\n",
    "\n",
    "############################################\n",
    "# 9. SHOW LABEL DISTRIBUTION & BUILD WEIGHT\n",
    "############################################\n",
    "# We'll collect the final label distribution from auto_ner_df\n",
    "\n",
    "label_counts = Counter(auto_ner_df[\"ner_label\"])\n",
    "print(\"\\nLabel distribution in auto_ner_df:\", label_counts)\n",
    "\n",
    "# Example weighting approach:\n",
    "# For each label in label2id, weight = 1 / (count_of_label + 1)\n",
    "weight_list = [0]*len(label_list)\n",
    "for lbl, idx in label2id.items():\n",
    "    c = label_counts.get(lbl, 0)\n",
    "    # Avoid 1/0 -> 1/(c+1)\n",
    "    weight_list[idx] = 1.0 / (c+1)\n",
    "\n",
    "weight_tensor = torch.tensor(weight_list, dtype=torch.float)\n",
    "print(\"Weights:\", weight_tensor)\n",
    "\n",
    "############################################\n",
    "# 10. BUILD CUSTOM TRAINER WITH WEIGHTED LOSS\n",
    "############################################\n",
    "class WeightedNERTrainer(Trainer):\n",
    "    def __init__(self, *args, weight=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.weight = weight\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # shape: [batch_size, seq_len, num_labels]\n",
    "\n",
    "        # Flatten\n",
    "        loss_mask = (labels != -100)\n",
    "        active_logits = logits.view(-1, self.model.config.num_labels)\n",
    "        active_labels = torch.where(\n",
    "            loss_mask.view(-1), \n",
    "            labels.view(-1),\n",
    "            torch.tensor(-100, device=labels.device)\n",
    "        )\n",
    "\n",
    "        # Weighted cross entropy\n",
    "        loss = F.cross_entropy(\n",
    "            active_logits,\n",
    "            active_labels,\n",
    "            weight=self.weight,\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Metric\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "    for pred_row, label_row in zip(predictions, labels):\n",
    "        temp_true_labels = []\n",
    "        temp_true_preds = []\n",
    "        for p_i, l_i in zip(pred_row, label_row):\n",
    "            if l_i == -100:\n",
    "                continue\n",
    "            temp_true_labels.append(id2label[l_i])\n",
    "            temp_true_preds.append(id2label[p_i])\n",
    "        if temp_true_labels:\n",
    "            true_labels.append(temp_true_labels)\n",
    "            true_preds.append(temp_true_preds)\n",
    "    if not true_labels:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"accuracy\": 1.0}\n",
    "    results = seqeval.compute(predictions=true_preds, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }\n",
    "\n",
    "############################################\n",
    "# 11. MODEL INIT & TRAIN\n",
    "############################################\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Must resize embeddings since we added special tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"greenlandic_ner_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",  # or \"steps\"\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,  # more epochs to help minority classes\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=50,\n",
    "    fp16=False,  # on Apple MPS, keep fp16 off\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = WeightedNERTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer, padding=True),\n",
    "    weight=weight_tensor,  # Weighted cross-entropy\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "############################################\n",
    "# 12. SAVE MODEL & TEST\n",
    "############################################\n",
    "trainer.save_model(\"greenlandic_ner_model\")\n",
    "tokenizer.save_pretrained(\"greenlandic_ner_model\")\n",
    "\n",
    "# Quick test\n",
    "inference_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"greenlandic_ner_model\",\n",
    "    tokenizer=\"greenlandic_ner_model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "test_text = \"Nukúnguasik traveled from Ikerssuaq to Nuuk.\"\n",
    "print(\"\\nInference on sample:\", test_text)\n",
    "print(inference_pipeline(test_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (51, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   story_id  51 non-null     int64 \n",
      " 1   title     51 non-null     object\n",
      " 2   text      51 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.3+ KB\n",
      "None\n",
      "Duplicate story IDs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned text:\n",
      "Our forefathers have told us much of the coming of earth, and of men, and it was a long, long while ago. Those who lived long before our day, they did not know how to store their words in little black marks, as you do; they could only tell stories. And they told of many things, and therefore we are  ...\n",
      "\n",
      "Scraped 337 candidate personal names from Wiktionary.\n",
      "Scraped 76 candidate town names from Wikipedia.\n",
      "Loaded manually curated candidate entities:\n",
      "  entity_candidate entity\n",
      "0            Ailaq  B-PER\n",
      "1             Aluk  B-PER\n",
      "2           Alátaq  B-PER\n",
      "3         Amerdloq  B-PER\n",
      "4          Anarteq  B-PER\n",
      "Final Entity Dictionary (sample):\n",
      "aaju: B-PER\n",
      "aaneeraq: B-PER\n",
      "aani: B-PER\n",
      "aaninnguaq: B-PER\n",
      "aannguaq: B-PER\n",
      "aappilattoq: B-LOC\n",
      "aaqa: B-PER\n",
      "aasiaat: B-LOC\n",
      "aggu: B-PER\n",
      "ailaq: B-PER\n",
      "aima: B-PER\n",
      "aja: B-PER\n",
      "ajaaja: B-PER\n",
      "aka: B-PER\n",
      "akisooq: B-PER\n",
      "akitsinnguaq: B-PER\n",
      "akunnaaq: B-LOC\n",
      "aleqa: B-PER\n",
      "alibak: B-PER\n",
      "alluitsup paa: B-LOC\n",
      "Saved final entity dictionary to 'final_entity_dictionary.csv'.\n",
      "Auto-labeled DataFrame shape: (49656, 4)\n",
      "Saved auto-labeled NER data to 'auto_ner_data.csv'.\n",
      "Grouped DataFrame shape: (2044, 4)\n",
      "   doc_id  sentence_id                                             tokens  \\\n",
      "0       0            0  [Our, forefathers, have, told, us, much, of, t...   \n",
      "1       0            1  [Those, who, lived, long, before, our, day, ,,...   \n",
      "2       0            2  [And, they, told, of, many, things, ,, and, th...   \n",
      "3       0            3  [Old, women, do, not, waste, their, words, idl...   \n",
      "4       0            4                      [Old, age, does, not, lie, .]   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "3   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
      "4                                 [O, O, O, O, O, O]  \n",
      "Train size: (1635, 4)\n",
      "Validation size: (409, 4)\n",
      "Number of special tokens added: 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1635/1635 [00:00<00:00, 6749.95 examples/s]\n",
      "Map: 100%|██████████| 409/409 [00:00<00:00, 7178.36 examples/s]\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed datasets ready for training:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1635\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 409\n",
      "    })\n",
      "})\n",
      "\n",
      "Label distribution in auto_ner_df: Counter({'O': 49212, 'B-PER': 390, 'B-MISC': 44, 'B-LOC': 10})\n",
      "Weight tensor: tensor([2.0320e-05, 2.5575e-03, 9.0909e-02, 2.2222e-02])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukaskreibig/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/2l/6514_hd91tv5448lmq79vpbw0000gn/T/ipykernel_38399/50814932.py:359: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedNERTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedNERTrainer(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Trainer.__init__() got an unexpected keyword argument 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 359\u001b[0m\n\u001b[1;32m    342\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m    343\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreenlandic_ner_checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    344\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m     remove_unused_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    355\u001b[0m )\n\u001b[1;32m    357\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForTokenClassification(tokenizer, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 359\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mWeightedNERTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessed_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessed_datasets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseqeval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mid2label\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgold\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m            \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    383\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Trainer.__init__() got an unexpected keyword argument 'weight'"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# 1. SETUP & IMPORTS\n",
    "############################################\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Adjust for Apple MPS\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import (\n",
    "    pipeline, AutoTokenizer, AutoModelForTokenClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "\n",
    "############################################\n",
    "# 2. LOAD & EXPLORE FOLKTALES DATA\n",
    "############################################\n",
    "\n",
    "df = pd.read_pickle(\"eskimo_folktales.pkl\")\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "print(df.info())\n",
    "print(\"Duplicate story IDs:\", df.story_id.duplicated().sum())\n",
    "\n",
    "df[\"text_length\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "plt.hist(df[\"text_length\"], bins=20)\n",
    "plt.title(\"Distribution of Story Lengths\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Number of Stories\")\n",
    "plt.show()\n",
    "\n",
    "############################################\n",
    "# 3. CLEAN THE TEXT\n",
    "############################################\n",
    "\n",
    "def clean_text_for_ner(text: str) -> str:\n",
    "    # Unify line endings and split into paragraphs\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    paragraphs = re.split(r'\\n\\s*\\n+', text.strip())\n",
    "    cleaned_paragraphs = []\n",
    "    for para in paragraphs:\n",
    "        para = re.sub(r'\\n+', ' ', para)\n",
    "        # Normalize quotes/dashes\n",
    "        para = para.replace('’', \"'\").replace('‘', \"'\").replace('—', '-')\n",
    "        para = re.sub(r'\\s+', ' ', para).strip()\n",
    "        cleaned_paragraphs.append(para)\n",
    "    return \"\\n\\n\".join(cleaned_paragraphs)\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_for_ner)\n",
    "print(\"Sample cleaned text:\")\n",
    "print(df[\"clean_text\"].iloc[0][:300], \"...\\n\")\n",
    "\n",
    "############################################\n",
    "# 4. SCRAPE CANDIDATE ENTITIES FROM THE WEB\n",
    "############################################\n",
    "\n",
    "# 4a. Scrape candidate personal names from Wiktionary\n",
    "url_names = \"https://en.wiktionary.org/wiki/Appendix:Greenlandic_given_names\"\n",
    "resp_names = requests.get(url_names)\n",
    "soup_names = BeautifulSoup(resp_names.text, \"html.parser\")\n",
    "scraped_names = set()\n",
    "for dd in soup_names.select(\"dl dd\"):\n",
    "    for link in dd.find_all(\"a\"):\n",
    "        candidate = link.get_text(strip=True)\n",
    "        if candidate and len(candidate) > 1:\n",
    "            scraped_names.add(candidate)\n",
    "print(f\"Scraped {len(scraped_names)} candidate personal names from Wiktionary.\")\n",
    "\n",
    "# 4b. Scrape candidate town names from Wikipedia\n",
    "url_towns = \"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Greenland\"\n",
    "resp_towns = requests.get(url_towns)\n",
    "soup_towns = BeautifulSoup(resp_towns.text, \"html.parser\")\n",
    "scraped_towns = set()\n",
    "tables = soup_towns.find_all(\"table\", class_=\"wikitable\")\n",
    "for table in tables:\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        for link in row.find_all(\"a\", href=True):\n",
    "            candidate = link.get_text(strip=True)\n",
    "            if candidate and len(candidate) > 1:\n",
    "                # Filter out common wiki noise\n",
    "                if any(bad in candidate.lower() for bad in [\n",
    "                    \"edit\", \"coordinate\", \"article\", \"statement\", \"isbn\",\n",
    "                    \"list of\", \"administrative\", \"autonomy\", \"history\", \"portal\"\n",
    "                ]):\n",
    "                    continue\n",
    "                scraped_towns.add(candidate)\n",
    "print(f\"Scraped {len(scraped_towns)} candidate town names from Wikipedia.\")\n",
    "\n",
    "############################################\n",
    "# 5. LOAD MANUALLY CURATED CANDIDATE CSV & MERGE WITH SCRAPED DATA\n",
    "############################################\n",
    "\n",
    "# Load manually curated candidate CSV (assumed to be in BIO format)\n",
    "try:\n",
    "    manual_df = pd.read_csv(\"candidate_entities_finished.csv\")\n",
    "    print(\"Loaded manually curated candidate entities:\")\n",
    "    print(manual_df.head())\n",
    "    # Clean the columns: remove extra whitespace & make keys lowercase\n",
    "    manual_df[\"entity_candidate\"] = manual_df[\"entity_candidate\"].str.strip().str.lower()\n",
    "    manual_df[\"entity\"] = manual_df[\"entity\"].str.strip()\n",
    "    # Fix any erroneous label (e.g., \"B-O\" becomes \"O\")\n",
    "    manual_df[\"entity\"] = manual_df[\"entity\"].replace({\"B-O\": \"O\", \"B-O \": \"O\"})\n",
    "    manual_dict = dict(zip(manual_df[\"entity_candidate\"], manual_df[\"entity\"]))\n",
    "except Exception as e:\n",
    "    print(\"Manual candidate CSV not found; proceeding with scraped data only.\")\n",
    "    manual_dict = {}\n",
    "\n",
    "# Merge manual data takes precedence over scraped\n",
    "entity_dict = manual_dict.copy()\n",
    "for name in scraped_names:\n",
    "    key = name.strip().lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-PER\"\n",
    "for town in scraped_towns:\n",
    "    key = town.strip().lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-LOC\"\n",
    "\n",
    "print(\"Final Entity Dictionary (sample):\")\n",
    "for key, val in sorted(entity_dict.items())[:20]:\n",
    "    print(f\"{key}: {val}\")\n",
    "\n",
    "# Save dictionary for review\n",
    "final_entity_df = pd.DataFrame(list(entity_dict.items()), columns=[\"entity_candidate\", \"entity\"])\n",
    "final_entity_df.to_csv(\"final_entity_dictionary.csv\", index=False)\n",
    "print(\"Saved final entity dictionary to 'final_entity_dictionary.csv'.\")\n",
    "\n",
    "############################################\n",
    "# 6. AUTO-LABEL FOLKTALE TEXTS USING DICTIONARY (BIO FORMATTING)\n",
    "############################################\n",
    "\n",
    "def get_entity_label_bio(token, entity_dict, prev_entity):\n",
    "    token_lower = token.strip().lower()\n",
    "    if token_lower in entity_dict:\n",
    "        label = entity_dict[token_lower]\n",
    "    elif token_lower.endswith(\"s\"):  # simple plural handling\n",
    "        label = entity_dict.get(token_lower[:-1], \"O\")\n",
    "    else:\n",
    "        label = \"O\"\n",
    "\n",
    "    if label == \"O\":\n",
    "        return \"O\", None\n",
    "    entity_type = label.split(\"-\", 1)[-1]\n",
    "    if prev_entity == entity_type:\n",
    "        return f\"I-{entity_type}\", entity_type\n",
    "    else:\n",
    "        return f\"B-{entity_type}\", entity_type\n",
    "\n",
    "def auto_label_bio_using_dict(text, entity_dict):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    data_rows = []\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        prev_entity = None\n",
    "        for token in tokens:\n",
    "            bio_label, current_entity = get_entity_label_bio(token, entity_dict, prev_entity)\n",
    "            data_rows.append({\n",
    "                \"sentence_id\": sent_id,\n",
    "                \"token\": token,\n",
    "                \"ner_label\": bio_label\n",
    "            })\n",
    "            prev_entity = current_entity if bio_label != \"O\" else None\n",
    "    return data_rows\n",
    "\n",
    "all_rows = []\n",
    "doc_id = 0\n",
    "for _, row in df.iterrows():\n",
    "    labeled_tokens = auto_label_bio_using_dict(row[\"clean_text\"], entity_dict)\n",
    "    for item in labeled_tokens:\n",
    "        all_rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sentence_id\": item[\"sentence_id\"],\n",
    "            \"token\": item[\"token\"],\n",
    "            \"ner_label\": item[\"ner_label\"]\n",
    "        })\n",
    "    doc_id += 1\n",
    "\n",
    "auto_ner_df = pd.DataFrame(all_rows)\n",
    "auto_ner_df.to_csv(\"auto_ner_data.csv\", index=False)\n",
    "print(\"Auto-labeled DataFrame shape:\", auto_ner_df.shape)\n",
    "print(\"Saved auto-labeled NER data to 'auto_ner_data.csv'.\")\n",
    "\n",
    "############################################\n",
    "# 7. GROUP TOKENS BY SENTENCE FOR TRAINING EXAMPLES\n",
    "############################################\n",
    "\n",
    "grouped = auto_ner_df.groupby([\"doc_id\", \"sentence_id\"])\n",
    "examples = []\n",
    "for (doc_id, sent_id), group in grouped:\n",
    "    tokens = group[\"token\"].tolist()\n",
    "    labels = group[\"ner_label\"].tolist()\n",
    "    examples.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"sentence_id\": sent_id,\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": labels\n",
    "    })\n",
    "df_grouped = pd.DataFrame(examples)\n",
    "print(\"Grouped DataFrame shape:\", df_grouped.shape)\n",
    "print(df_grouped.head())\n",
    "\n",
    "############################################\n",
    "# 8. SPLIT TRAIN / VALIDATION & CREATE DatasetDict\n",
    "############################################\n",
    "\n",
    "train_size = int(0.8 * len(df_grouped))\n",
    "train_df = df_grouped.iloc[:train_size]\n",
    "val_df = df_grouped.iloc[train_size:]\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Validation size:\", val_df.shape)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "############################################\n",
    "# 9. TOKENIZATION & LABEL ALIGNMENT FOR TRAINING\n",
    "############################################\n",
    "\n",
    "# Define label list (include all needed BIO tags)\n",
    "label_list = [\"O\", \"B-PER\", \"B-LOC\", \"B-MISC\"]\n",
    "label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# --- Add all known dictionary tokens to tokenizer to prevent subword-splitting ---\n",
    "special_tokens = [k for k in entity_dict.keys() if len(k) > 4]\n",
    "num_added = tokenizer.add_tokens(special_tokens)\n",
    "print(\"Number of special tokens added:\", num_added)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i in range(len(examples[\"tokens\"])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        example_labels = examples[\"ner_tags\"][i]\n",
    "        aligned_labels = []\n",
    "        prev_wid = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                # For same-word sub-token, convert B- tag to I- tag if applicable\n",
    "                label_str = example_labels[wid]\n",
    "                if wid == prev_wid and label_str != \"O\" and label_str.startswith(\"B-\"):\n",
    "                    label_str = \"I-\" + label_str[2:]\n",
    "                aligned_labels.append(label2id.get(label_str, 0))\n",
    "            prev_wid = wid\n",
    "        all_labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "print(\"Processed datasets ready for training:\")\n",
    "print(processed_datasets)\n",
    "\n",
    "############################################\n",
    "# 10. PRINT LABEL DISTRIBUTION & COMPUTE WEIGHTS\n",
    "############################################\n",
    "\n",
    "label_counts = Counter(auto_ner_df[\"ner_label\"])\n",
    "print(\"\\nLabel distribution in auto_ner_df:\", label_counts)\n",
    "\n",
    "# Compute weight: weight = 1/(count+1) for each label index\n",
    "weight_list = [1.0/(label_counts.get(lbl,0)+1) for lbl in label_list]\n",
    "weight_tensor = torch.tensor(weight_list, dtype=torch.float)\n",
    "print(\"Weight tensor:\", weight_tensor)\n",
    "\n",
    "############################################\n",
    "# 11. DEFINE CUSTOM WEIGHTED TRAINER\n",
    "############################################\n",
    "\n",
    "class WeightedNERTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Remove unwanted key if present\n",
    "        kwargs.pop(\"num_items_in_batch\", None)\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # [batch_size, seq_len, num_labels]\n",
    "        loss_mask = (labels != -100)\n",
    "        active_logits = logits.view(-1, self.model.config.num_labels)\n",
    "        active_labels = torch.where(\n",
    "            loss_mask.view(-1),\n",
    "            labels.view(-1),\n",
    "            torch.tensor(-100, device=labels.device)\n",
    "        )\n",
    "        loss = F.cross_entropy(\n",
    "            active_logits,\n",
    "            active_labels,\n",
    "            weight=self.weight,\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "############################################\n",
    "# 12. MODEL INIT & TRAINING SETUP\n",
    "############################################\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "# Resize embeddings because we added special tokens.\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.gradient_checkpointing_enable()  # reduces memory usage\n",
    "\n",
    "# Optionally compile model if using torch 2.0+ (MPS should be okay)\n",
    "if hasattr(torch, \"compile\"):\n",
    "    model = torch.compile(model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"greenlandic_ner_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=50,\n",
    "    fp16=False,  # on MPS, keep fp16 disabled\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n",
    "\n",
    "trainer = WeightedNERTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    weight=weight_tensor,\n",
    "    compute_metrics=(\n",
    "        lambda p: evaluate.load(\"seqeval\").compute(\n",
    "            predictions=[\n",
    "                [id2label[label] for label in pred if label != -100] \n",
    "                for pred in np.argmax(p[0], axis=2)\n",
    "            ],\n",
    "            references=[\n",
    "                [id2label[label] for label in gold if label != -100] \n",
    "                for gold in p[1]\n",
    "            ],\n",
    "            zero_division=0\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"greenlandic_ner_model\")\n",
    "tokenizer.save_pretrained(\"greenlandic_ner_model\")\n",
    "\n",
    "############################################\n",
    "# 13. INFERENCE\n",
    "############################################\n",
    "\n",
    "ner_infer = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"greenlandic_ner_model\",\n",
    "    tokenizer=\"greenlandic_ner_model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "test_text = \"Nukúnguasik traveled from Ikerssuaq to Nuuk.\"\n",
    "print(\"\\nInference output on sample text:\")\n",
    "print(ner_infer(test_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (51, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   story_id  51 non-null     int64 \n",
      " 1   title     51 non-null     object\n",
      " 2   text      51 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.3+ KB\n",
      "None\n",
      "Duplicate story IDs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned text:\n",
      " Our forefathers have told us much of the coming of earth, and of men, and it was a long, long while ago. Those who lived long before our day, they did not know how to store their words in little black marks, as you do; they could only tell stories. And they told of many things, and therefore we are  ...\n",
      "\n",
      "Scraped 337 candidate personal names from Wiktionary.\n",
      "Scraped 76 candidate town names from Wikipedia.\n",
      "Loaded manually curated candidate entities:\n",
      "  entity_candidate entity\n",
      "0            Ailaq  B-PER\n",
      "1             Aluk  B-PER\n",
      "2           Alátaq  B-PER\n",
      "3         Amerdloq  B-PER\n",
      "4          Anarteq  B-PER\n",
      "Final Entity Dictionary (sample):\n",
      "aaju: B-PER\n",
      "aaneeraq: B-PER\n",
      "aani: B-PER\n",
      "aaninnguaq: B-PER\n",
      "aannguaq: B-PER\n",
      "aappilattoq: B-LOC\n",
      "aaqa: B-PER\n",
      "aasiaat: B-LOC\n",
      "aggu: B-PER\n",
      "ailaq: B-PER\n",
      "aima: B-PER\n",
      "aja: B-PER\n",
      "ajaaja: B-PER\n",
      "aka: B-PER\n",
      "akisooq: B-PER\n",
      "akitsinnguaq: B-PER\n",
      "akunnaaq: B-LOC\n",
      "aleqa: B-PER\n",
      "alibak: B-PER\n",
      "alluitsup paa: B-LOC\n",
      "Saved final entity dictionary to 'final_entity_dictionary.csv'.\n",
      "\n",
      "Auto-labeled DataFrame shape: (49656, 4)\n",
      "Saved auto-labeled NER data to 'auto_ner_data.csv'.\n",
      "\n",
      "Grouped DataFrame shape: (2044, 4)\n",
      "   doc_id  sentence_id                                             tokens  \\\n",
      "0       0            0  [Our, forefathers, have, told, us, much, of, t...   \n",
      "1       0            1  [Those, who, lived, long, before, our, day, ,,...   \n",
      "2       0            2  [And, they, told, of, many, things, ,, and, th...   \n",
      "3       0            3  [Old, women, do, not, waste, their, words, idl...   \n",
      "4       0            4                      [Old, age, does, not, lie, .]   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "3   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
      "4                                 [O, O, O, O, O, O]  \n",
      "Train size: (1635, 4)\n",
      "Validation size: (409, 4)\n",
      "\n",
      "Number of special tokens added: 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1635/1635 [00:00<00:00, 5609.40 examples/s]\n",
      "Map: 100%|██████████| 409/409 [00:00<00:00, 7130.23 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed datasets ready for training:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1635\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 409\n",
      "    })\n",
      "})\n",
      "\n",
      "Label distribution in auto_ner_df: Counter({'O': 49212, 'B-PER': 390, 'B-MISC': 44, 'B-LOC': 10})\n",
      "Weight tensor: tensor([2.0320e-05, 2.5575e-03, 9.0909e-02, 2.2222e-02])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/lukaskreibig/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/2l/6514_hd91tv5448lmq79vpbw0000gn/T/ipykernel_38399/284644772.py:352: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedNERTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedNERTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nLoweringException: TypeError: 'NoneType' object is not callable\n  target: aten.cumsum.default\n  args[0]: TensorBox(StorageBox(\n    Pointwise(\n      'mps',\n      torch.int32,\n      def inner_fn(index):\n          i0, i1 = index\n          tmp0 = ops.load(primals_1, i1 + 65 * i0)\n          tmp1 = ops.constant(1, torch.int64)\n          tmp2 = tmp0 != tmp1\n          tmp3 = ops.to_dtype(tmp2, torch.int32, src_dtype=torch.bool)\n          return tmp3\n      ,\n      ranges=[8, 65],\n      origin_node=convert_element_type,\n      origins=OrderedSet([convert_element_type, ne])\n    )\n  ))\n  args[1]: 1\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 375\u001b[0m\n\u001b[1;32m    372\u001b[0m trainer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m weight_tensor\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 375\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreenlandic_ner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    378\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreenlandic_ner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/trainer.py:2556\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2549\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2550\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2553\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2554\u001b[0m )\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2556\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2559\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2560\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2561\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2562\u001b[0m ):\n\u001b[1;32m   2563\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2564\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/trainer.py:3718\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3717\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3718\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3720\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3723\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3724\u001b[0m ):\n",
      "Cell \u001b[0;32mIn[7], line 295\u001b[0m, in \u001b[0;36mWeightedNERTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    294\u001b[0m labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits  \u001b[38;5;66;03m# [batch_size, seq_length, num_labels]\u001b[39;00m\n\u001b[1;32m    297\u001b[0m loss_mask \u001b[38;5;241m=\u001b[39m (labels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:574\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    570\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    578\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    579\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:1380\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1375\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1376\u001b[0m             )\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:1164\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m   1162\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1164\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:547\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    544\u001b[0m     dynamo_tls\u001b[38;5;241m.\u001b[39mtraced_frame_infos\u001b[38;5;241m.\u001b[39mappend(info)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_context(CompileContext(compile_id)):\n\u001b[0;32m--> 547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:986\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    984\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 986\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m# NB: We only put_code_state in success case.  Success case here\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;66;03m# does include graph breaks; specifically, if a graph break still\u001b[39;00m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;66;03m# resulted in a partially compiled graph, we WILL return here.  An\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[38;5;66;03m# to upload for graph break though, because this can prevent\u001b[39;00m\n\u001b[1;32m    996\u001b[0m     \u001b[38;5;66;03m# extra graph break compilations.)\u001b[39;00m\n\u001b[1;32m    997\u001b[0m     put_code_state()\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:715\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    713\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39minstall_callbacks())\n\u001b[1;32m    714\u001b[0m     stack\u001b[38;5;241m.\u001b[39menter_context(CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord())\n\u001b[0;32m--> 715\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_utils_internal.py:95\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m skip \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[1;32m     98\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     99\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:750\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    748\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 750\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1361\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1358\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1359\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1361\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:231\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(torch_function_mode_stack_state_mgr)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:662\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 662\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    664\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2868\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2867\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2868\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:1052\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m-> 1052\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1053\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:962\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 962\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:3048\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3047\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mRETURN_VALUE\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 3048\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:3033\u001b[0m, in \u001b[0;36mInstructionTranslator._return\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3028\u001b[0m _step_logger()(\n\u001b[1;32m   3029\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m   3030\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3031\u001b[0m )\n\u001b[1;32m   3032\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m, inst\u001b[38;5;241m.\u001b[39mopname)\n\u001b[0;32m-> 3033\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3034\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3036\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   3037\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3038\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3039\u001b[0m return_inst \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3040\u001b[0m     create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_CONST\u001b[39m\u001b[38;5;124m\"\u001b[39m, argval\u001b[38;5;241m=\u001b[39minst\u001b[38;5;241m.\u001b[39margval)\n\u001b[1;32m   3043\u001b[0m )\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([return_inst])\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1136\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m   1133\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count_calls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2\u001b[38;5;241m.\u001b[39mgraph_outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1135\u001b[0m     output\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m-> 1136\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_output_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_replacements\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m     )\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2\u001b[38;5;241m.\u001b[39mgraph_outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1142\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(pass2\u001b[38;5;241m.\u001b[39mcreate_store(graph_output_var))\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1382\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root, replaced_outputs)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[0;32m-> 1382\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lazy_graph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LazyGraphModule\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, _LazyGraphModule) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(compiled_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m), _LazyGraphModule)\n\u001b[1;32m   1388\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m compiled_fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_lazy_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1392\u001b[0m     \u001b[38;5;66;03m# this is a _LazyGraphModule. This makes it easier for dynamo to\u001b[39;00m\n\u001b[1;32m   1393\u001b[0m     \u001b[38;5;66;03m# optimize a _LazyGraphModule.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1432\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_user_compiler\u001b[39m(\u001b[38;5;28mself\u001b[39m, gm: fx\u001b[38;5;241m.\u001b[39mGraphModule) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompiledFn:\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m   1427\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1428\u001b[0m         phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1429\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1430\u001b[0m         dynamo_compile_column_us\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd_cumulative_compile_time_us\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1431\u001b[0m     ):\n\u001b[0;32m-> 1432\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1483\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1481\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1482\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e)\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[1;32m   1484\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m   1485\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m signpost_event(\n\u001b[1;32m   1488\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1495\u001b[0m     },\n\u001b[1;32m   1496\u001b[0m )\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1462\u001b[0m, in \u001b[0;36mOutputGraph._call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[1;32m   1461\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[0;32m-> 1462\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1463\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py:130\u001b[0m, in \u001b[0;36mWrapBackendDebug.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/__init__.py:2340\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   2337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[1;32m   2338\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 2340\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:1863\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   1856\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inference_compiler(unlifted_gm, example_inputs_)\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[1;32m   1859\u001b[0m     tracing_context\n\u001b[1;32m   1860\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39m_disable(), functorch_config\u001b[38;5;241m.\u001b[39mpatch(\n\u001b[1;32m   1861\u001b[0m     unlift_effect_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m ):\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1866\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/backends/common.py:83\u001b[0m, in \u001b[0;36mAotAutograd.__call__\u001b[0;34m(self, gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[0;32m---> 83\u001b[0m         cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1155\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler, cudagraphs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m AOTAutogradCache\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m   1146\u001b[0m         dispatch_and_compile,\n\u001b[1;32m   1147\u001b[0m         mod,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m         remote,\n\u001b[1;32m   1153\u001b[0m     )\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1155\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mdispatch_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mGmWrapper):\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;66;03m# This function is called by the flatten_graph_inputs wrapper, which boxes\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;66;03m# the inputs so that they can be freed before the end of this scope.\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;66;03m# For overhead reasons, this is not the default wrapper, see comment:\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/122535/files#r1560096481\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mboxed_forward\u001b[39m(runtime_args: List[Any]):\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1131\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.dispatch_and_compile\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1129\u001b[0m functional_call \u001b[38;5;241m=\u001b[39m create_functional_call(mod, params_spec, params_len)\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39m_disable():\n\u001b[0;32m-> 1131\u001b[0m     compiled_fn, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshape_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:580\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_aot_dispatcher_function\u001b[39m(\n\u001b[1;32m    573\u001b[0m     flat_fn,\n\u001b[1;32m    574\u001b[0m     fake_flat_args: FakifiedFlatArgs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m     shape_env: Optional[ShapeEnv],\n\u001b[1;32m    578\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Callable, ViewAndMutationMeta]:\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_aot_dispatcher_function\u001b[39m\u001b[38;5;124m\"\u001b[39m, log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 580\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m            \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_env\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:830\u001b[0m, in \u001b[0;36m_create_aot_dispatcher_function\u001b[0;34m(flat_fn, fake_flat_args, aot_config, fake_mode, shape_env)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m aot_dispatch_base\n\u001b[1;32m    828\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m choose_dispatcher(needs_autograd, aot_config)\n\u001b[0;32m--> 830\u001b[0m compiled_fn, fw_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_dup_fake_script_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn, fw_metadata\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:678\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m    675\u001b[0m     tracing_context\u001b[38;5;241m.\u001b[39mfw_metadata \u001b[38;5;241m=\u001b[39m inner_meta\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TracingContext\u001b[38;5;241m.\u001b[39mreport_output_strides() \u001b[38;5;28;01mas\u001b[39;00m fwd_output_strides:\n\u001b[0;32m--> 678\u001b[0m     compiled_fw_func \u001b[38;5;241m=\u001b[39m \u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjusted_flat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(compiled_fw_func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    681\u001b[0m     compiled_fw_func \u001b[38;5;241m=\u001b[39m make_boxed_func(compiled_fw_func)\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:489\u001b[0m, in \u001b[0;36mSerializableAOTDispatchCompiler.__call__\u001b[0;34m(self, gm, example_inputs)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    486\u001b[0m     gm: torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule,\n\u001b[1;32m    487\u001b[0m     example_inputs: Sequence[InputType],\n\u001b[1;32m    488\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutputCode:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:1741\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler_base\u001b[0;34m(gm, example_inputs, is_inference)\u001b[0m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1739\u001b[0m     model_outputs_node\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_visible_output_idxs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatic_input_idxs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_static_input_idxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxed_forward_device_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:569\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m stack\u001b[38;5;241m.\u001b[39menter_context(DebugContext())\n\u001b[1;32m    564\u001b[0m get_chromium_event_logger()\u001b[38;5;241m.\u001b[39madd_event_data(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    566\u001b[0m     is_backward\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_backward\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    567\u001b[0m )\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_compiler_debug\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_compile_fx_inner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompiler_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minductor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py:102\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     inner_compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:685\u001b[0m, in \u001b[0;36m_compile_fx_inner\u001b[0;34m(gm, example_inputs, **graph_kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m TritonBundler\u001b[38;5;241m.\u001b[39mbegin_compile()\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 685\u001b[0m     mb_compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgraph_kwargs\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     mb_compiled_graph\u001b[38;5;241m.\u001b[39m_time_taken_ns \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:1129\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[0;34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfx_codegen_and_compile\u001b[39m(\n\u001b[1;32m   1120\u001b[0m     gm: GraphModule,\n\u001b[1;32m   1121\u001b[0m     example_inputs: Sequence[InputType],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgraph_kwargs: Unpack[_CompileFxKwargs],\n\u001b[1;32m   1126\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutputCode:\n\u001b[1;32m   1127\u001b[0m     scheme: FxCompile \u001b[38;5;241m=\u001b[39m _InProcessFxCompile()\n\u001b[0;32m-> 1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:979\u001b[0m, in \u001b[0;36m_InProcessFxCompile.codegen_and_compile\u001b[0;34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[0m\n\u001b[1;32m    977\u001b[0m metrics_helper \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mCachedMetricsHelper()\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_graph_handler(graph):\n\u001b[0;32m--> 979\u001b[0m     \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m     output_strides: List[Optional[Tuple[_StrideExprStr, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mgraph_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;66;03m# We'll put the output strides in the compiled graph so we\u001b[39;00m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;66;03m# can later return them to the caller via TracingContext\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/graph.py:855\u001b[0m, in \u001b[0;36mGraphLowering.run\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphLowering.run\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 855\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/fx/interpreter.py:167\u001b[0m, in \u001b[0;36mInterpreter.run\u001b[0;34m(self, initial_env, enable_io_processing, *args)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv[node] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_traceback:\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/graph.py:1496\u001b[0m, in \u001b[0;36mGraphLowering.run_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1495\u001b[0m     debug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;66;03m# require the same stride order for dense outputs,\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;66;03m# 1. user-land view() will not throw because inductor\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;66;03m# output different strides than eager\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# 2: as_strided ops, we need make sure its input has same size/stride with\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# eager model to align with eager behavior.\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m as_strided_ops \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1506\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39mas_strided\u001b[38;5;241m.\u001b[39mdefault,\n\u001b[1;32m   1507\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39mas_strided_\u001b[38;5;241m.\u001b[39mdefault,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1510\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39maten\u001b[38;5;241m.\u001b[39mresize_as\u001b[38;5;241m.\u001b[39mdefault,\n\u001b[1;32m   1511\u001b[0m ]\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/fx/interpreter.py:230\u001b[0m, in \u001b[0;36mInterpreter.run_node\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kwargs, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/graph.py:1143\u001b[0m, in \u001b[0;36mGraphLowering.call_function\u001b[0;34m(self, target, args, kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LoweringException(e, target, args, kwargs)\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[1;32m   1144\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m   1145\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/graph.py:1133\u001b[0m, in \u001b[0;36mGraphLowering.call_function\u001b[0;34m(self, target, args, kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     old_args, old_kwargs \u001b[38;5;241m=\u001b[39m args, kwargs\n\u001b[1;32m   1131\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m layout_constraints(n, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1133\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mlowerings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layout_constraints:\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;66;03m# layout_constraints are allowed to make new copies of the inputs.\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;66;03m# if they do, and if the target is mutable, then we need to\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;66;03m# write the new values back into the original inputs.\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate_mutation(n, old_args, old_kwargs, args, kwargs)  \u001b[38;5;66;03m# type: ignore[possibly-undefined]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/lowering.py:409\u001b[0m, in \u001b[0;36m_register_lowering.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unpacked:\n\u001b[1;32m    407\u001b[0m     args \u001b[38;5;241m=\u001b[39m [args]\n\u001b[0;32m--> 409\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mdecomp_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m validate_ir(out)\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/lowering.py:5768\u001b[0m, in \u001b[0;36mcumsum\u001b[0;34m(x, axis, dtype)\u001b[0m\n\u001b[1;32m   5765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (ops\u001b[38;5;241m.\u001b[39madd(a, b),)\n\u001b[1;32m   5767\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m _make_scan_inner(x, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m-> 5768\u001b[0m (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mir\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mScan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombine_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5770\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fallback_cumsum(x, dim\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/ir.py:2004\u001b[0m, in \u001b[0;36mScan.create\u001b[0;34m(cls, device, dtypes, inner_fns, size, axis, combine_fn, reduction_hint, can_fallback_to_aten, **kwargs)\u001b[0m\n\u001b[1;32m   2001\u001b[0m pointwise_ranges \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39msize[:axis], \u001b[38;5;241m*\u001b[39msize[axis \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m :]]\n\u001b[1;32m   2002\u001b[0m scan_ranges \u001b[38;5;241m=\u001b[39m [size[axis]]\n\u001b[0;32m-> 2004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mV\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBackendFeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCAN\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2005\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dtypes)\n\u001b[1;32m   2007\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dtypes) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m V\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mhas_feature(\n\u001b[1;32m   2008\u001b[0m     device, BackendFeature\u001b[38;5;241m.\u001b[39mTUPLE_REDUCTION\n\u001b[1;32m   2009\u001b[0m ):\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/graph.py:505\u001b[0m, in \u001b[0;36mGraphLowering.has_feature\u001b[0;34m(self, device, feature)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhas_feature\u001b[39m(\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    501\u001b[0m     device: Union[torch\u001b[38;5;241m.\u001b[39m_inductor\u001b[38;5;241m.\u001b[39mir\u001b[38;5;241m.\u001b[39mIRNode, device, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m    502\u001b[0m     feature: BackendFeature,\n\u001b[1;32m    503\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feature, BackendFeature), feature\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_backend_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_device_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/torch/_inductor/codegen/common.py:324\u001b[0m, in \u001b[0;36mget_backend_features\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    322\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(device_type)\n\u001b[1;32m    323\u001b[0m scheduling \u001b[38;5;241m=\u001b[39m get_scheduling_for_device(device_type)\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheduling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_backend_features(device)\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: backend='inductor' raised:\nLoweringException: TypeError: 'NoneType' object is not callable\n  target: aten.cumsum.default\n  args[0]: TensorBox(StorageBox(\n    Pointwise(\n      'mps',\n      torch.int32,\n      def inner_fn(index):\n          i0, i1 = index\n          tmp0 = ops.load(primals_1, i1 + 65 * i0)\n          tmp1 = ops.constant(1, torch.int64)\n          tmp2 = tmp0 != tmp1\n          tmp3 = ops.to_dtype(tmp2, torch.int32, src_dtype=torch.bool)\n          return tmp3\n      ,\n      ranges=[8, 65],\n      origin_node=convert_element_type,\n      origins=OrderedSet([convert_element_type, ne])\n    )\n  ))\n  args[1]: 1\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# 1. SETUP & IMPORTS\n",
    "############################################\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Adjust for your Apple MPS backend\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    pipeline, AutoTokenizer, AutoModelForTokenClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "\n",
    "############################################\n",
    "# 2. LOAD & EXPLORE FOLKTALES DATA\n",
    "############################################\n",
    "\n",
    "df = pd.read_pickle(\"eskimo_folktales.pkl\")\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "print(df.info())\n",
    "print(\"Duplicate story IDs:\", df.story_id.duplicated().sum())\n",
    "\n",
    "df[\"text_length\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "plt.hist(df[\"text_length\"], bins=20)\n",
    "plt.title(\"Distribution of Story Lengths\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Number of Stories\")\n",
    "plt.show()\n",
    "\n",
    "############################################\n",
    "# 3. CLEAN THE TEXT\n",
    "############################################\n",
    "\n",
    "def clean_text_for_ner(text: str) -> str:\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    paragraphs = re.split(r'\\n\\s*\\n+', text.strip())\n",
    "    cleaned_paragraphs = []\n",
    "    for para in paragraphs:\n",
    "        para = re.sub(r'\\n+', ' ', para)\n",
    "        para = para.replace('’', \"'\").replace('‘', \"'\").replace('—', '-')\n",
    "        para = re.sub(r'\\s+', ' ', para).strip()\n",
    "        cleaned_paragraphs.append(para)\n",
    "    return \"\\n\\n\".join(cleaned_paragraphs)\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_for_ner)\n",
    "print(\"Sample cleaned text:\\n\", df[\"clean_text\"].iloc[0][:300], \"...\\n\")\n",
    "\n",
    "############################################\n",
    "# 4. SCRAPE CANDIDATE ENTITIES FROM THE WEB\n",
    "############################################\n",
    "\n",
    "# 4a. Scrape candidate personal names from Wiktionary\n",
    "url_names = \"https://en.wiktionary.org/wiki/Appendix:Greenlandic_given_names\"\n",
    "resp_names = requests.get(url_names)\n",
    "soup_names = BeautifulSoup(resp_names.text, \"html.parser\")\n",
    "scraped_names = set()\n",
    "for dd in soup_names.select(\"dl dd\"):\n",
    "    for link in dd.find_all(\"a\"):\n",
    "        candidate = link.get_text(strip=True)\n",
    "        if candidate and len(candidate) > 1:\n",
    "            scraped_names.add(candidate)\n",
    "print(f\"Scraped {len(scraped_names)} candidate personal names from Wiktionary.\")\n",
    "\n",
    "# 4b. Scrape candidate town names from Wikipedia\n",
    "url_towns = \"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Greenland\"\n",
    "resp_towns = requests.get(url_towns)\n",
    "soup_towns = BeautifulSoup(resp_towns.text, \"html.parser\")\n",
    "scraped_towns = set()\n",
    "tables = soup_towns.find_all(\"table\", class_=\"wikitable\")\n",
    "for table in tables:\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        for link in row.find_all(\"a\", href=True):\n",
    "            candidate = link.get_text(strip=True)\n",
    "            if candidate and len(candidate) > 1:\n",
    "                if any(bad in candidate.lower() for bad in [\n",
    "                        \"edit\", \"coordinate\", \"article\", \"statement\", \"isbn\",\n",
    "                        \"list of\", \"administrative\", \"autonomy\", \"history\", \"portal\"\n",
    "                ]):\n",
    "                    continue\n",
    "                scraped_towns.add(candidate)\n",
    "print(f\"Scraped {len(scraped_towns)} candidate town names from Wikipedia.\")\n",
    "\n",
    "############################################\n",
    "# 5. LOAD & MERGE MANUALLY CURATED CANDIDATE CSV\n",
    "############################################\n",
    "\n",
    "try:\n",
    "    manual_df = pd.read_csv(\"candidate_entities_finished.csv\")\n",
    "    print(\"Loaded manually curated candidate entities:\")\n",
    "    print(manual_df.head())\n",
    "    manual_df[\"entity_candidate\"] = manual_df[\"entity_candidate\"].str.strip().str.lower()\n",
    "    manual_df[\"entity\"] = manual_df[\"entity\"].str.strip().replace({\"B-O\": \"O\", \"B-O \": \"O\"})\n",
    "    manual_dict = dict(zip(manual_df[\"entity_candidate\"], manual_df[\"entity\"]))\n",
    "except Exception as e:\n",
    "    print(\"Manual candidate CSV not found; proceeding with scraped data only.\")\n",
    "    manual_dict = {}\n",
    "\n",
    "entity_dict = manual_dict.copy()\n",
    "for name in scraped_names:\n",
    "    key = name.strip().lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-PER\"\n",
    "for town in scraped_towns:\n",
    "    key = town.strip().lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-LOC\"\n",
    "\n",
    "print(\"Final Entity Dictionary (sample):\")\n",
    "for key, val in sorted(entity_dict.items())[:20]:\n",
    "    print(f\"{key}: {val}\")\n",
    "\n",
    "final_entity_df = pd.DataFrame(list(entity_dict.items()), columns=[\"entity_candidate\", \"entity\"])\n",
    "final_entity_df.to_csv(\"final_entity_dictionary.csv\", index=False)\n",
    "print(\"Saved final entity dictionary to 'final_entity_dictionary.csv'.\")\n",
    "\n",
    "############################################\n",
    "# 6. AUTO-LABEL FOLKTALE TEXTS (BIO FORMAT)\n",
    "############################################\n",
    "\n",
    "def get_entity_label_bio(token, entity_dict, prev_entity):\n",
    "    token_lower = token.strip().lower()\n",
    "    if token_lower in entity_dict:\n",
    "        label = entity_dict[token_lower]\n",
    "    elif token_lower.endswith(\"s\"):\n",
    "        label = entity_dict.get(token_lower[:-1], \"O\")\n",
    "    else:\n",
    "        label = \"O\"\n",
    "    if label == \"O\":\n",
    "        return \"O\", None\n",
    "    entity_type = label.split(\"-\", 1)[-1]\n",
    "    if prev_entity == entity_type:\n",
    "        return f\"I-{entity_type}\", entity_type\n",
    "    else:\n",
    "        return f\"B-{entity_type}\", entity_type\n",
    "\n",
    "def auto_label_bio_using_dict(text, entity_dict):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    data_rows = []\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        prev_entity = None\n",
    "        for token in tokens:\n",
    "            bio_label, current_entity = get_entity_label_bio(token, entity_dict, prev_entity)\n",
    "            data_rows.append({\n",
    "                \"sentence_id\": sent_id,\n",
    "                \"token\": token,\n",
    "                \"ner_label\": bio_label\n",
    "            })\n",
    "            prev_entity = current_entity if bio_label != \"O\" else None\n",
    "    return data_rows\n",
    "\n",
    "all_rows = []\n",
    "doc_id = 0\n",
    "for _, row in df.iterrows():\n",
    "    labeled_tokens = auto_label_bio_using_dict(row[\"clean_text\"], entity_dict)\n",
    "    for item in labeled_tokens:\n",
    "        all_rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sentence_id\": item[\"sentence_id\"],\n",
    "            \"token\": item[\"token\"],\n",
    "            \"ner_label\": item[\"ner_label\"]\n",
    "        })\n",
    "    doc_id += 1\n",
    "\n",
    "auto_ner_df = pd.DataFrame(all_rows)\n",
    "auto_ner_df.to_csv(\"auto_ner_data.csv\", index=False)\n",
    "print(\"\\nAuto-labeled DataFrame shape:\", auto_ner_df.shape)\n",
    "print(\"Saved auto-labeled NER data to 'auto_ner_data.csv'.\")\n",
    "\n",
    "############################################\n",
    "# 7. GROUP TOKENS BY SENTENCE FOR TRAINING EXAMPLES\n",
    "############################################\n",
    "\n",
    "grouped = auto_ner_df.groupby([\"doc_id\", \"sentence_id\"])\n",
    "examples = []\n",
    "for (doc_id, sent_id), group in grouped:\n",
    "    tokens = group[\"token\"].tolist()\n",
    "    labels = group[\"ner_label\"].tolist()\n",
    "    examples.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"sentence_id\": sent_id,\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": labels\n",
    "    })\n",
    "df_grouped = pd.DataFrame(examples)\n",
    "print(\"\\nGrouped DataFrame shape:\", df_grouped.shape)\n",
    "print(df_grouped.head())\n",
    "\n",
    "############################################\n",
    "# 8. SPLIT TRAIN/VALIDATION & CREATE DatasetDict\n",
    "############################################\n",
    "\n",
    "train_size = int(0.8 * len(df_grouped))\n",
    "train_df = df_grouped.iloc[:train_size]\n",
    "val_df = df_grouped.iloc[train_size:]\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Validation size:\", val_df.shape)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "############################################\n",
    "# 9. TOKENIZATION & LABEL ALIGNMENT FOR TRAINING\n",
    "############################################\n",
    "\n",
    "# Define label list (using standard BIO tags)\n",
    "label_list = [\"O\", \"B-PER\", \"B-LOC\", \"B-MISC\"]\n",
    "label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# --- Add all dictionary keys (special tokens) to tokenizer ---\n",
    "special_tokens = [k for k in entity_dict.keys() if len(k) > 4]\n",
    "num_added = tokenizer.add_tokens(special_tokens)\n",
    "print(\"\\nNumber of special tokens added:\", num_added)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i in range(len(examples[\"tokens\"])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        example_labels = examples[\"ner_tags\"][i]\n",
    "        aligned_labels = []\n",
    "        prev_wid = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                label_str = example_labels[wid]\n",
    "                # If token is a subsequent subword token of the same word, change B- to I-\n",
    "                if wid == prev_wid and label_str != \"O\" and label_str.startswith(\"B-\"):\n",
    "                    label_str = \"I-\" + label_str[2:]\n",
    "                aligned_labels.append(label2id.get(label_str, 0))\n",
    "            prev_wid = wid\n",
    "        all_labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "print(\"\\nProcessed datasets ready for training:\")\n",
    "print(processed_datasets)\n",
    "\n",
    "############################################\n",
    "# 10. SHOW LABEL DISTRIBUTION & COMPUTE WEIGHTS\n",
    "############################################\n",
    "\n",
    "label_counts = Counter(auto_ner_df[\"ner_label\"])\n",
    "print(\"\\nLabel distribution in auto_ner_df:\", label_counts)\n",
    "\n",
    "# Compute weight for each label as 1/(count+1)\n",
    "weight_list = [1.0 / (label_counts.get(lbl, 0) + 1) for lbl in label_list]\n",
    "weight_tensor = torch.tensor(weight_list, dtype=torch.float)\n",
    "print(\"Weight tensor:\", weight_tensor)\n",
    "\n",
    "############################################\n",
    "# 11. DEFINE CUSTOM WEIGHTED TRAINER\n",
    "############################################\n",
    "\n",
    "class WeightedNERTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Remove any extraneous keyword arguments\n",
    "        kwargs.pop(\"num_items_in_batch\", None)\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # [batch_size, seq_length, num_labels]\n",
    "        loss_mask = (labels != -100)\n",
    "        active_logits = logits.view(-1, self.model.config.num_labels)\n",
    "        active_labels = torch.where(\n",
    "            loss_mask.view(-1),\n",
    "            labels.view(-1),\n",
    "            torch.tensor(-100, device=labels.device)\n",
    "        )\n",
    "        loss = F.cross_entropy(\n",
    "            active_logits,\n",
    "            active_labels,\n",
    "            weight=self.weight,\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "############################################\n",
    "# 12. MODEL INIT & TRAINING SETUP\n",
    "############################################\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "# Resize embeddings to account for new special tokens.\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.gradient_checkpointing_enable()  # Reduce memory usage\n",
    "\n",
    "# Try compiling the model; if it fails, suppress errors and fall back to eager mode.\n",
    "if hasattr(torch, \"compile\"):\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "    except Exception as e:\n",
    "        print(\"Model compilation failed, falling back to eager mode.\")\n",
    "        import torch._dynamo\n",
    "        torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"greenlandic_ner_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=50,\n",
    "    fp16=False,  # Disable fp16 on MPS\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n",
    "\n",
    "trainer = WeightedNERTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda p: evaluate.load(\"seqeval\").compute(\n",
    "        predictions=[\n",
    "            [id2label[label] for label in np.argmax(p[0], axis=2)[i] if label != -100]\n",
    "            for i in range(len(p[1]))\n",
    "        ],\n",
    "        references=[\n",
    "            [id2label[label] for label in p[1][i] if label != -100]\n",
    "            for i in range(len(p[1]))\n",
    "        ],\n",
    "        zero_division=0\n",
    "    )\n",
    ")\n",
    "# Set weight tensor on trainer (used in compute_loss)\n",
    "trainer.weight = weight_tensor\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"greenlandic_ner_model\")\n",
    "tokenizer.save_pretrained(\"greenlandic_ner_model\")\n",
    "\n",
    "############################################\n",
    "# 13. INFERENCE\n",
    "############################################\n",
    "\n",
    "ner_infer = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"greenlandic_ner_model\",\n",
    "    tokenizer=\"greenlandic_ner_model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "test_text = \"Nukúnguasik traveled from Ikerssuaq to Nuuk.\"\n",
    "print(\"\\nInference output on sample text:\")\n",
    "print(ner_infer(test_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lukaskreibig/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded. Shape: (51, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   story_id  51 non-null     int64 \n",
      " 1   title     51 non-null     object\n",
      " 2   text      51 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.3+ KB\n",
      "None\n",
      "Duplicate story IDs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned text:\n",
      " Our forefathers have told us much of the coming of earth, and of men, and it was a long, long while ago. Those who lived long before our day, they did not know how to store their words in little black marks, as you do; they could only tell stories. And they told of many things, and therefore we are  ...\n",
      "\n",
      "Scraped 337 candidate personal names from Wiktionary.\n",
      "Scraped 76 candidate town names from Wikipedia.\n",
      "Loaded manually curated candidate entities:\n",
      "  entity_candidate entity\n",
      "0            Ailaq  B-PER\n",
      "1             Aluk  B-PER\n",
      "2           Alátaq  B-PER\n",
      "3         Amerdloq  B-PER\n",
      "4          Anarteq  B-PER\n",
      "Final Entity Dictionary (sample):\n",
      "aaju: B-PER\n",
      "aaneeraq: B-PER\n",
      "aani: B-PER\n",
      "aaninnguaq: B-PER\n",
      "aannguaq: B-PER\n",
      "aappilattoq: B-LOC\n",
      "aaqa: B-PER\n",
      "aasiaat: B-LOC\n",
      "aggu: B-PER\n",
      "ailaq: B-PER\n",
      "aima: B-PER\n",
      "aja: B-PER\n",
      "ajaaja: B-PER\n",
      "aka: B-PER\n",
      "akisooq: B-PER\n",
      "akitsinnguaq: B-PER\n",
      "akunnaaq: B-LOC\n",
      "aleqa: B-PER\n",
      "alibak: B-PER\n",
      "alluitsup paa: B-LOC\n",
      "Saved final entity dictionary to 'final_entity_dictionary.csv'.\n",
      "\n",
      "Auto-labeled DataFrame shape: (49656, 4)\n",
      "Saved auto-labeled NER data to 'auto_ner_data.csv'.\n",
      "\n",
      "Grouped DataFrame shape: (2044, 4)\n",
      "   doc_id  sentence_id                                             tokens  \\\n",
      "0       0            0  [Our, forefathers, have, told, us, much, of, t...   \n",
      "1       0            1  [Those, who, lived, long, before, our, day, ,,...   \n",
      "2       0            2  [And, they, told, of, many, things, ,, and, th...   \n",
      "3       0            3  [Old, women, do, not, waste, their, words, idl...   \n",
      "4       0            4                      [Old, age, does, not, lie, .]   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "3   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
      "4                                 [O, O, O, O, O, O]  \n",
      "Train size: (1635, 4)\n",
      "Validation size: (409, 4)\n",
      "\n",
      "Number of special tokens added: 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1635/1635 [00:00<00:00, 15676.33 examples/s]\n",
      "Map: 100%|██████████| 409/409 [00:00<00:00, 17595.11 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed datasets ready for training:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1635\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 409\n",
      "    })\n",
      "})\n",
      "\n",
      "Label distribution in auto_ner_df: Counter({'O': 49212, 'B-PER': 390, 'B-MISC': 44, 'B-LOC': 10})\n",
      "Weight tensor: tensor([2.0320e-05, 2.5575e-03, 9.0909e-02, 2.2222e-02])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "/Users/lukaskreibig/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/2l/6514_hd91tv5448lmq79vpbw0000gn/T/ipykernel_41146/2525380503.py:354: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4085' max='4085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4085/4085 41:59, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.955975</td>\n",
       "      <td>0.999500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.986842</td>\n",
       "      <td>0.974026</td>\n",
       "      <td>0.980392</td>\n",
       "      <td>0.999833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.007599</td>\n",
       "      <td>0.835165</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.904762</td>\n",
       "      <td>0.998750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.938272</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.962025</td>\n",
       "      <td>0.999583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference output on sample text:\n",
      "[{'entity_group': 'PER', 'score': 0.99882084, 'word': 'Nuk', 'start': 0, 'end': 3}, {'entity_group': 'LOC', 'score': 0.98761046, 'word': 'I', 'start': 26, 'end': 27}, {'entity_group': 'LOC', 'score': 0.7367588, 'word': 'Nu', 'start': 39, 'end': 41}]\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# 1. SETUP & IMPORTS\n",
    "############################################\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"  # Allow up to 90% of MPS memory\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    pipeline, AutoTokenizer, AutoModelForTokenClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "\n",
    "############################################\n",
    "# 2. LOAD & EXPLORE FOLKTALES DATA\n",
    "############################################\n",
    "\n",
    "df = pd.read_pickle(\"eskimo_folktales.pkl\")\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "print(df.info())\n",
    "print(\"Duplicate story IDs:\", df.story_id.duplicated().sum())\n",
    "\n",
    "df[\"text_length\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "plt.hist(df[\"text_length\"], bins=20)\n",
    "plt.title(\"Distribution of Story Lengths\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Number of Stories\")\n",
    "plt.show()\n",
    "\n",
    "############################################\n",
    "# 3. CLEAN THE TEXT\n",
    "############################################\n",
    "\n",
    "def clean_text_for_ner(text: str) -> str:\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    paragraphs = re.split(r'\\n\\s*\\n+', text.strip())\n",
    "    cleaned_paragraphs = []\n",
    "    for para in paragraphs:\n",
    "        para = re.sub(r'\\n+', ' ', para)\n",
    "        para = para.replace('’', \"'\").replace('‘', \"'\").replace('—', '-')\n",
    "        para = re.sub(r'\\s+', ' ', para).strip()\n",
    "        cleaned_paragraphs.append(para)\n",
    "    return \"\\n\\n\".join(cleaned_paragraphs)\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_for_ner)\n",
    "print(\"Sample cleaned text:\\n\", df[\"clean_text\"].iloc[0][:300], \"...\\n\")\n",
    "\n",
    "############################################\n",
    "# 4. SCRAPE CANDIDATE ENTITIES FROM THE WEB\n",
    "############################################\n",
    "\n",
    "# 4a. Scrape candidate personal names from Wiktionary\n",
    "url_names = \"https://en.wiktionary.org/wiki/Appendix:Greenlandic_given_names\"\n",
    "resp_names = requests.get(url_names)\n",
    "soup_names = BeautifulSoup(resp_names.text, \"html.parser\")\n",
    "scraped_names = set()\n",
    "for dd in soup_names.select(\"dl dd\"):\n",
    "    for link in dd.find_all(\"a\"):\n",
    "        candidate = link.get_text(strip=True)\n",
    "        if candidate and len(candidate) > 1:\n",
    "            scraped_names.add(candidate)\n",
    "print(f\"Scraped {len(scraped_names)} candidate personal names from Wiktionary.\")\n",
    "\n",
    "# 4b. Scrape candidate town names from Wikipedia\n",
    "url_towns = \"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Greenland\"\n",
    "resp_towns = requests.get(url_towns)\n",
    "soup_towns = BeautifulSoup(resp_towns.text, \"html.parser\")\n",
    "scraped_towns = set()\n",
    "tables = soup_towns.find_all(\"table\", class_=\"wikitable\")\n",
    "for table in tables:\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        for link in row.find_all(\"a\", href=True):\n",
    "            candidate = link.get_text(strip=True)\n",
    "            if candidate and len(candidate) > 1:\n",
    "                # Skip typical noisy links\n",
    "                if any(bad in candidate.lower() for bad in [\n",
    "                    \"edit\", \"coordinate\", \"article\", \"statement\", \"isbn\",\n",
    "                    \"list of\", \"administrative\", \"autonomy\", \"history\", \"portal\"\n",
    "                ]):\n",
    "                    continue\n",
    "                scraped_towns.add(candidate)\n",
    "print(f\"Scraped {len(scraped_towns)} candidate town names from Wikipedia.\")\n",
    "\n",
    "############################################\n",
    "# 5. LOAD & MERGE MANUALLY CURATED CANDIDATE CSV\n",
    "############################################\n",
    "\n",
    "try:\n",
    "    manual_df = pd.read_csv(\"candidate_entities_finished.csv\")\n",
    "    print(\"Loaded manually curated candidate entities:\")\n",
    "    print(manual_df.head())\n",
    "    # Standardize: strip and lower-case keys; fix any O labels\n",
    "    manual_df[\"entity_candidate\"] = manual_df[\"entity_candidate\"].str.strip().str.lower()\n",
    "    manual_df[\"entity\"] = manual_df[\"entity\"].str.strip().replace({\"B-O\": \"O\", \"B-O \": \"O\"})\n",
    "    manual_dict = dict(zip(manual_df[\"entity_candidate\"], manual_df[\"entity\"]))\n",
    "except Exception as e:\n",
    "    print(\"Manual candidate CSV not found; proceeding with scraped data only.\")\n",
    "    manual_dict = {}\n",
    "\n",
    "entity_dict = manual_dict.copy()\n",
    "for name in scraped_names:\n",
    "    key = name.strip().lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-PER\"  # Default scraped personal names as B-PER\n",
    "for town in scraped_towns:\n",
    "    key = town.strip().lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-LOC\"  # Default scraped town names as B-LOC\n",
    "\n",
    "print(\"Final Entity Dictionary (sample):\")\n",
    "for key, val in sorted(entity_dict.items())[:20]:\n",
    "    print(f\"{key}: {val}\")\n",
    "\n",
    "final_entity_df = pd.DataFrame(list(entity_dict.items()), columns=[\"entity_candidate\", \"entity\"])\n",
    "final_entity_df.to_csv(\"final_entity_dictionary.csv\", index=False)\n",
    "print(\"Saved final entity dictionary to 'final_entity_dictionary.csv'.\")\n",
    "\n",
    "############################################\n",
    "# 6. AUTO-LABEL FOLKTALE TEXTS USING ENTITY DICTIONARY (BIO FORMAT)\n",
    "############################################\n",
    "\n",
    "def get_entity_label_bio(token, entity_dict, prev_entity):\n",
    "    token_lower = token.strip().lower()\n",
    "    if token_lower in entity_dict:\n",
    "        label = entity_dict[token_lower]\n",
    "    elif token_lower.endswith(\"s\"):\n",
    "        label = entity_dict.get(token_lower[:-1], \"O\")\n",
    "    else:\n",
    "        label = \"O\"\n",
    "    if label == \"O\":\n",
    "        return \"O\", None\n",
    "    entity_type = label.split(\"-\", 1)[-1]\n",
    "    if prev_entity == entity_type:\n",
    "        return f\"I-{entity_type}\", entity_type\n",
    "    else:\n",
    "        return f\"B-{entity_type}\", entity_type\n",
    "\n",
    "def auto_label_bio_using_dict(text, entity_dict):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    data_rows = []\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        prev_entity = None\n",
    "        for token in tokens:\n",
    "            bio_label, current_entity = get_entity_label_bio(token, entity_dict, prev_entity)\n",
    "            data_rows.append({\n",
    "                \"sentence_id\": sent_id,\n",
    "                \"token\": token,\n",
    "                \"ner_label\": bio_label\n",
    "            })\n",
    "            prev_entity = current_entity if bio_label != \"O\" else None\n",
    "    return data_rows\n",
    "\n",
    "all_rows = []\n",
    "doc_id = 0\n",
    "for _, row in df.iterrows():\n",
    "    labeled_tokens = auto_label_bio_using_dict(row[\"clean_text\"], entity_dict)\n",
    "    for item in labeled_tokens:\n",
    "        all_rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sentence_id\": item[\"sentence_id\"],\n",
    "            \"token\": item[\"token\"],\n",
    "            \"ner_label\": item[\"ner_label\"]\n",
    "        })\n",
    "    doc_id += 1\n",
    "\n",
    "auto_ner_df = pd.DataFrame(all_rows)\n",
    "auto_ner_df.to_csv(\"auto_ner_data.csv\", index=False)\n",
    "print(\"\\nAuto-labeled DataFrame shape:\", auto_ner_df.shape)\n",
    "print(\"Saved auto-labeled NER data to 'auto_ner_data.csv'.\")\n",
    "\n",
    "############################################\n",
    "# 7. GROUP TOKENS BY SENTENCE FOR TRAINING EXAMPLES\n",
    "############################################\n",
    "\n",
    "grouped = auto_ner_df.groupby([\"doc_id\", \"sentence_id\"])\n",
    "examples = []\n",
    "for (doc_id, sent_id), group in grouped:\n",
    "    tokens = group[\"token\"].tolist()\n",
    "    labels = group[\"ner_label\"].tolist()\n",
    "    examples.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"sentence_id\": sent_id,\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": labels\n",
    "    })\n",
    "df_grouped = pd.DataFrame(examples)\n",
    "print(\"\\nGrouped DataFrame shape:\", df_grouped.shape)\n",
    "print(df_grouped.head())\n",
    "\n",
    "############################################\n",
    "# 8. SPLIT TRAIN/VALIDATION & CREATE DATASETS\n",
    "############################################\n",
    "\n",
    "train_size = int(0.8 * len(df_grouped))\n",
    "train_df = df_grouped.iloc[:train_size]\n",
    "val_df = df_grouped.iloc[train_size:]\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Validation size:\", val_df.shape)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "############################################\n",
    "# 9. TOKENIZATION & LABEL ALIGNMENT FOR TRAINING\n",
    "############################################\n",
    "\n",
    "# Define label list (using standard BIO tags)\n",
    "label_list = [\"O\", \"B-PER\", \"B-LOC\", \"B-MISC\"]\n",
    "label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# Add special tokens from our entity dictionary to the tokenizer to avoid undesired subword splitting.\n",
    "special_tokens = [k for k in entity_dict.keys() if len(k) > 4]\n",
    "num_added = tokenizer.add_tokens(special_tokens)\n",
    "print(\"\\nNumber of special tokens added:\", num_added)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i in range(len(examples[\"tokens\"])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        example_labels = examples[\"ner_tags\"][i]\n",
    "        aligned_labels = []\n",
    "        prev_wid = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                label_str = example_labels[wid]\n",
    "                # For contiguous subword tokens of the same original word, convert B- to I-\n",
    "                if wid == prev_wid and label_str != \"O\" and label_str.startswith(\"B-\"):\n",
    "                    label_str = \"I-\" + label_str[2:]\n",
    "                aligned_labels.append(label2id.get(label_str, 0))\n",
    "            prev_wid = wid\n",
    "        all_labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    load_from_cache_file=False  # disable caching to save disk space\n",
    ")\n",
    "print(\"\\nProcessed datasets ready for training:\")\n",
    "print(processed_datasets)\n",
    "\n",
    "############################################\n",
    "# 10. SHOW LABEL DISTRIBUTION & COMPUTE WEIGHTS\n",
    "############################################\n",
    "\n",
    "label_counts = Counter(auto_ner_df[\"ner_label\"])\n",
    "print(\"\\nLabel distribution in auto_ner_df:\", label_counts)\n",
    "\n",
    "# Compute weights for each label as 1/(count+1)\n",
    "weight_list = [1.0 / (label_counts.get(lbl, 0) + 1) for lbl in label_list]\n",
    "weight_tensor = torch.tensor(weight_list, dtype=torch.float)\n",
    "print(\"Weight tensor:\", weight_tensor)\n",
    "\n",
    "############################################\n",
    "# 11. TRAIN THE CUSTOM NER MODEL\n",
    "############################################\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "    for pred_row, label_row in zip(predictions, labels):\n",
    "        temp_true_labels = []\n",
    "        temp_true_preds = []\n",
    "        for p_i, l_i in zip(pred_row, label_row):\n",
    "            if l_i == -100:\n",
    "                continue\n",
    "            temp_true_labels.append(id2label[l_i])\n",
    "            temp_true_preds.append(id2label[p_i])\n",
    "        if temp_true_labels:\n",
    "            true_labels.append(temp_true_labels)\n",
    "            true_preds.append(temp_true_preds)\n",
    "    if len(true_labels) == 0:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"accuracy\": 1.0}\n",
    "    results = seqeval.compute(predictions=true_preds, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results.get(\"overall_precision\", 0.0),\n",
    "        \"recall\": results.get(\"overall_recall\", 0.0),\n",
    "        \"f1\": results.get(\"overall_f1\", 0.0),\n",
    "        \"accuracy\": results.get(\"overall_accuracy\", 1.0)\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# Remove gradient checkpointing to reduce MPS issues\n",
    "# model.gradient_checkpointing_enable()  # (Commented out)\n",
    "\n",
    "# Before training, try clearing any unused memory on the MPS device.\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"greenlandic_ner_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=1,  # Lower batch size for MPS\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=50,\n",
    "    fp16=False,  # Disabled for MPS\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# Use the standard Trainer (without extra parameters like weight in constructor)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"greenlandic_ner_model\")\n",
    "tokenizer.save_pretrained(\"greenlandic_ner_model\")\n",
    "\n",
    "############################################\n",
    "# 12. INFERENCE\n",
    "############################################\n",
    "\n",
    "ner_infer = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"greenlandic_ner_model\",\n",
    "    tokenizer=\"greenlandic_ner_model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "test_text = \"Nukúnguasik traveled from Ikerssuaq to Nuuk.\"\n",
    "print(\"\\nInference output on sample text:\")\n",
    "print(ner_infer(test_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Data loaded. Shape: (51, 3)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   story_id  51 non-null     int64 \n",
      " 1   title     51 non-null     object\n",
      " 2   text      51 non-null     object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.3+ KB\n",
      "None\n",
      "Duplicate story IDs: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/lukaskreibig/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cleaned text:\n",
      " Our forefathers have told us much of the coming of earth, and of men, and it was a long, long while ago. Those who lived long before our day, they did not know how to store their words in little black marks, as you do; they could only tell stories. And they told of many things, and therefore we are  ...\n",
      "\n",
      "Scraped 337 candidate personal names from Wiktionary.\n",
      "Scraped 76 candidate town names from Wikipedia.\n",
      "Loaded manually curated candidate entities:\n",
      "  entity_candidate entity\n",
      "0            Ailaq  B-PER\n",
      "1             Aluk  B-PER\n",
      "2           Alátaq  B-PER\n",
      "3         Amerdloq  B-PER\n",
      "4          Anarteq  B-PER\n",
      "Final Entity Dictionary (sample):\n",
      "aaju: B-PER\n",
      "aaneeraq: B-PER\n",
      "aani: B-PER\n",
      "aaninnguaq: B-PER\n",
      "aannguaq: B-PER\n",
      "aappilattoq: B-LOC\n",
      "aaqa: B-PER\n",
      "aasiaat: B-LOC\n",
      "aggu: B-PER\n",
      "ailaq: B-PER\n",
      "aima: B-PER\n",
      "aja: B-PER\n",
      "ajaaja: B-PER\n",
      "aka: B-PER\n",
      "akisooq: B-PER\n",
      "akitsinnguaq: B-PER\n",
      "akunnaaq: B-LOC\n",
      "aleqa: B-PER\n",
      "alibak: B-PER\n",
      "alluitsup paa: B-LOC\n",
      "Saved final entity dictionary to 'final_entity_dictionary.csv'.\n",
      "\n",
      "Auto-labeled DataFrame shape: (49656, 4)\n",
      "Saved auto-labeled NER data to 'auto_ner_data.csv'.\n",
      "\n",
      "Grouped DataFrame shape: (2044, 4)\n",
      "   doc_id  sentence_id                                             tokens  \\\n",
      "0       0            0  [Our, forefathers, have, told, us, much, of, t...   \n",
      "1       0            1  [Those, who, lived, long, before, our, day, ,,...   \n",
      "2       0            2  [And, they, told, of, many, things, ,, and, th...   \n",
      "3       0            3  [Old, women, do, not, waste, their, words, idl...   \n",
      "4       0            4                      [Old, age, does, not, lie, .]   \n",
      "\n",
      "                                            ner_tags  \n",
      "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "3   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
      "4                                 [O, O, O, O, O, O]  \n",
      "Train size: (1635, 4)\n",
      "Validation size: (409, 4)\n",
      "\n",
      "Number of special tokens added: 398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1635/1635 [00:00<00:00, 18420.88 examples/s]\n",
      "Map: 100%|██████████| 409/409 [00:00<00:00, 17284.51 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed datasets ready for training:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 1635\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 409\n",
      "    })\n",
      "})\n",
      "\n",
      "Label distribution in auto_ner_df: Counter({'O': 49212, 'B-PER': 390, 'B-MISC': 44, 'B-LOC': 10})\n",
      "Weight tensor: tensor([2.0320e-05, 2.5575e-03, 9.0909e-02, 2.2222e-02])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of CustomXLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/lukaskreibig/Documents/climate-dashboard/.venv/lib/python3.10/site-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/2l/6514_hd91tv5448lmq79vpbw0000gn/T/ipykernel_49603/1809248619.py:369: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='334' max='4085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 334/4085 03:45 < 42:29, 1.47 it/s, Epoch 0.41/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# 1. SETUP & IMPORTS\n",
    "############################################\n",
    "\n",
    "import os\n",
    "# Allow MPS to use up to 90% of available memory.\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.9\"\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "from transformers import (\n",
    "    pipeline, AutoTokenizer, AutoModelForTokenClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('words')\n",
    "\n",
    "# Determine device (MPS if available, otherwise CPU)\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "############################################\n",
    "# 2. LOAD & EXPLORE FOLKTALES DATA\n",
    "############################################\n",
    "\n",
    "df = pd.read_pickle(\"eskimo_folktales.pkl\")\n",
    "print(\"Data loaded. Shape:\", df.shape)\n",
    "print(df.info())\n",
    "print(\"Duplicate story IDs:\", df.story_id.duplicated().sum())\n",
    "\n",
    "df[\"text_length\"] = df[\"text\"].apply(lambda x: len(x.split()))\n",
    "plt.hist(df[\"text_length\"], bins=20)\n",
    "plt.title(\"Distribution of Story Lengths\")\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Number of Stories\")\n",
    "plt.show()\n",
    "\n",
    "############################################\n",
    "# 3. CLEAN THE TEXT\n",
    "############################################\n",
    "\n",
    "def clean_text_for_ner(text: str) -> str:\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    paragraphs = re.split(r'\\n\\s*\\n+', text.strip())\n",
    "    cleaned_paragraphs = []\n",
    "    for para in paragraphs:\n",
    "        para = re.sub(r'\\n+', ' ', para)\n",
    "        para = para.replace('’', \"'\").replace('‘', \"'\").replace('—', '-')\n",
    "        para = re.sub(r'\\s+', ' ', para).strip()\n",
    "        cleaned_paragraphs.append(para)\n",
    "    return \"\\n\\n\".join(cleaned_paragraphs)\n",
    "\n",
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text_for_ner)\n",
    "print(\"Sample cleaned text:\\n\", df[\"clean_text\"].iloc[0][:300], \"...\\n\")\n",
    "\n",
    "############################################\n",
    "# 4. SCRAPE CANDIDATE ENTITIES FROM THE WEB\n",
    "############################################\n",
    "\n",
    "# 4a. Scrape candidate personal names from Wiktionary\n",
    "url_names = \"https://en.wiktionary.org/wiki/Appendix:Greenlandic_given_names\"\n",
    "resp_names = requests.get(url_names)\n",
    "soup_names = BeautifulSoup(resp_names.text, \"html.parser\")\n",
    "scraped_names = set()\n",
    "for dd in soup_names.select(\"dl dd\"):\n",
    "    for link in dd.find_all(\"a\"):\n",
    "        candidate = link.get_text(strip=True)\n",
    "        if candidate and len(candidate) > 1:\n",
    "            scraped_names.add(candidate)\n",
    "print(f\"Scraped {len(scraped_names)} candidate personal names from Wiktionary.\")\n",
    "\n",
    "# 4b. Scrape candidate town names from Wikipedia\n",
    "url_towns = \"https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Greenland\"\n",
    "resp_towns = requests.get(url_towns)\n",
    "soup_towns = BeautifulSoup(resp_towns.text, \"html.parser\")\n",
    "scraped_towns = set()\n",
    "tables = soup_towns.find_all(\"table\", class_=\"wikitable\")\n",
    "for table in tables:\n",
    "    for row in table.find_all(\"tr\"):\n",
    "        for link in row.find_all(\"a\", href=True):\n",
    "            candidate = link.get_text(strip=True)\n",
    "            if candidate and len(candidate) > 1:\n",
    "                if any(bad in candidate.lower() for bad in [\n",
    "                    \"edit\", \"coordinate\", \"article\", \"statement\", \"isbn\",\n",
    "                    \"list of\", \"administrative\", \"autonomy\", \"history\", \"portal\"\n",
    "                ]):\n",
    "                    continue\n",
    "                scraped_towns.add(candidate)\n",
    "print(f\"Scraped {len(scraped_towns)} candidate town names from Wikipedia.\")\n",
    "\n",
    "############################################\n",
    "# 5. LOAD & MERGE MANUALLY CURATED CANDIDATE CSV\n",
    "############################################\n",
    "\n",
    "try:\n",
    "    manual_df = pd.read_csv(\"candidate_entities_finished.csv\")\n",
    "    print(\"Loaded manually curated candidate entities:\")\n",
    "    print(manual_df.head())\n",
    "    # Standardize keys: strip and lowercase; fix any stray labels (e.g., \"B-O\" to \"O\")\n",
    "    manual_df[\"entity_candidate\"] = manual_df[\"entity_candidate\"].str.strip().str.lower()\n",
    "    manual_df[\"entity\"] = manual_df[\"entity\"].str.strip().replace({\"B-O\": \"O\", \"B-O \": \"O\"})\n",
    "    manual_dict = dict(zip(manual_df[\"entity_candidate\"], manual_df[\"entity\"]))\n",
    "except Exception as e:\n",
    "    print(\"Manual candidate CSV not found; proceeding with scraped data only.\")\n",
    "    manual_dict = {}\n",
    "\n",
    "entity_dict = manual_dict.copy()\n",
    "for name in scraped_names:\n",
    "    key = name.strip().lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-PER\"  # Default scraped personal names as B-PER\n",
    "for town in scraped_towns:\n",
    "    key = town.strip().lower()\n",
    "    if key not in entity_dict:\n",
    "        entity_dict[key] = \"B-LOC\"  # Default scraped town names as B-LOC\n",
    "\n",
    "print(\"Final Entity Dictionary (sample):\")\n",
    "for key, val in sorted(entity_dict.items())[:20]:\n",
    "    print(f\"{key}: {val}\")\n",
    "\n",
    "final_entity_df = pd.DataFrame(list(entity_dict.items()), columns=[\"entity_candidate\", \"entity\"])\n",
    "final_entity_df.to_csv(\"final_entity_dictionary.csv\", index=False)\n",
    "print(\"Saved final entity dictionary to 'final_entity_dictionary.csv'.\")\n",
    "\n",
    "############################################\n",
    "# 6. AUTO-LABEL FOLKTALE TEXTS USING ENTITY DICTIONARY (BIO FORMAT)\n",
    "############################################\n",
    "\n",
    "def get_entity_label_bio(token, entity_dict, prev_entity):\n",
    "    token_lower = token.strip().lower()\n",
    "    if token_lower in entity_dict:\n",
    "        label = entity_dict[token_lower]\n",
    "    elif token_lower.endswith(\"s\"):\n",
    "        label = entity_dict.get(token_lower[:-1], \"O\")\n",
    "    else:\n",
    "        label = \"O\"\n",
    "    if label == \"O\":\n",
    "        return \"O\", None\n",
    "    # Split only on the first hyphen in case there are extra dashes in the entity type.\n",
    "    entity_type = label.split(\"-\", 1)[-1]\n",
    "    if prev_entity == entity_type:\n",
    "        return f\"I-{entity_type}\", entity_type\n",
    "    else:\n",
    "        return f\"B-{entity_type}\", entity_type\n",
    "\n",
    "def auto_label_bio_using_dict(text, entity_dict):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    data_rows = []\n",
    "    for sent_id, sentence in enumerate(sentences):\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        prev_entity = None\n",
    "        for token in tokens:\n",
    "            bio_label, current_entity = get_entity_label_bio(token, entity_dict, prev_entity)\n",
    "            data_rows.append({\n",
    "                \"sentence_id\": sent_id,\n",
    "                \"token\": token,\n",
    "                \"ner_label\": bio_label\n",
    "            })\n",
    "            prev_entity = current_entity if bio_label != \"O\" else None\n",
    "    return data_rows\n",
    "\n",
    "all_rows = []\n",
    "doc_id = 0\n",
    "for _, row in df.iterrows():\n",
    "    labeled_tokens = auto_label_bio_using_dict(row[\"clean_text\"], entity_dict)\n",
    "    for item in labeled_tokens:\n",
    "        all_rows.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"sentence_id\": item[\"sentence_id\"],\n",
    "            \"token\": item[\"token\"],\n",
    "            \"ner_label\": item[\"ner_label\"]\n",
    "        })\n",
    "    doc_id += 1\n",
    "\n",
    "auto_ner_df = pd.DataFrame(all_rows)\n",
    "auto_ner_df.to_csv(\"auto_ner_data.csv\", index=False)\n",
    "print(\"\\nAuto-labeled DataFrame shape:\", auto_ner_df.shape)\n",
    "print(\"Saved auto-labeled NER data to 'auto_ner_data.csv'.\")\n",
    "\n",
    "############################################\n",
    "# 7. GROUP TOKENS BY SENTENCE FOR TRAINING EXAMPLES\n",
    "############################################\n",
    "\n",
    "grouped = auto_ner_df.groupby([\"doc_id\", \"sentence_id\"])\n",
    "examples = []\n",
    "for (doc_id, sent_id), group in grouped:\n",
    "    tokens = group[\"token\"].tolist()\n",
    "    labels = group[\"ner_label\"].tolist()\n",
    "    examples.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"sentence_id\": sent_id,\n",
    "        \"tokens\": tokens,\n",
    "        \"ner_tags\": labels\n",
    "    })\n",
    "df_grouped = pd.DataFrame(examples)\n",
    "print(\"\\nGrouped DataFrame shape:\", df_grouped.shape)\n",
    "print(df_grouped.head())\n",
    "\n",
    "############################################\n",
    "# 8. SPLIT TRAIN/VALIDATION & CREATE DATASETS\n",
    "############################################\n",
    "\n",
    "train_size = int(0.8 * len(df_grouped))\n",
    "train_df = df_grouped.iloc[:train_size]\n",
    "val_df = df_grouped.iloc[train_size:]\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Validation size:\", val_df.shape)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset\n",
    "})\n",
    "\n",
    "############################################\n",
    "# 9. TOKENIZATION & LABEL ALIGNMENT FOR TRAINING\n",
    "############################################\n",
    "\n",
    "# Define your label list – using our standard BIO tags (adjust if needed)\n",
    "label_list = [\"O\", \"B-PER\", \"B-LOC\", \"B-MISC\"]\n",
    "label2id = {lbl: i for i, lbl in enumerate(label_list)}\n",
    "id2label = {i: lbl for lbl, i in label2id.items()}\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "# To reduce undesired subword splitting, add special tokens from our entity dictionary.\n",
    "special_tokens = [k for k in entity_dict.keys() if len(k) > 4]\n",
    "num_added = tokenizer.add_tokens(special_tokens)\n",
    "print(\"\\nNumber of special tokens added:\", num_added)\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    all_labels = []\n",
    "    for i in range(len(examples[\"tokens\"])):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        example_labels = examples[\"ner_tags\"][i]\n",
    "        aligned_labels = []\n",
    "        prev_wid = None\n",
    "        for wid in word_ids:\n",
    "            if wid is None:\n",
    "                aligned_labels.append(-100)\n",
    "            else:\n",
    "                label_str = example_labels[wid]\n",
    "                # Merge contiguous subword tokens: if same word index, convert B- to I-\n",
    "                if wid == prev_wid and label_str != \"O\" and label_str.startswith(\"B-\"):\n",
    "                    label_str = \"I-\" + label_str[2:]\n",
    "                aligned_labels.append(label2id.get(label_str, 0))\n",
    "            prev_wid = wid\n",
    "        all_labels.append(aligned_labels)\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "processed_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    "    load_from_cache_file=False  # Disable caching to save disk space\n",
    ")\n",
    "print(\"\\nProcessed datasets ready for training:\")\n",
    "print(processed_datasets)\n",
    "\n",
    "############################################\n",
    "# 10. SHOW LABEL DISTRIBUTION & COMPUTE WEIGHTS\n",
    "############################################\n",
    "\n",
    "label_counts = Counter(auto_ner_df[\"ner_label\"])\n",
    "print(\"\\nLabel distribution in auto_ner_df:\", label_counts)\n",
    "\n",
    "# Compute weights for each label as 1/(count+1)\n",
    "weight_list = [1.0 / (label_counts.get(lbl, 0) + 1) for lbl in label_list]\n",
    "weight_tensor = torch.tensor(weight_list, dtype=torch.float)\n",
    "print(\"Weight tensor:\", weight_tensor)\n",
    "\n",
    "############################################\n",
    "# 11. CUSTOM MODEL CLASS TO REMOVE EXTRA KEYWORDS\n",
    "############################################\n",
    "\n",
    "# Create a custom subclass to override forward() so that it ignores the unexpected keyword argument.\n",
    "from transformers.models.xlm_roberta.modeling_xlm_roberta import XLMRobertaForTokenClassification\n",
    "\n",
    "class CustomXLMRobertaForTokenClassification(XLMRobertaForTokenClassification):\n",
    "    def forward(self, *args, **kwargs):\n",
    "        # Remove unexpected keyword 'num_items_in_batch' if present.\n",
    "        kwargs.pop(\"num_items_in_batch\", None)\n",
    "        return super().forward(*args, **kwargs)\n",
    "\n",
    "# Instantiate our custom model:\n",
    "model = CustomXLMRobertaForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))  # Resize embeddings to account for new special tokens\n",
    "\n",
    "############################################\n",
    "# 12. TRAIN THE CUSTOM NER MODEL\n",
    "############################################\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "    for pred_row, label_row in zip(predictions, labels):\n",
    "        temp_true_labels = []\n",
    "        temp_true_preds = []\n",
    "        for p_i, l_i in zip(pred_row, label_row):\n",
    "            if l_i == -100:\n",
    "                continue\n",
    "            temp_true_labels.append(id2label[l_i])\n",
    "            temp_true_preds.append(id2label[p_i])\n",
    "        if temp_true_labels:\n",
    "            true_labels.append(temp_true_labels)\n",
    "            true_preds.append(temp_true_preds)\n",
    "    if len(true_labels) == 0:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"accuracy\": 1.0}\n",
    "    results = seqeval.compute(predictions=true_preds, references=true_labels, zero_division=0)\n",
    "    return {\n",
    "        \"precision\": results.get(\"overall_precision\", 0.0),\n",
    "        \"recall\": results.get(\"overall_recall\", 0.0),\n",
    "        \"f1\": results.get(\"overall_f1\", 0.0),\n",
    "        \"accuracy\": results.get(\"overall_accuracy\", 1.0)\n",
    "    }\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, padding=True)\n",
    "\n",
    "# Clear MPS cache if available\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"greenlandic_ner_checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=1,       # Lower batch size for MPS\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    logging_steps=50,\n",
    "    fp16=False,                          # fp16 disabled on MPS\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_datasets[\"train\"],\n",
    "    eval_dataset=processed_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"greenlandic_ner_model\")\n",
    "tokenizer.save_pretrained(\"greenlandic_ner_model\")\n",
    "\n",
    "############################################\n",
    "# 13. INFERENCE\n",
    "############################################\n",
    "\n",
    "ner_infer = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"greenlandic_ner_model\",\n",
    "    tokenizer=\"greenlandic_ner_model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "test_text = \"Nukúnguasik traveled from Ikerssuaq to Nuuk.\"\n",
    "print(\"\\nInference output on sample text:\")\n",
    "print(ner_infer(test_text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
